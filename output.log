2023-10-22 17:58:11 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:58:11 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:11 [scrapy.extensions.telnet] INFO: Telnet Password: 9b5c7671d9d6f1cb
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:11 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:13 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 17:58:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17269,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3934665,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.257016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 0, 58, 23, 124289),
 'httpcompression/response_bytes': 18085934,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 50,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/AttributeError': 50,
 'start_time': datetime.datetime(2023, 10, 23, 0, 58, 11, 867273)}
2023-10-22 17:58:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 17:58:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:58:25 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:25 [scrapy.extensions.telnet] INFO: Telnet Password: eeeb5dc40f7e5543
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:25 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:26 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 17:58:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17249,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3932517,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 10.194672,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 0, 58, 36, 65623),
 'httpcompression/response_bytes': 18083233,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 50,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/AttributeError': 50,
 'start_time': datetime.datetime(2023, 10, 23, 0, 58, 25, 870951)}
2023-10-22 17:58:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 17:58:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': '11'}
2023-10-22 17:58:53 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:53 [scrapy.extensions.telnet] INFO: Telnet Password: 16111dc3457c0eb2
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:53 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:55 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:59:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:59:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:59:20 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:59:20 [scrapy.extensions.telnet] INFO: Telnet Password: 54552a52c6ff65e7
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:59:20 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:59:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:59:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:01:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:01:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:01:18 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:01:18 [scrapy.extensions.telnet] INFO: Telnet Password: 6c9f34180780b806
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:01:18 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:01:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:01:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:21 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:01:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:01:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:01:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': 'ff'}
2023-10-22 18:01:26 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:01:26 [scrapy.extensions.telnet] INFO: Telnet Password: 7728fde1c686152e
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:01:26 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:01:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:01:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:27 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:04:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:04:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:04:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:04:02 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:04:02 [scrapy.extensions.telnet] INFO: Telnet Password: 7ac2cb01b2926e75
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:04:02 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:04:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:04:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:04:04 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:04:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:04:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17092,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3937127,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.118511,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 4, 15, 626704),
 'httpcompression/response_bytes': 18096335,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 4, 2, 508193)}
2023-10-22 18:04:15 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 18:33:49 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:33:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:33:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:33:49 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:33:49 [scrapy.extensions.telnet] INFO: Telnet Password: d82c551d182cbe3d
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:33:49 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:33:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:33:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:33:50 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:34:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:34:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17108,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3952542,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 12.631892,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 34, 1, 991556),
 'httpcompression/response_bytes': 18116370,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 33, 49, 359664)}
2023-10-22 18:34:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 18:34:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:34:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:34:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:34:09 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:34:09 [scrapy.extensions.telnet] INFO: Telnet Password: 986d7f3d4ced0cd0
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:34:09 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:34:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:34:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:34:10 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:34:20 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:34:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17224,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3951674,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.07249,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 34, 20, 541469),
 'httpcompression/response_bytes': 18116581,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 34, 9, 468979)}
2023-10-22 18:34:20 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-23 23:27:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:27:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:27:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:27:25 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:27:25 [scrapy.extensions.telnet] INFO: Telnet Password: a03e9462d9653945
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:27:25 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:27:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:27:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:28:33 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:28:33 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:28:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:28:33 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:28:33 [scrapy.extensions.telnet] INFO: Telnet Password: d42157d56822e353
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:28:33 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:28:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:28:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:29:33 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 30 pages/min), scraped 290 items (at 290 items/min)
2023-10-23 23:29:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-23 23:29:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20595,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3938374,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 80.059072,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 23, 29, 53, 184323),
 'httpcompression/response_bytes': 18286442,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 23, 28, 33, 125251)}
2023-10-23 23:29:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-23 23:33:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:33:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:33:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:33:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:33:09 [scrapy.extensions.telnet] INFO: Telnet Password: 1a3d5ce2b3f45252
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:33:09 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:33:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:33:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:33:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-23 23:33:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79366,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 3.662685,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 23, 33, 13, 364840),
 'httpcompression/response_bytes': 354915,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 23, 23, 33, 9, 702155)}
2023-10-23 23:33:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 01:22:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:22:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:22:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:22:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:22:37 [scrapy.extensions.telnet] INFO: Telnet Password: 97f19cafc5a34e66
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:22:37 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:22:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:22:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:42:06 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 01:48:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:48:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:48:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:48:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:48:13 [scrapy.extensions.telnet] INFO: Telnet Password: 0ef9d5e3c3c939d1
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:48:13 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:48:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:48:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:48:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:48:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:48:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:13 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:49:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:13 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:50:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:50:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:50:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:50:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:50:39 [scrapy.extensions.telnet] INFO: Telnet Password: 4c3e4da91c3e6bfb
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:50:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:50:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:50:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:51:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:39 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:51:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:53:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:53:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:53:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:53:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:53:02 [scrapy.extensions.telnet] INFO: Telnet Password: 1efd2404680fa838
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:53:02 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:53:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:53:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:53:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:53:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:53:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:53:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:53:50 [scrapy.extensions.telnet] INFO: Telnet Password: 7b2199a09209ec1d
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:53:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:53:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:53:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:55:04 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:55:04 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:55:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:55:04 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:55:04 [scrapy.extensions.telnet] INFO: Telnet Password: 6fc0fb1af9d51637
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:55:04 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:55:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:55:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:56:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:56:09 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:57:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:57:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:57:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:57:39 [scrapy.extensions.telnet] INFO: Telnet Password: 6ea2c6cfb41aff55
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:57:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:57:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:58:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:58:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:58:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:58:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:58:56 [scrapy.extensions.telnet] INFO: Telnet Password: 589c824d0a5c582f
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:58:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:58:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:58:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:02:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:02:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:02:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:02:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:02:06 [scrapy.extensions.telnet] INFO: Telnet Password: d4a775acd308502d
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:02:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:02:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:02:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:02:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:02:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:02:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:02:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:02:35 [scrapy.extensions.telnet] INFO: Telnet Password: a1439efaf433ec0a
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:02:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:02:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:02:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:03:06 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:35 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:43 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:03:43 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:03:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:03:43 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:03:43 [scrapy.extensions.telnet] INFO: Telnet Password: 3b96fa9767c6704d
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:03:43 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:03:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:03:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:06 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:04:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:04:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:04:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:04:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:04:10 [scrapy.extensions.telnet] INFO: Telnet Password: 4d18c6b9a212e161
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:04:10 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:04:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:04:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:04:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:04:56 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x28373c8c7d0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 55, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 17,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 12,
 'downloader/request_bytes': 26083,
 'downloader/request_count': 68,
 'downloader/request_method_count/GET': 68,
 'downloader/response_bytes': 3936049,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 169.65009,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 4, 56, 252219),
 'httpcompression/response_bytes': 17994885,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 51,
 'log_count/INFO': 12,
 'log_count/WARNING': 6,
 'response_received_count': 51,
 'retry/count': 17,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 5,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 67,
 'scheduler/dequeued/memory': 67,
 'scheduler/enqueued': 67,
 'scheduler/enqueued/memory': 67,
 'spider_exceptions/FileNotFoundError': 50,
 'start_time': datetime.datetime(2023, 10, 24, 2, 2, 6, 602129)}
2023-10-24 02:04:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:05:15 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 02:05:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:05:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 7,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 7,
 'downloader/request_bytes': 22059,
 'downloader/request_count': 58,
 'downloader/request_method_count/GET': 58,
 'downloader/response_bytes': 3940135,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 73.337621,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 5, 23, 909371),
 'httpcompression/response_bytes': 18014876,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 48,
 'request_depth_max': 1,
 'response_received_count': 51,
 'retry/count': 7,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 7,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 57,
 'scheduler/dequeued/memory': 57,
 'scheduler/enqueued': 57,
 'scheduler/enqueued/memory': 57,
 'start_time': datetime.datetime(2023, 10, 24, 2, 4, 10, 571750)}
2023-10-24 02:05:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:50:15 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:15 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:15 [scrapy.extensions.telnet] INFO: Telnet Password: 2b55b3a10beac79e
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:15 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:50:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=150000&minPrice=35000&maxFloorArea=2000&minFloorArea=250&maxSiteArea=10000&minSiteArea=200&numParkingSpaces=1&energyEfficiency=1&keywords=factory%2Bshop&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:50:28 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:50:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34957,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2840838,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.434935,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 50, 28, 664032),
 'httpcompression/response_bytes': 9351115,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 50, 15, 229097)}
2023-10-24 02:50:28 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:50:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:48 [scrapy.extensions.telnet] INFO: Telnet Password: 8d681800b23e4a72
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:48 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:50:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:57 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:57 [scrapy.extensions.telnet] INFO: Telnet Password: 9c50c2a7dbef01af
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:57 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:51:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=150000&minPrice=35000&maxFloorArea=2000&minFloorArea=250&maxSiteArea=10000&minSiteArea=200&numParkingSpaces=1&energyEfficiency=1&keywords=factory&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:51:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:51:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34603,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2839029,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.885398,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 51, 11, 126514),
 'httpcompression/response_bytes': 9338333,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 50, 57, 241116)}
2023-10-24 02:51:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:51:19 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:51:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:51:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:51:19 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:51:19 [scrapy.extensions.telnet] INFO: Telnet Password: 81e53762eadb2501
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:51:19 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:51:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:51:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:51:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=1000000&numParkingSpaces=1&energyEfficiency=1&keywords=factory&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:51:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:51:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 30456,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2839072,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 16.002299,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 51, 35, 645678),
 'httpcompression/response_bytes': 9338119,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 51, 19, 643379)}
2023-10-24 02:51:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:51:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:51:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:51:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:51:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:51:58 [scrapy.extensions.telnet] INFO: Telnet Password: 8bffe712f76b64c3
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:51:58 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:51:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:51:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:52:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 29428,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2998434,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 14.283853,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 52, 12, 943884),
 'httpcompression/response_bytes': 10854209,
 'httpcompression/response_count': 50,
 'item_scraped_count': 16,
 'log_count/ERROR': 5,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 5,
 'start_time': datetime.datetime(2023, 10, 24, 2, 51, 58, 660031)}
2023-10-24 02:52:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:52:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:52:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:52:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:52:26 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:52:26 [scrapy.extensions.telnet] INFO: Telnet Password: 688f2bdc53f49ba6
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:52:26 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:52:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:52:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:52:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 507, in Client
    answer_challenge(c, authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 751, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 215, in recv_bytes
    buf = self._recv_bytes(maxlength)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:52:40 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x16df173d290>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 55, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 27995,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3949410,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.87832,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 52, 40, 684184),
 'httpcompression/response_bytes': 18136370,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 51,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/EOFError': 1,
 'spider_exceptions/FileNotFoundError': 49,
 'start_time': datetime.datetime(2023, 10, 24, 2, 52, 26, 805864)}
2023-10-24 02:52:40 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:53:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:53:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:53:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:53:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:53:47 [scrapy.extensions.telnet] INFO: Telnet Password: c41d9c7923c81ce4
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:53:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:53:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:53:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:54:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 29698,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3913133,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.353105,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 54, 0, 794794),
 'httpcompression/response_bytes': 18623381,
 'httpcompression/response_count': 50,
 'item_scraped_count': 192,
 'log_count/ERROR': 31,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 19,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 31,
 'start_time': datetime.datetime(2023, 10, 24, 2, 53, 47, 441689)}
2023-10-24 02:54:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 03:16:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:16:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:16:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:16:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:16:40 [scrapy.extensions.telnet] INFO: Telnet Password: 6ec5e4c8f5c6e849
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:16:40 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:16:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:16:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:16:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 03:16:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 21267,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3817741,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 12.43017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 3, 16, 52, 730155),
 'httpcompression/response_bytes': 17407705,
 'httpcompression/response_count': 50,
 'item_scraped_count': 356,
 'log_count/ERROR': 15,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 35,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 15,
 'start_time': datetime.datetime(2023, 10, 24, 3, 16, 40, 299985)}
2023-10-24 03:16:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 03:19:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:19:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:19:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:19:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:19:34 [scrapy.extensions.telnet] INFO: Telnet Password: 075ada4189015eec
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:19:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:19:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:19:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:19:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:19:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:19:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:19:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:19:47 [scrapy.extensions.telnet] INFO: Telnet Password: fff2f445b24a8940
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:19:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:19:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:19:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:19:59 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 03:19:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 19716,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3944105,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.488783,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 3, 19, 59, 445498),
 'httpcompression/response_bytes': 18024360,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 48,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 3, 19, 47, 956715)}
2023-10-24 03:19:59 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:52:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:52:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:52:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:52:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:52:30 [scrapy.extensions.telnet] INFO: Telnet Password: 07e15f4c6e558b22
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:52:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:52:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:52:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:52:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:52:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20517,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3602287,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 26.730949,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 52, 56, 915232),
 'httpcompression/response_bytes': 15407464,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 52, 30, 184283)}
2023-10-24 05:52:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:53:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:53:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:53:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:53:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:53:45 [scrapy.extensions.telnet] INFO: Telnet Password: e9f764025428e6db
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:53:45 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:53:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:53:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:53:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:54:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20516,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3601139,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 24.905852,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 54, 10, 843020),
 'httpcompression/response_bytes': 15407408,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 53, 45, 937168)}
2023-10-24 05:54:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:54:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:54:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:54:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:54:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:54:14 [scrapy.extensions.telnet] INFO: Telnet Password: 5c09fd50ca0d8a73
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:54:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:54:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:54:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:54:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:38 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:54:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20516,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3601170,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 24.419378,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 54, 38, 876401),
 'httpcompression/response_bytes': 15407409,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 54, 14, 457023)}
2023-10-24 05:54:38 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:54:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:54:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:54:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:54:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:54:44 [scrapy.extensions.telnet] INFO: Telnet Password: d895aa8a08e81314
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:54:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:54:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:54:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:55:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:55:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:55:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:55:14 [scrapy.extensions.telnet] INFO: Telnet Password: cb5730cc7101ebc6
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:55:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:55:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:55:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:55:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:55:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:55:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:55:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:55:36 [scrapy.extensions.telnet] INFO: Telnet Password: e6b2804945694fb2
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:55:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:55:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:55:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:55:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5013&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:56:04 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:56:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20463,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3480007,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 28.196052,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 56, 4, 626540),
 'httpcompression/response_bytes': 14510005,
 'httpcompression/response_count': 50,
 'item_scraped_count': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 30,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 5, 55, 36, 430488)}
2023-10-24 05:56:04 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:56:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:56:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:56:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:56:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:56:56 [scrapy.extensions.telnet] INFO: Telnet Password: 30cffb5742593986
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:56:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:56:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:56:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:59:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:59:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:59:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:59:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:59:06 [scrapy.extensions.telnet] INFO: Telnet Password: 6bf3ee038599a49d
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:59:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:59:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:59:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:59:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:00:59 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x15146cf0b10>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 56, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 06:00:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79799,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 25.333023,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 59, 32, 30419),
 'httpcompression/response_bytes': 351686,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 5, 59, 6, 697396)}
2023-10-24 06:00:59 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:01:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:01:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:01:23 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:01:23 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:01:23 [scrapy.extensions.telnet] INFO: Telnet Password: 0b702b032ee6893d
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:01:23 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:01:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:01:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:01:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 53, in parse
    yield scrapy.follow(url=next_page_url, callback=self.parse)
          ^^^^^^^^^^^^^
AttributeError: module 'scrapy' has no attribute 'follow'
2023-10-24 06:01:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:01:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79954,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 8.319788,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 1, 31, 504912),
 'httpcompression/response_bytes': 351932,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 1, 23, 185124)}
2023-10-24 06:01:34 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:04:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:04:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:04:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:04:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:04:16 [scrapy.extensions.telnet] INFO: Telnet Password: d5969e2b3adba7a8
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:04:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:04:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:04:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:04:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:04:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 608,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79116,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 13.915329,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 4, 30, 714968),
 'httpcompression/response_bytes': 342400,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 4, 16, 799639)}
2023-10-24 06:04:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:15:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:15:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:15:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:15:55 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:15:55 [scrapy.extensions.telnet] INFO: Telnet Password: 1e26c5442af7fe40
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:15:55 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:15:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:15:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:16:29 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:17:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 76434,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 33.736372,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 16, 29, 33460),
 'httpcompression/response_bytes': 343907,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 15, 55, 297088)}
2023-10-24 06:17:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:17:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:17:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:17:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:17:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:17:44 [scrapy.extensions.telnet] INFO: Telnet Password: 67654778a8b92c22
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:17:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:17:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:17:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:17:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:17:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79345,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 6.767828,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 17, 51, 29941),
 'httpcompression/response_bytes': 354391,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 17, 44, 262113)}
2023-10-24 06:17:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:18:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:18:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:18:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:18:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:18:06 [scrapy.extensions.telnet] INFO: Telnet Password: 094f52b356e19d3a
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:18:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:18:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:18:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:18:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:20:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79349,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 8.04743,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 18, 14, 329277),
 'httpcompression/response_bytes': 354391,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 18, 6, 281847)}
2023-10-24 06:20:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:23:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:23:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:23:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:23:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:23:39 [scrapy.extensions.telnet] INFO: Telnet Password: 1e371f6a07d02edd
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:23:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:23:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:23:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:23:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:23:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79905,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 14.886245,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 23, 54, 143867),
 'httpcompression/response_bytes': 363521,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1262,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 23, 39, 257622)}
2023-10-24 06:23:55 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:29:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:29:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:29:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:29:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:29:44 [scrapy.extensions.telnet] INFO: Telnet Password: 64d20a6fdb1abbd3
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:29:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:29:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:29:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:29:48 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:29:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79399,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.84203,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 29, 48, 379917),
 'httpcompression/response_bytes': 361397,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 29, 44, 537887)}
2023-10-24 06:29:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:31:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:31:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:31:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:31:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:31:14 [scrapy.extensions.telnet] INFO: Telnet Password: 8f2b9810bf4bccf8
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:31:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:31:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:31:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:31:17 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:31:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79799,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.644789,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 31, 17, 911182),
 'httpcompression/response_bytes': 361168,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 31, 14, 266393)}
2023-10-24 06:31:17 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:38:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:38:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:38:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:38:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:38:35 [scrapy.extensions.telnet] INFO: Telnet Password: 4b60057a5ae08282
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:38:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:38:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:38:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:39:35 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 19 pages/min), scraped 190 items (at 190 items/min)
2023-10-24 06:40:35 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 9 pages/min), scraped 280 items (at 90 items/min)
2023-10-24 06:41:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:41:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24373,
 'downloader/request_count': 49,
 'downloader/request_method_count/GET': 49,
 'downloader/response_bytes': 3852602,
 'downloader/response_count': 49,
 'downloader/response_status_count/200': 49,
 'elapsed_time_seconds': 175.715549,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 41, 31, 427751),
 'httpcompression/response_bytes': 17657395,
 'httpcompression/response_count': 49,
 'item_scraped_count': 490,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 48,
 'response_received_count': 49,
 'scheduler/dequeued': 49,
 'scheduler/dequeued/memory': 49,
 'scheduler/enqueued': 49,
 'scheduler/enqueued/memory': 49,
 'start_time': datetime.datetime(2023, 10, 24, 6, 38, 35, 712202)}
2023-10-24 06:41:31 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:42:29 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:42:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:42:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:42:29 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:42:29 [scrapy.extensions.telnet] INFO: Telnet Password: aefdb6543a0fc2aa
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:42:29 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:42:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:42:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:43:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=16> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 06:43:27 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:43:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8613,
 'downloader/request_count': 16,
 'downloader/request_method_count/GET': 16,
 'downloader/response_bytes': 1262116,
 'downloader/response_count': 16,
 'downloader/response_status_count/200': 16,
 'elapsed_time_seconds': 58.260414,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 43, 27, 616822),
 'httpcompression/response_bytes': 5918640,
 'httpcompression/response_count': 16,
 'item_scraped_count': 154,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 15,
 'response_received_count': 16,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 42, 29, 356408)}
2023-10-24 06:43:27 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:28:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:28:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:28:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:28:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:28:36 [scrapy.extensions.telnet] INFO: Telnet Password: 1d8f42c710cf81d8
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:28:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:28:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:28:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:29:36 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 15 pages/min), scraped 150 items (at 150 items/min)
2023-10-24 07:30:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    items['area'] = item['attributes']['area']
    ^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:30:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:30:06 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1befd9ac690>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 90, in close
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:30:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 10819,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 1738633,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'elapsed_time_seconds': 89.362009,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 30, 6, 256915),
 'httpcompression/response_bytes': 7974757,
 'httpcompression/response_count': 22,
 'item_scraped_count': 210,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 21,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 7, 28, 36, 894906)}
2023-10-24 07:30:06 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:30:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:30:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:30:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:30:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:30:16 [scrapy.extensions.telnet] INFO: Telnet Password: bc8f0b50e3f0f212
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:30:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:30:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:30:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:31:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 07:32:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:32:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:32:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:32:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:32:12 [scrapy.extensions.telnet] INFO: Telnet Password: ee8e5b526be6a01c
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:32:12 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:32:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:32:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:38:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:38:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:38:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:38:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:38:32 [scrapy.extensions.telnet] INFO: Telnet Password: ae2029cabcc2f00c
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:38:32 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:38:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:38:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:38:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:38:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:38:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:38:51 [scrapy.extensions.telnet] INFO: Telnet Password: 888db44d1be4f347
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:38:51 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:38:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:38:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 07:39:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:39:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:39:23 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:39:23 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:39:23 [scrapy.extensions.telnet] INFO: Telnet Password: d7ff28a2af3afa5e
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:39:23 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:39:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:39:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-10-24 07:39:32 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 160 items (at 160 items/min)
2023-10-24 07:39:49 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:39:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:39:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:39:49 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:39:49 [scrapy.extensions.telnet] INFO: Telnet Password: c03c04f3043ae98b
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:39:49 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:39:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:39:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2023-10-24 07:39:51 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 80 items (at 80 items/min)
2023-10-24 07:40:23 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 70 items (at 70 items/min)
2023-10-24 07:40:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:40:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:40:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:40:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:40:28 [scrapy.extensions.telnet] INFO: Telnet Password: bedc82c251503c91
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:40:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:40:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:40:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6027
2023-10-24 07:40:32 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 13 pages/min), scraped 290 items (at 130 items/min)
2023-10-24 07:40:49 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 140 items (at 140 items/min)
2023-10-24 07:40:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:40:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:40:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:40:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:40:50 [scrapy.extensions.telnet] INFO: Telnet Password: feccb415dd605bf2
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:40:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:40:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:40:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6028
2023-10-24 07:40:51 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 17 pages/min), scraped 250 items (at 170 items/min)
2023-10-24 07:41:23 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 13 pages/min), scraped 200 items (at 130 items/min)
2023-10-24 07:41:28 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 140 items (at 140 items/min)
2023-10-24 07:41:32 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 19 pages/min), scraped 480 items (at 190 items/min)
2023-10-24 07:41:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:41:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3936113,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 182.654622,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 41, 35, 126523),
 'httpcompression/response_bytes': 18069948,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 38, 32, 471901)}
2023-10-24 07:41:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:41:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 41, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:41:43 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:41:43 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x190eeba3bd0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:41:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8811,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 1419940,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 53.113325,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 41, 43, 354209),
 'httpcompression/response_bytes': 6514800,
 'httpcompression/response_count': 18,
 'item_scraped_count': 170,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 17,
 'response_received_count': 18,
 'scheduler/dequeued': 18,
 'scheduler/dequeued/memory': 18,
 'scheduler/enqueued': 18,
 'scheduler/enqueued/memory': 18,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 7, 40, 50, 240884)}
2023-10-24 07:41:43 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:41:49 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 17 pages/min), scraped 310 items (at 170 items/min)
2023-10-24 07:41:51 [scrapy.extensions.logstats] INFO: Crawled 39 pages (at 14 pages/min), scraped 390 items (at 140 items/min)
2023-10-24 07:42:23 [scrapy.extensions.logstats] INFO: Crawled 36 pages (at 16 pages/min), scraped 360 items (at 160 items/min)
2023-10-24 07:42:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24855,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3937746,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 212.273575,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 23, 794195),
 'httpcompression/response_bytes': 18059905,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 38, 51, 520620)}
2023-10-24 07:42:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:42:28 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 24 pages/min), scraped 380 items (at 240 items/min)
2023-10-24 07:42:49 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 17 pages/min), scraped 480 items (at 170 items/min)
2023-10-24 07:42:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3936628,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 182.743873,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 51, 764218),
 'httpcompression/response_bytes': 18066868,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 39, 49, 20345)}
2023-10-24 07:42:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:42:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24826,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3935861,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 150.394355,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 58, 548812),
 'httpcompression/response_bytes': 18063390,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 40, 28, 154457)}
2023-10-24 07:42:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:43:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:43:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3934667,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 223.40661,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 43, 6, 970595),
 'httpcompression/response_bytes': 18073291,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 39, 23, 563985)}
2023-10-24 07:43:06 [scrapy.core.engine] INFO: Spider closed (finished)
