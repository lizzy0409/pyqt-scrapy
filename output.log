2023-10-22 17:58:11 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:58:11 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:11 [scrapy.extensions.telnet] INFO: Telnet Password: 9b5c7671d9d6f1cb
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:11 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:13 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 17:58:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17269,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3934665,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.257016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 0, 58, 23, 124289),
 'httpcompression/response_bytes': 18085934,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 50,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/AttributeError': 50,
 'start_time': datetime.datetime(2023, 10, 23, 0, 58, 11, 867273)}
2023-10-22 17:58:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 17:58:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:58:25 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:25 [scrapy.extensions.telnet] INFO: Telnet Password: eeeb5dc40f7e5543
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:25 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:26 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 17:58:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17249,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3932517,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 10.194672,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 0, 58, 36, 65623),
 'httpcompression/response_bytes': 18083233,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 50,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/AttributeError': 50,
 'start_time': datetime.datetime(2023, 10, 23, 0, 58, 25, 870951)}
2023-10-22 17:58:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 17:58:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': '11'}
2023-10-22 17:58:53 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:53 [scrapy.extensions.telnet] INFO: Telnet Password: 16111dc3457c0eb2
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:53 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:55 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:59:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:59:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:59:20 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:59:20 [scrapy.extensions.telnet] INFO: Telnet Password: 54552a52c6ff65e7
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:59:20 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:59:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:59:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:01:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:01:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:01:18 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:01:18 [scrapy.extensions.telnet] INFO: Telnet Password: 6c9f34180780b806
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:01:18 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:01:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:01:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:21 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:01:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:01:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:01:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': 'ff'}
2023-10-22 18:01:26 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:01:26 [scrapy.extensions.telnet] INFO: Telnet Password: 7728fde1c686152e
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:01:26 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:01:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:01:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:27 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:04:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:04:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:04:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:04:02 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:04:02 [scrapy.extensions.telnet] INFO: Telnet Password: 7ac2cb01b2926e75
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:04:02 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:04:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:04:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:04:04 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:04:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:04:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17092,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3937127,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.118511,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 4, 15, 626704),
 'httpcompression/response_bytes': 18096335,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 4, 2, 508193)}
2023-10-22 18:04:15 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 18:33:49 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:33:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:33:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:33:49 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:33:49 [scrapy.extensions.telnet] INFO: Telnet Password: d82c551d182cbe3d
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:33:49 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:33:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:33:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:33:50 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:34:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:34:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17108,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3952542,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 12.631892,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 34, 1, 991556),
 'httpcompression/response_bytes': 18116370,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 33, 49, 359664)}
2023-10-22 18:34:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 18:34:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:34:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:34:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:34:09 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:34:09 [scrapy.extensions.telnet] INFO: Telnet Password: 986d7f3d4ced0cd0
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:34:09 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:34:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:34:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:34:10 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:34:20 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:34:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17224,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3951674,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.07249,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 34, 20, 541469),
 'httpcompression/response_bytes': 18116581,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 34, 9, 468979)}
2023-10-22 18:34:20 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-23 23:27:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:27:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:27:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:27:25 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:27:25 [scrapy.extensions.telnet] INFO: Telnet Password: a03e9462d9653945
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:27:25 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:27:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:27:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:28:33 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:28:33 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:28:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:28:33 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:28:33 [scrapy.extensions.telnet] INFO: Telnet Password: d42157d56822e353
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:28:33 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:28:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:28:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:29:33 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 30 pages/min), scraped 290 items (at 290 items/min)
2023-10-23 23:29:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-23 23:29:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20595,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3938374,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 80.059072,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 23, 29, 53, 184323),
 'httpcompression/response_bytes': 18286442,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 23, 28, 33, 125251)}
2023-10-23 23:29:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-23 23:33:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:33:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:33:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:33:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:33:09 [scrapy.extensions.telnet] INFO: Telnet Password: 1a3d5ce2b3f45252
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:33:09 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:33:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:33:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:33:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-23 23:33:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79366,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 3.662685,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 23, 33, 13, 364840),
 'httpcompression/response_bytes': 354915,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 23, 23, 33, 9, 702155)}
2023-10-23 23:33:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 01:22:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:22:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:22:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:22:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:22:37 [scrapy.extensions.telnet] INFO: Telnet Password: 97f19cafc5a34e66
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:22:37 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:22:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:22:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:42:06 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 01:48:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:48:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:48:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:48:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:48:13 [scrapy.extensions.telnet] INFO: Telnet Password: 0ef9d5e3c3c939d1
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:48:13 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:48:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:48:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:48:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:48:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:48:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:13 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:49:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:13 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:50:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:50:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:50:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:50:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:50:39 [scrapy.extensions.telnet] INFO: Telnet Password: 4c3e4da91c3e6bfb
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:50:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:50:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:50:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:51:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:39 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:51:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:53:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:53:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:53:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:53:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:53:02 [scrapy.extensions.telnet] INFO: Telnet Password: 1efd2404680fa838
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:53:02 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:53:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:53:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:53:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:53:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:53:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:53:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:53:50 [scrapy.extensions.telnet] INFO: Telnet Password: 7b2199a09209ec1d
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:53:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:53:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:53:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:55:04 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:55:04 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:55:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:55:04 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:55:04 [scrapy.extensions.telnet] INFO: Telnet Password: 6fc0fb1af9d51637
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:55:04 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:55:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:55:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:56:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:56:09 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:57:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:57:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:57:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:57:39 [scrapy.extensions.telnet] INFO: Telnet Password: 6ea2c6cfb41aff55
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:57:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:57:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:58:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:58:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:58:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:58:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:58:56 [scrapy.extensions.telnet] INFO: Telnet Password: 589c824d0a5c582f
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:58:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:58:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:58:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:02:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:02:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:02:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:02:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:02:06 [scrapy.extensions.telnet] INFO: Telnet Password: d4a775acd308502d
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:02:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:02:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:02:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:02:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:02:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:02:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:02:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:02:35 [scrapy.extensions.telnet] INFO: Telnet Password: a1439efaf433ec0a
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:02:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:02:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:02:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:03:06 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:35 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:43 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:03:43 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:03:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:03:43 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:03:43 [scrapy.extensions.telnet] INFO: Telnet Password: 3b96fa9767c6704d
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:03:43 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:03:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:03:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:06 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:04:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:04:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:04:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:04:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:04:10 [scrapy.extensions.telnet] INFO: Telnet Password: 4d18c6b9a212e161
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:04:10 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:04:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:04:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:04:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:04:56 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x28373c8c7d0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 55, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 17,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 12,
 'downloader/request_bytes': 26083,
 'downloader/request_count': 68,
 'downloader/request_method_count/GET': 68,
 'downloader/response_bytes': 3936049,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 169.65009,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 4, 56, 252219),
 'httpcompression/response_bytes': 17994885,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 51,
 'log_count/INFO': 12,
 'log_count/WARNING': 6,
 'response_received_count': 51,
 'retry/count': 17,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 5,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 67,
 'scheduler/dequeued/memory': 67,
 'scheduler/enqueued': 67,
 'scheduler/enqueued/memory': 67,
 'spider_exceptions/FileNotFoundError': 50,
 'start_time': datetime.datetime(2023, 10, 24, 2, 2, 6, 602129)}
2023-10-24 02:04:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:05:15 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 02:05:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:05:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 7,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 7,
 'downloader/request_bytes': 22059,
 'downloader/request_count': 58,
 'downloader/request_method_count/GET': 58,
 'downloader/response_bytes': 3940135,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 73.337621,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 5, 23, 909371),
 'httpcompression/response_bytes': 18014876,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 48,
 'request_depth_max': 1,
 'response_received_count': 51,
 'retry/count': 7,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 7,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 57,
 'scheduler/dequeued/memory': 57,
 'scheduler/enqueued': 57,
 'scheduler/enqueued/memory': 57,
 'start_time': datetime.datetime(2023, 10, 24, 2, 4, 10, 571750)}
2023-10-24 02:05:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:50:15 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:15 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:15 [scrapy.extensions.telnet] INFO: Telnet Password: 2b55b3a10beac79e
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:15 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:50:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=150000&minPrice=35000&maxFloorArea=2000&minFloorArea=250&maxSiteArea=10000&minSiteArea=200&numParkingSpaces=1&energyEfficiency=1&keywords=factory%2Bshop&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:50:28 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:50:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34957,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2840838,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.434935,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 50, 28, 664032),
 'httpcompression/response_bytes': 9351115,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 50, 15, 229097)}
2023-10-24 02:50:28 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:50:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:48 [scrapy.extensions.telnet] INFO: Telnet Password: 8d681800b23e4a72
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:48 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:50:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:57 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:57 [scrapy.extensions.telnet] INFO: Telnet Password: 9c50c2a7dbef01af
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:57 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:51:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=150000&minPrice=35000&maxFloorArea=2000&minFloorArea=250&maxSiteArea=10000&minSiteArea=200&numParkingSpaces=1&energyEfficiency=1&keywords=factory&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:51:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:51:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34603,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2839029,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.885398,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 51, 11, 126514),
 'httpcompression/response_bytes': 9338333,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 50, 57, 241116)}
2023-10-24 02:51:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:51:19 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:51:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:51:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:51:19 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:51:19 [scrapy.extensions.telnet] INFO: Telnet Password: 81e53762eadb2501
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:51:19 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:51:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:51:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:51:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=1000000&numParkingSpaces=1&energyEfficiency=1&keywords=factory&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:51:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:51:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 30456,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2839072,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 16.002299,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 51, 35, 645678),
 'httpcompression/response_bytes': 9338119,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 51, 19, 643379)}
2023-10-24 02:51:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:51:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:51:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:51:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:51:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:51:58 [scrapy.extensions.telnet] INFO: Telnet Password: 8bffe712f76b64c3
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:51:58 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:51:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:51:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:52:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 29428,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2998434,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 14.283853,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 52, 12, 943884),
 'httpcompression/response_bytes': 10854209,
 'httpcompression/response_count': 50,
 'item_scraped_count': 16,
 'log_count/ERROR': 5,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 5,
 'start_time': datetime.datetime(2023, 10, 24, 2, 51, 58, 660031)}
2023-10-24 02:52:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:52:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:52:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:52:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:52:26 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:52:26 [scrapy.extensions.telnet] INFO: Telnet Password: 688f2bdc53f49ba6
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:52:26 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:52:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:52:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:52:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 507, in Client
    answer_challenge(c, authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 751, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 215, in recv_bytes
    buf = self._recv_bytes(maxlength)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:52:40 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x16df173d290>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 55, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 27995,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3949410,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.87832,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 52, 40, 684184),
 'httpcompression/response_bytes': 18136370,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 51,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/EOFError': 1,
 'spider_exceptions/FileNotFoundError': 49,
 'start_time': datetime.datetime(2023, 10, 24, 2, 52, 26, 805864)}
2023-10-24 02:52:40 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:53:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:53:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:53:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:53:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:53:47 [scrapy.extensions.telnet] INFO: Telnet Password: c41d9c7923c81ce4
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:53:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:53:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:53:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:54:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 29698,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3913133,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.353105,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 54, 0, 794794),
 'httpcompression/response_bytes': 18623381,
 'httpcompression/response_count': 50,
 'item_scraped_count': 192,
 'log_count/ERROR': 31,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 19,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 31,
 'start_time': datetime.datetime(2023, 10, 24, 2, 53, 47, 441689)}
2023-10-24 02:54:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 03:16:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:16:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:16:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:16:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:16:40 [scrapy.extensions.telnet] INFO: Telnet Password: 6ec5e4c8f5c6e849
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:16:40 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:16:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:16:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:16:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 03:16:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 21267,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3817741,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 12.43017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 3, 16, 52, 730155),
 'httpcompression/response_bytes': 17407705,
 'httpcompression/response_count': 50,
 'item_scraped_count': 356,
 'log_count/ERROR': 15,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 35,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 15,
 'start_time': datetime.datetime(2023, 10, 24, 3, 16, 40, 299985)}
2023-10-24 03:16:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 03:19:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:19:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:19:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:19:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:19:34 [scrapy.extensions.telnet] INFO: Telnet Password: 075ada4189015eec
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:19:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:19:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:19:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:19:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:19:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:19:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:19:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:19:47 [scrapy.extensions.telnet] INFO: Telnet Password: fff2f445b24a8940
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:19:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:19:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:19:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:19:59 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 03:19:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 19716,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3944105,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.488783,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 3, 19, 59, 445498),
 'httpcompression/response_bytes': 18024360,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 48,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 3, 19, 47, 956715)}
2023-10-24 03:19:59 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:52:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:52:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:52:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:52:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:52:30 [scrapy.extensions.telnet] INFO: Telnet Password: 07e15f4c6e558b22
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:52:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:52:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:52:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:52:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:52:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20517,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3602287,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 26.730949,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 52, 56, 915232),
 'httpcompression/response_bytes': 15407464,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 52, 30, 184283)}
2023-10-24 05:52:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:53:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:53:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:53:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:53:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:53:45 [scrapy.extensions.telnet] INFO: Telnet Password: e9f764025428e6db
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:53:45 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:53:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:53:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:53:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:54:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20516,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3601139,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 24.905852,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 54, 10, 843020),
 'httpcompression/response_bytes': 15407408,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 53, 45, 937168)}
2023-10-24 05:54:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:54:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:54:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:54:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:54:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:54:14 [scrapy.extensions.telnet] INFO: Telnet Password: 5c09fd50ca0d8a73
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:54:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:54:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:54:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:54:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:38 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:54:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20516,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3601170,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 24.419378,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 54, 38, 876401),
 'httpcompression/response_bytes': 15407409,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 54, 14, 457023)}
2023-10-24 05:54:38 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:54:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:54:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:54:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:54:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:54:44 [scrapy.extensions.telnet] INFO: Telnet Password: d895aa8a08e81314
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:54:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:54:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:54:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:55:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:55:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:55:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:55:14 [scrapy.extensions.telnet] INFO: Telnet Password: cb5730cc7101ebc6
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:55:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:55:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:55:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:55:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:55:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:55:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:55:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:55:36 [scrapy.extensions.telnet] INFO: Telnet Password: e6b2804945694fb2
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:55:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:55:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:55:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:55:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5013&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:56:04 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:56:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20463,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3480007,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 28.196052,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 56, 4, 626540),
 'httpcompression/response_bytes': 14510005,
 'httpcompression/response_count': 50,
 'item_scraped_count': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 30,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 5, 55, 36, 430488)}
2023-10-24 05:56:04 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:56:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:56:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:56:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:56:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:56:56 [scrapy.extensions.telnet] INFO: Telnet Password: 30cffb5742593986
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:56:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:56:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:56:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:59:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:59:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:59:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:59:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:59:06 [scrapy.extensions.telnet] INFO: Telnet Password: 6bf3ee038599a49d
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:59:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:59:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:59:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:59:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:00:59 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x15146cf0b10>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 56, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 06:00:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79799,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 25.333023,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 59, 32, 30419),
 'httpcompression/response_bytes': 351686,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 5, 59, 6, 697396)}
2023-10-24 06:00:59 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:01:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:01:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:01:23 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:01:23 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:01:23 [scrapy.extensions.telnet] INFO: Telnet Password: 0b702b032ee6893d
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:01:23 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:01:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:01:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:01:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 53, in parse
    yield scrapy.follow(url=next_page_url, callback=self.parse)
          ^^^^^^^^^^^^^
AttributeError: module 'scrapy' has no attribute 'follow'
2023-10-24 06:01:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:01:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79954,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 8.319788,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 1, 31, 504912),
 'httpcompression/response_bytes': 351932,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 1, 23, 185124)}
2023-10-24 06:01:34 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:04:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:04:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:04:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:04:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:04:16 [scrapy.extensions.telnet] INFO: Telnet Password: d5969e2b3adba7a8
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:04:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:04:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:04:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:04:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:04:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 608,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79116,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 13.915329,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 4, 30, 714968),
 'httpcompression/response_bytes': 342400,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 4, 16, 799639)}
2023-10-24 06:04:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:15:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:15:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:15:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:15:55 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:15:55 [scrapy.extensions.telnet] INFO: Telnet Password: 1e26c5442af7fe40
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:15:55 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:15:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:15:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:16:29 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:17:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 76434,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 33.736372,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 16, 29, 33460),
 'httpcompression/response_bytes': 343907,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 15, 55, 297088)}
2023-10-24 06:17:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:17:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:17:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:17:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:17:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:17:44 [scrapy.extensions.telnet] INFO: Telnet Password: 67654778a8b92c22
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:17:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:17:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:17:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:17:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:17:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79345,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 6.767828,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 17, 51, 29941),
 'httpcompression/response_bytes': 354391,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 17, 44, 262113)}
2023-10-24 06:17:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:18:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:18:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:18:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:18:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:18:06 [scrapy.extensions.telnet] INFO: Telnet Password: 094f52b356e19d3a
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:18:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:18:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:18:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:18:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:20:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79349,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 8.04743,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 18, 14, 329277),
 'httpcompression/response_bytes': 354391,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 18, 6, 281847)}
2023-10-24 06:20:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:23:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:23:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:23:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:23:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:23:39 [scrapy.extensions.telnet] INFO: Telnet Password: 1e371f6a07d02edd
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:23:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:23:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:23:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:23:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:23:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79905,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 14.886245,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 23, 54, 143867),
 'httpcompression/response_bytes': 363521,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1262,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 23, 39, 257622)}
2023-10-24 06:23:55 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:29:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:29:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:29:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:29:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:29:44 [scrapy.extensions.telnet] INFO: Telnet Password: 64d20a6fdb1abbd3
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:29:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:29:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:29:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:29:48 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:29:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79399,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.84203,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 29, 48, 379917),
 'httpcompression/response_bytes': 361397,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 29, 44, 537887)}
2023-10-24 06:29:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:31:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:31:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:31:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:31:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:31:14 [scrapy.extensions.telnet] INFO: Telnet Password: 8f2b9810bf4bccf8
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:31:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:31:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:31:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:31:17 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:31:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79799,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.644789,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 31, 17, 911182),
 'httpcompression/response_bytes': 361168,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 31, 14, 266393)}
2023-10-24 06:31:17 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:38:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:38:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:38:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:38:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:38:35 [scrapy.extensions.telnet] INFO: Telnet Password: 4b60057a5ae08282
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:38:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:38:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:38:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:39:35 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 19 pages/min), scraped 190 items (at 190 items/min)
2023-10-24 06:40:35 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 9 pages/min), scraped 280 items (at 90 items/min)
2023-10-24 06:41:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:41:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24373,
 'downloader/request_count': 49,
 'downloader/request_method_count/GET': 49,
 'downloader/response_bytes': 3852602,
 'downloader/response_count': 49,
 'downloader/response_status_count/200': 49,
 'elapsed_time_seconds': 175.715549,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 41, 31, 427751),
 'httpcompression/response_bytes': 17657395,
 'httpcompression/response_count': 49,
 'item_scraped_count': 490,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 48,
 'response_received_count': 49,
 'scheduler/dequeued': 49,
 'scheduler/dequeued/memory': 49,
 'scheduler/enqueued': 49,
 'scheduler/enqueued/memory': 49,
 'start_time': datetime.datetime(2023, 10, 24, 6, 38, 35, 712202)}
2023-10-24 06:41:31 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:42:29 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:42:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:42:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:42:29 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:42:29 [scrapy.extensions.telnet] INFO: Telnet Password: aefdb6543a0fc2aa
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:42:29 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:42:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:42:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:43:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=16> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 06:43:27 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:43:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8613,
 'downloader/request_count': 16,
 'downloader/request_method_count/GET': 16,
 'downloader/response_bytes': 1262116,
 'downloader/response_count': 16,
 'downloader/response_status_count/200': 16,
 'elapsed_time_seconds': 58.260414,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 43, 27, 616822),
 'httpcompression/response_bytes': 5918640,
 'httpcompression/response_count': 16,
 'item_scraped_count': 154,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 15,
 'response_received_count': 16,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 42, 29, 356408)}
2023-10-24 06:43:27 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:28:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:28:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:28:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:28:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:28:36 [scrapy.extensions.telnet] INFO: Telnet Password: 1d8f42c710cf81d8
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:28:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:28:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:28:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:29:36 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 15 pages/min), scraped 150 items (at 150 items/min)
2023-10-24 07:30:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    items['area'] = item['attributes']['area']
    ^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:30:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:30:06 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1befd9ac690>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 90, in close
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:30:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 10819,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 1738633,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'elapsed_time_seconds': 89.362009,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 30, 6, 256915),
 'httpcompression/response_bytes': 7974757,
 'httpcompression/response_count': 22,
 'item_scraped_count': 210,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 21,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 7, 28, 36, 894906)}
2023-10-24 07:30:06 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:30:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:30:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:30:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:30:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:30:16 [scrapy.extensions.telnet] INFO: Telnet Password: bc8f0b50e3f0f212
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:30:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:30:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:30:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:31:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 07:32:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:32:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:32:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:32:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:32:12 [scrapy.extensions.telnet] INFO: Telnet Password: ee8e5b526be6a01c
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:32:12 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:32:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:32:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:38:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:38:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:38:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:38:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:38:32 [scrapy.extensions.telnet] INFO: Telnet Password: ae2029cabcc2f00c
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:38:32 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:38:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:38:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:38:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:38:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:38:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:38:51 [scrapy.extensions.telnet] INFO: Telnet Password: 888db44d1be4f347
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:38:51 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:38:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:38:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 07:39:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:39:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:39:23 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:39:23 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:39:23 [scrapy.extensions.telnet] INFO: Telnet Password: d7ff28a2af3afa5e
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:39:23 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:39:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:39:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-10-24 07:39:32 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 160 items (at 160 items/min)
2023-10-24 07:39:49 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:39:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:39:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:39:49 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:39:49 [scrapy.extensions.telnet] INFO: Telnet Password: c03c04f3043ae98b
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:39:49 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:39:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:39:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2023-10-24 07:39:51 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 80 items (at 80 items/min)
2023-10-24 07:40:23 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 70 items (at 70 items/min)
2023-10-24 07:40:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:40:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:40:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:40:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:40:28 [scrapy.extensions.telnet] INFO: Telnet Password: bedc82c251503c91
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:40:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:40:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:40:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6027
2023-10-24 07:40:32 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 13 pages/min), scraped 290 items (at 130 items/min)
2023-10-24 07:40:49 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 140 items (at 140 items/min)
2023-10-24 07:40:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:40:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:40:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:40:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:40:50 [scrapy.extensions.telnet] INFO: Telnet Password: feccb415dd605bf2
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:40:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:40:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:40:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6028
2023-10-24 07:40:51 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 17 pages/min), scraped 250 items (at 170 items/min)
2023-10-24 07:41:23 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 13 pages/min), scraped 200 items (at 130 items/min)
2023-10-24 07:41:28 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 140 items (at 140 items/min)
2023-10-24 07:41:32 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 19 pages/min), scraped 480 items (at 190 items/min)
2023-10-24 07:41:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:41:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3936113,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 182.654622,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 41, 35, 126523),
 'httpcompression/response_bytes': 18069948,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 38, 32, 471901)}
2023-10-24 07:41:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:41:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 41, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:41:43 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:41:43 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x190eeba3bd0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:41:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8811,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 1419940,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 53.113325,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 41, 43, 354209),
 'httpcompression/response_bytes': 6514800,
 'httpcompression/response_count': 18,
 'item_scraped_count': 170,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 17,
 'response_received_count': 18,
 'scheduler/dequeued': 18,
 'scheduler/dequeued/memory': 18,
 'scheduler/enqueued': 18,
 'scheduler/enqueued/memory': 18,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 7, 40, 50, 240884)}
2023-10-24 07:41:43 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:41:49 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 17 pages/min), scraped 310 items (at 170 items/min)
2023-10-24 07:41:51 [scrapy.extensions.logstats] INFO: Crawled 39 pages (at 14 pages/min), scraped 390 items (at 140 items/min)
2023-10-24 07:42:23 [scrapy.extensions.logstats] INFO: Crawled 36 pages (at 16 pages/min), scraped 360 items (at 160 items/min)
2023-10-24 07:42:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24855,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3937746,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 212.273575,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 23, 794195),
 'httpcompression/response_bytes': 18059905,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 38, 51, 520620)}
2023-10-24 07:42:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:42:28 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 24 pages/min), scraped 380 items (at 240 items/min)
2023-10-24 07:42:49 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 17 pages/min), scraped 480 items (at 170 items/min)
2023-10-24 07:42:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3936628,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 182.743873,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 51, 764218),
 'httpcompression/response_bytes': 18066868,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 39, 49, 20345)}
2023-10-24 07:42:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:42:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24826,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3935861,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 150.394355,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 58, 548812),
 'httpcompression/response_bytes': 18063390,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 40, 28, 154457)}
2023-10-24 07:42:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:43:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:43:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3934667,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 223.40661,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 43, 6, 970595),
 'httpcompression/response_bytes': 18073291,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 39, 23, 563985)}
2023-10-24 07:43:06 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:55:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:55:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:55:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:55:41 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:55:41 [scrapy.extensions.telnet] INFO: Telnet Password: 932081b4673f6b53
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:55:41 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:55:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:55:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:56:26 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:56:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 56203,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7532566,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 44.566623,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 56, 26, 61231),
 'httpcompression/response_bytes': 26788379,
 'httpcompression/response_count': 110,
 'item_scraped_count': 200,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 7, 55, 41, 494608)}
2023-10-24 07:56:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:56:54 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:56:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:56:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:56:54 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:56:54 [scrapy.extensions.telnet] INFO: Telnet Password: eba441a2646bd872
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:56:54 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:56:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:56:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:57:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:57:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:57:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:57:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:57:20 [scrapy.extensions.telnet] INFO: Telnet Password: 01f34d4bb39988b9
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:57:20 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:57:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:57:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 07:57:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:57:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 56195,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7527321,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 42.06403,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 57, 36, 473926),
 'httpcompression/response_bytes': 26795643,
 'httpcompression/response_count': 110,
 'item_scraped_count': 200,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 7, 56, 54, 409896)}
2023-10-24 07:57:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:58:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-st-kilda-south-medical-centre-and-day-hospital-37-mitford-street-elwood-vic-3184-504384172> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 61, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-24 07:58:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 43, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-stage-8-eadie-court-brendale-qld-4500-504437472> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 61, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-12-24-26-hancock-way-baringa-qld-4551-504448392> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 61, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:05 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:58:05 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x2697ee44950>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 66, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 51467,
 'downloader/request_count': 101,
 'downloader/request_method_count/GET': 101,
 'downloader/response_bytes': 6924178,
 'downloader/response_count': 101,
 'downloader/response_status_count/200': 101,
 'elapsed_time_seconds': 44.757337,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 58, 5, 424793),
 'httpcompression/response_bytes': 24726357,
 'httpcompression/response_count': 101,
 'item_scraped_count': 178,
 'log_count/ERROR': 5,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 101,
 'scheduler/dequeued': 101,
 'scheduler/dequeued/memory': 101,
 'scheduler/enqueued': 101,
 'scheduler/enqueued/memory': 101,
 'spider_exceptions/BrokenPipeError': 4,
 'start_time': datetime.datetime(2023, 10, 24, 7, 57, 20, 667456)}
2023-10-24 07:58:05 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:58:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:58:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:58:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:58:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:58:44 [scrapy.extensions.telnet] INFO: Telnet Password: 55dce4ecba50e6f8
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:58:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:58:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:58:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:59:33 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:59:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 55686,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7465917,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 48.212655,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 59, 33, 207717),
 'httpcompression/response_bytes': 26585987,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 7, 58, 44, 995062)}
2023-10-24 07:59:33 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:19:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:19:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:19:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:19:03 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:19:03 [scrapy.extensions.telnet] INFO: Telnet Password: c1ef03b06400aed1
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:19:03 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:19:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:19:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:19:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:19:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:19:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:19:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:19:28 [scrapy.extensions.telnet] INFO: Telnet Password: cf6a9e1d008fbb9e
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:19:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:19:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:19:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 08:19:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:19:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 55704,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7469941,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 51.296644,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 19, 54, 600071),
 'httpcompression/response_bytes': 26629876,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 8, 19, 3, 303427)}
2023-10-24 08:19:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:20:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-57-stewart-street-richmond-vic-3121-504432064> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 88, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 08:20:28 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 109 pages/min), scraped 198 items (at 198 items/min)
2023-10-24 08:20:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-cannon-hill-qld-4170-504421940> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 88, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 08:20:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:20:32 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1f3ba844f50>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 93, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 08:20:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 56213,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7534202,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 64.309094,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 20, 32, 872617),
 'httpcompression/response_bytes': 26836349,
 'httpcompression/response_count': 110,
 'item_scraped_count': 198,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'spider_exceptions/BrokenPipeError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 8, 19, 28, 563523)}
2023-10-24 08:20:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:20:42 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:20:42 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:20:42 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:20:42 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:20:42 [scrapy.extensions.telnet] INFO: Telnet Password: 54f2f333df081684
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:20:42 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:20:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:20:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:20:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=3000000&minPrice=250000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:20:46 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:20:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 327,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 78577,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 4.084264,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 20, 46, 257224),
 'httpcompression/response_bytes': 361668,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 20, 42, 172960)}
2023-10-24 08:20:49 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:21:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:21:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:21:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:21:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:21:45 [scrapy.extensions.telnet] INFO: Telnet Password: 01cd845edf394c13
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:21:45 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:21:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:21:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:22:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:22:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:22:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:22:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:22:46 [scrapy.extensions.telnet] INFO: Telnet Password: 167e4ab8bf7bc38e
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:22:46 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:22:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:22:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:22:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=50000000&minPrice=100000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:22:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:22:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 328,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 80933,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 7.230053,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 22, 53, 698750),
 'httpcompression/response_bytes': 377578,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 22, 46, 468697)}
2023-10-24 08:22:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:23:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:23:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:23:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:23:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:23:44 [scrapy.extensions.telnet] INFO: Telnet Password: 3d82a83fb5d03318
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:23:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:23:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:23:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:24:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=50000000&minPrice=200000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:24:02 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:24:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 328,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 80119,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 18.207429,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 24, 2, 917116),
 'httpcompression/response_bytes': 377538,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 23, 44, 709687)}
2023-10-24 08:24:02 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:24:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:24:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:24:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:24:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:24:35 [scrapy.extensions.telnet] INFO: Telnet Password: eabfa325ec91b719
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:24:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:24:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:24:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:24:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=50000000&minPrice=200000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:24:57 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:24:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 328,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 80706,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 22.16591,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 24, 57, 511091),
 'httpcompression/response_bytes': 377578,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 24, 35, 345181)}
2023-10-24 08:24:57 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:25:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:25:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:25:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:25:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:25:16 [scrapy.extensions.telnet] INFO: Telnet Password: 62ae9c9416af68d9
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:25:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:25:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:25:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:26:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:26:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 60210,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7544917,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 50.688079,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 26, 7, 25861),
 'httpcompression/response_bytes': 26873973,
 'httpcompression/response_count': 110,
 'item_scraped_count': 200,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 8, 25, 16, 337782)}
2023-10-24 08:26:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:32:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:32:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:32:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:32:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:32:34 [scrapy.extensions.telnet] INFO: Telnet Password: ae377a1cff76af2d
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:32:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:32:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:32:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:33:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:33:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 59612,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7476976,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 49.038763,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 33, 23, 105230),
 'httpcompression/response_bytes': 26652173,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 8, 32, 34, 66467)}
2023-10-24 08:33:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:36:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:36:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:36:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:36:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:36:37 [scrapy.extensions.telnet] INFO: Telnet Password: 1188626b1e043b09
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:36:37 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:36:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:36:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:37:01 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:37:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:37:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:37:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:37:01 [scrapy.extensions.telnet] INFO: Telnet Password: 653e6b41058f5ab8
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:37:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:37:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:37:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:37:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:37:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 57473,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7466003,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 57.198611,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 37, 58, 979328),
 'httpcompression/response_bytes': 26611007,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 8, 37, 1, 780717)}
2023-10-24 08:37:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:50:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:50:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:50:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:50:53 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:50:53 [scrapy.extensions.telnet] INFO: Telnet Password: 699d982202fd3601
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:50:53 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:50:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:50:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:51:53 [scrapy.extensions.logstats] INFO: Crawled 126 pages (at 126 pages/min), scraped 243 items (at 243 items/min)
2023-10-24 08:52:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:52:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:52:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:52:24 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:52:24 [scrapy.extensions.telnet] INFO: Telnet Password: 4e96442e113b1a27
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:52:24 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:52:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:52:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 08:52:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:52:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 121913,
 'downloader/request_count': 220,
 'downloader/request_method_count/GET': 220,
 'downloader/response_bytes': 14918866,
 'downloader/response_count': 220,
 'downloader/response_status_count/200': 220,
 'elapsed_time_seconds': 97.546085,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 52, 30, 710858),
 'httpcompression/response_bytes': 52481655,
 'httpcompression/response_count': 220,
 'item_scraped_count': 400,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 20,
 'response_received_count': 220,
 'scheduler/dequeued': 220,
 'scheduler/dequeued/memory': 220,
 'scheduler/enqueued': 220,
 'scheduler/enqueued/memory': 220,
 'start_time': datetime.datetime(2023, 10, 24, 8, 50, 53, 164773)}
2023-10-24 08:52:30 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:53:24 [scrapy.extensions.logstats] INFO: Crawled 112 pages (at 112 pages/min), scraped 211 items (at 211 items/min)
2023-10-24 08:53:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:53:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:53:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:53:41 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:53:41 [scrapy.extensions.telnet] INFO: Telnet Password: df2ee8c119b35647
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:53:41 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:53:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:53:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:55:01 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:55:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:55:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:55:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:55:01 [scrapy.extensions.telnet] INFO: Telnet Password: 934249599af40e14
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:55:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:55:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:55:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:00:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:00:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:00:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:00:59 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:00:59 [scrapy.extensions.telnet] INFO: Telnet Password: 85a169f8b568f341
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:00:59 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:00:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:00:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:01:59 [scrapy.extensions.logstats] INFO: Crawled 92 pages (at 92 pages/min), scraped 172 items (at 172 items/min)
2023-10-24 09:02:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:02:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:02:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:02:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:02:14 [scrapy.extensions.telnet] INFO: Telnet Password: 5bdf55c436e0e114
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:02:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:02:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:02:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:02:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=400000&page=1&minPrice=200001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=400000&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '200001.0'
2023-10-24 09:02:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=200000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=400000&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '200000.0'
2023-10-24 09:03:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:03:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 61463,
 'downloader/request_count': 111,
 'downloader/request_method_count/GET': 111,
 'downloader/response_bytes': 7435404,
 'downloader/response_count': 111,
 'downloader/response_status_count/200': 111,
 'elapsed_time_seconds': 46.086581,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 3, 0, 847496),
 'httpcompression/response_bytes': 25959916,
 'httpcompression/response_count': 111,
 'item_scraped_count': 198,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 111,
 'scheduler/dequeued': 111,
 'scheduler/dequeued/memory': 111,
 'scheduler/enqueued': 111,
 'scheduler/enqueued/memory': 111,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 2, 14, 760915)}
2023-10-24 09:03:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:05:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:05:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:05:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:05:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:05:09 [scrapy.extensions.telnet] INFO: Telnet Password: 9743ea6090947823
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:05:09 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:05:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:05:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:06:09 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 30 pages/min), scraped 57 items (at 57 items/min)
2023-10-24 09:07:45 [scrapy.extensions.logstats] INFO: Crawled 42 pages (at 12 pages/min), scraped 79 items (at 22 items/min)
2023-10-24 09:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=250000&page=1&minPrice=125001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=250000&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '125001.0'
2023-10-24 09:08:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=125000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=250000&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '125000.0'
2023-10-24 09:08:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:08:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:08:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:08:41 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:08:41 [scrapy.extensions.telnet] INFO: Telnet Password: ab9cb6dd8ce05f0f
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:08:41 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:08:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:08:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:09:41 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 10 items (at 10 items/min)
2023-10-24 09:10:41 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 11 pages/min), scraped 21 items (at 11 items/min)
2023-10-24 09:11:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=100000&page=1&minPrice=50001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=100000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '50001.0'
2023-10-24 09:11:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=50000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=100000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '50000.0'
2023-10-24 09:11:05 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:11:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:11:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:11:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:11:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:11:22 [scrapy.extensions.telnet] INFO: Telnet Password: c6e0c896633ba2ed
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:11:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:11:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:11:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:11:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:11:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:11:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:11:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:11:36 [scrapy.extensions.telnet] INFO: Telnet Password: a9c583827990add9
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:11:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:11:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:11:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:12:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:12:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:12:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:12:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:12:22 [scrapy.extensions.telnet] INFO: Telnet Password: c1dce2fb39f61d3f
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:12:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:12:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:12:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:13:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:14:05 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:14:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1811,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 259280,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'elapsed_time_seconds': 102.172517,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 14, 5, 116471),
 'httpcompression/response_bytes': 867610,
 'httpcompression/response_count': 4,
 'item_scraped_count': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 10, 24, 9, 12, 22, 943954)}
2023-10-24 09:14:05 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:14:17 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:14:17 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:14:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:14:17 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:14:17 [scrapy.extensions.telnet] INFO: Telnet Password: b08353e622b98d05
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:14:17 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:14:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:14:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:15:34 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 2 items (at 2 items/min)
2023-10-24 09:15:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1000000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000000.0'
2023-10-24 09:16:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000&minPrice=1000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000001.0'
2023-10-24 09:16:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:16:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2546,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 315745,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 103.051112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 16, 0, 545676),
 'httpcompression/response_bytes': 1063386,
 'httpcompression/response_count': 5,
 'item_scraped_count': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 14, 17, 494564)}
2023-10-24 09:16:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:17:15 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:17:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:17:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:17:15 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:17:15 [scrapy.extensions.telnet] INFO: Telnet Password: 9294765fa695cb26
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:17:15 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:17:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:17:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:18:44 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 2 items (at 2 items/min)
2023-10-24 09:19:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000&minPrice=1000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000001.0'
2023-10-24 09:19:19 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 4 pages/min), scraped 3 items (at 1 items/min)
2023-10-24 09:20:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:20:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:20:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:20:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:20:20 [scrapy.extensions.telnet] INFO: Telnet Password: f98446f8f40aad66
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:20:20 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:20:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:20:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:22:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:22:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:22:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:22:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:22:22 [scrapy.extensions.telnet] INFO: Telnet Password: a1a44bc83b04b820
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:22:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:22:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:22:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:23:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '25000000.0'
2023-10-24 09:23:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000&minPrice=25000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '25000001.0'
2023-10-24 09:23:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:23:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3064,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 393710,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 48.428378,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 23, 10, 739065),
 'httpcompression/response_bytes': 1375685,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 22, 22, 310687)}
2023-10-24 09:23:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:24:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:24:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:24:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:24:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:24:14 [scrapy.extensions.telnet] INFO: Telnet Password: 9824fe61e4f5fe9e
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:24:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:24:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:24:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:24:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 54, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 09:24:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-penola-sa-5277-504405824> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:24:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:24:56 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x284f5e1e250>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 122, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:24:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 830,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 128159,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 42.142727,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 24, 56, 313626),
 'httpcompression/response_bytes': 436960,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/BrokenPipeError': 1,
 'spider_exceptions/EOFError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 9, 24, 14, 170899)}
2023-10-24 09:24:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:25:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:25:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:25:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:25:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:25:34 [scrapy.extensions.telnet] INFO: Telnet Password: cdfc2b17708ab247
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:25:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:25:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:25:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:26:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000&minPrice=1000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000001.0'
2023-10-24 09:26:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:26:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:26:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:26:18 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:26:18 [scrapy.extensions.telnet] INFO: Telnet Password: 1eb9b6daf06fa9d5
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:26:18 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:26:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:26:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:28:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:28:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:28:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:28:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:28:06 [scrapy.extensions.telnet] INFO: Telnet Password: 1d8fdaede86c5ec9
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:28:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:28:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:28:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:28:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1500000&minPrice=750001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '750001.0'
2023-10-24 09:28:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=750000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '750000.0'
2023-10-24 09:28:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:28:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2540,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 315617,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 24.027487,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 28, 30, 915001),
 'httpcompression/response_bytes': 1063356,
 'httpcompression/response_count': 5,
 'item_scraped_count': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 28, 6, 887514)}
2023-10-24 09:28:30 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:29:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:29:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:29:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:29:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: 2da219a033252788
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:29:13 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:29:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:29:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:30:15 [scrapy.extensions.logstats] INFO: Crawled 76 pages (at 76 pages/min), scraped 135 items (at 135 items/min)
2023-10-24 09:30:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=175000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000&page=8)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '175000.0'
2023-10-24 09:30:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000&page=1&minPrice=175001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000&page=8)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '175001.0'
2023-10-24 09:30:19 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:30:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 44819,
 'downloader/request_count': 81,
 'downloader/request_method_count/GET': 81,
 'downloader/response_bytes': 5416513,
 'downloader/response_count': 81,
 'downloader/response_status_count/200': 81,
 'elapsed_time_seconds': 66.381128,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 30, 19, 794434),
 'httpcompression/response_bytes': 18936662,
 'httpcompression/response_count': 81,
 'item_scraped_count': 142,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 8,
 'response_received_count': 81,
 'scheduler/dequeued': 81,
 'scheduler/dequeued/memory': 81,
 'scheduler/enqueued': 81,
 'scheduler/enqueued/memory': 81,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 29, 13, 413306)}
2023-10-24 09:30:19 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:30:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:30:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:30:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:30:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:30:40 [scrapy.extensions.telnet] INFO: Telnet Password: 470f0a78c26dea99
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:30:40 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:30:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:30:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:31:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 54, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 09:31:59 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 4 items (at 4 items/min)
2023-10-24 09:31:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lots-1-15-hornby-road-port-broughton-sa-5522-504410552> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:32:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lots-20-21-inglis-circuit-gillman-sa-5013-504438988> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:32:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-totness-trading-hub-lots-14-16-innovation-drive-totness-sa-5250-504137763> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:32:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-8-183-tynte-street-north-adelaide-sa-5006-504386812> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:35:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:35:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:35:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:35:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:35:09 [scrapy.extensions.telnet] INFO: Telnet Password: 92d2e2677db42125
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:35:09 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:35:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:35:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:35:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000&minPrice=5000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '5000001.0'
2023-10-24 09:35:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '5000000.0'
2023-10-24 09:35:20 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:35:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3057,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 386291,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 10.78922,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 35, 20, 656740),
 'httpcompression/response_bytes': 1314104,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 35, 9, 867520)}
2023-10-24 09:35:20 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:36:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:36:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:36:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:36:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:36:01 [scrapy.extensions.telnet] INFO: Telnet Password: 511022abb2ba2c65
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:36:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:36:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:36:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:36:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000&minPrice=2500001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500001.0'
2023-10-24 09:36:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2500000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500000.0'
2023-10-24 09:36:16 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:36:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3055,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 383905,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 15.768391,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 36, 16, 782512),
 'httpcompression/response_bytes': 1295499,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 36, 1, 14121)}
2023-10-24 09:36:16 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:36:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:36:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:36:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:36:24 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:36:24 [scrapy.extensions.telnet] INFO: Telnet Password: d400acbb9e5f8eaf
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:36:24 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:36:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:36:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:36:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000&minPrice=2500001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500001.0'
2023-10-24 09:36:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2500000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500000.0'
2023-10-24 09:36:42 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:36:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3055,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 383913,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 17.907409,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 36, 42, 824749),
 'httpcompression/response_bytes': 1295503,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 36, 24, 917340)}
2023-10-24 09:36:42 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:37:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:37:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:37:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:37:03 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:37:03 [scrapy.extensions.telnet] INFO: Telnet Password: 10f091f643b8d828
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:37:03 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:37:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:37:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:37:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000&minPrice=12500001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '12500001.0'
2023-10-24 09:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=12500000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '12500000.0'
2023-10-24 09:39:07 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 6 items (at 6 items/min)
2023-10-24 09:39:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:39:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3064,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 386918,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 124.175775,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 39, 7, 654169),
 'httpcompression/response_bytes': 1317077,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 37, 3, 478394)}
2023-10-24 09:39:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:39:29 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:39:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:39:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:39:29 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:39:29 [scrapy.extensions.telnet] INFO: Telnet Password: 719e8f0a07c39354
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:39:29 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:39:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:39:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:40:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 71, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:40:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:40:01 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1eaac9d8c90>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 130, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:40:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3058,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 386479,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 31.459315,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 40, 1, 330339),
 'httpcompression/response_bytes': 1314139,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 9, 39, 29, 871024)}
2023-10-24 09:40:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:40:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:40:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:40:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:40:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:40:36 [scrapy.extensions.telnet] INFO: Telnet Password: 96350ca6d7bde006
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:40:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:40:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:40:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:42:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 71, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 09:42:41 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 4 items (at 4 items/min)
2023-10-24 09:42:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-penola-sa-5277-504405824> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 125, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:42:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-must-coonawarra-apartments-cameron-s-cottage-126-church-street-penola-sa-5277-504216852> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 125, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:44:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:44:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:44:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:44:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:44:02 [scrapy.extensions.telnet] INFO: Telnet Password: 8b5a9bb0127d6a7c
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:44:02 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:44:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:44:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:45:02 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 8 items (at 8 items/min)
2023-10-24 09:46:11 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:46:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:46:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:46:11 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:46:11 [scrapy.extensions.telnet] INFO: Telnet Password: 513c918356db1b20
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:46:11 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:46:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:46:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:46:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:46:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 13873,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 1504179,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'elapsed_time_seconds': 42.813879,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 46, 54, 40903),
 'httpcompression/response_bytes': 5206615,
 'httpcompression/response_count': 24,
 'item_scraped_count': 13,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 24,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2023, 10, 24, 9, 46, 11, 227024)}
2023-10-24 09:46:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:48:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:48:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:48:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:48:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:48:31 [scrapy.extensions.telnet] INFO: Telnet Password: c6d938a038cf0f55
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:48:31 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:48:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:48:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:48:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:48:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:48:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:48:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:48:50 [scrapy.extensions.telnet] INFO: Telnet Password: 32bf11b9f81b1a63
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:48:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:48:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:48:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:49:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:49:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:49:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:49:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:49:09 [scrapy.extensions.telnet] INFO: Telnet Password: 7f1e6d82f1cd5c5c
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:49:09 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:49:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:49:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:49:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:49:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 16501,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 1427577,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'elapsed_time_seconds': 48.890947,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 49, 58, 609674),
 'httpcompression/response_bytes': 4769678,
 'httpcompression/response_count': 24,
 'item_scraped_count': 13,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 24,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2023, 10, 24, 9, 49, 9, 718727)}
2023-10-24 09:49:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:09:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:09:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:09:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:09:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:09:28 [scrapy.extensions.telnet] INFO: Telnet Password: 36db6457e42fc8a5
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:09:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:09:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:09:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:10:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:10:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 33783,
 'downloader/request_count': 49,
 'downloader/request_method_count/GET': 49,
 'downloader/response_bytes': 2954370,
 'downloader/response_count': 49,
 'downloader/response_status_count/200': 49,
 'elapsed_time_seconds': 40.260189,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 10, 8, 849628),
 'httpcompression/response_bytes': 10057701,
 'httpcompression/response_count': 49,
 'item_scraped_count': 52,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 49,
 'scheduler/dequeued': 49,
 'scheduler/dequeued/memory': 49,
 'scheduler/enqueued': 49,
 'scheduler/enqueued/memory': 49,
 'start_time': datetime.datetime(2023, 10, 24, 10, 9, 28, 589439)}
2023-10-24 10:10:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:14:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:14:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:14:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:14:07 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:14:07 [scrapy.extensions.telnet] INFO: Telnet Password: 3f4afefc98e07218
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:14:07 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:14:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:14:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:15:07 [scrapy.extensions.logstats] INFO: Crawled 141 pages (at 141 pages/min), scraped 266 items (at 266 items/min)
2023-10-24 10:15:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:15:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:15:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:15:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:15:22 [scrapy.extensions.telnet] INFO: Telnet Password: 322ed7cdd58cc0df
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:15:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:15:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:15:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 10:15:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-24 10:15:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:15:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 113056,
 'downloader/request_count': 220,
 'downloader/request_method_count/GET': 220,
 'downloader/response_bytes': 14989195,
 'downloader/response_count': 220,
 'downloader/response_status_count/200': 219,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 89.39871,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 15, 36, 754655),
 'httpcompression/response_bytes': 53339006,
 'httpcompression/response_count': 220,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 399,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 20,
 'response_received_count': 220,
 'scheduler/dequeued': 220,
 'scheduler/dequeued/memory': 220,
 'scheduler/enqueued': 220,
 'scheduler/enqueued/memory': 220,
 'start_time': datetime.datetime(2023, 10, 24, 10, 14, 7, 355945)}
2023-10-24 10:15:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:15:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:15:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:15:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:15:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:15:50 [scrapy.extensions.telnet] INFO: Telnet Password: 3248841a6882b0e9
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:15:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:15:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:15:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:16:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:16:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:16:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:16:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:16:30 [scrapy.extensions.telnet] INFO: Telnet Password: a3193824353f3dab
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:16:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:16:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:16:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:17:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:17:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:17:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:17:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:17:20 [scrapy.extensions.telnet] INFO: Telnet Password: 31dabc8a84741a70
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:17:20 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:17:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:17:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:19:17 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:19:17 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:19:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:19:17 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:19:17 [scrapy.extensions.telnet] INFO: Telnet Password: 2af15132745499fa
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:19:17 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:19:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:19:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:20:08 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:20:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:20:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:20:08 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:20:08 [scrapy.extensions.telnet] INFO: Telnet Password: 808ef195b4fc3713
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:20:08 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:20:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:20:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 10:20:17 [scrapy.extensions.logstats] INFO: Crawled 107 pages (at 107 pages/min), scraped 196 items (at 196 items/min)
2023-10-24 10:20:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-3-5-foster-street-leichhardt-nsw-2040-504416788> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-24 10:20:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-39-dalton-street-kippa-ring-qld-4021-504443036> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-level-1-881-collins-street-docklands-vic-3008-504422116> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-5-charlotte-street-dardanup-wa-6236-504389544> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-45-riversdale-road-newtown-vic-3220-504422108> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-44-cubitt-street-cremorne-vic-3121-504194655> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-31-park-street-st-kilda-west-vic-3182-504405688> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 54, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-mount-gambier-business-park-lot-13-riddoch-highway-suttontown-sa-5291-504395240> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lots-1-2-235-237-pirie-street-adelaide-sa-5000-504416784> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-luton-house-2-luton-lane-hawthorn-vic-3122-504443020> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-union-hotel-271-pacific-highway-north-sydney-nsw-2060-504400544> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-7-eleven-carl-s-jr-357-brisbane-street-west-ipswich-qld-4305-504432144> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-253-255-259-261-and-265-pacific-highway-north-sydney-nsw-2060-504384232> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-38-42-winnima-circuit-pemulwuy-nsw-2145-504427256> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:45 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:20:45 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1a37342bd50>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 131, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34501,
 'downloader/request_count': 68,
 'downloader/request_method_count/GET': 68,
 'downloader/response_bytes': 4668761,
 'downloader/response_count': 68,
 'downloader/response_status_count/200': 68,
 'elapsed_time_seconds': 36.187257,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 20, 45, 28588),
 'httpcompression/response_bytes': 16754603,
 'httpcompression/response_count': 68,
 'item_scraped_count': 107,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 7,
 'response_received_count': 68,
 'scheduler/dequeued': 68,
 'scheduler/dequeued/memory': 68,
 'scheduler/enqueued': 68,
 'scheduler/enqueued/memory': 68,
 'spider_exceptions/BrokenPipeError': 15,
 'start_time': datetime.datetime(2023, 10, 24, 10, 20, 8, 841331)}
2023-10-24 10:20:45 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:21:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-24 10:21:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:21:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 112480,
 'downloader/request_count': 219,
 'downloader/request_method_count/GET': 219,
 'downloader/response_bytes': 14916024,
 'downloader/response_count': 219,
 'downloader/response_status_count/200': 218,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 117.672814,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 21, 15, 602940),
 'httpcompression/response_bytes': 53081003,
 'httpcompression/response_count': 219,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 398,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 20,
 'response_received_count': 219,
 'scheduler/dequeued': 219,
 'scheduler/dequeued/memory': 219,
 'scheduler/enqueued': 219,
 'scheduler/enqueued/memory': 219,
 'start_time': datetime.datetime(2023, 10, 24, 10, 19, 17, 930126)}
2023-10-24 10:21:15 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:21:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:21:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:21:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:21:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:21:47 [scrapy.extensions.telnet] INFO: Telnet Password: cba0e08b1afe6733
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:21:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:21:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:21:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:22:47 [scrapy.extensions.logstats] INFO: Crawled 121 pages (at 121 pages/min), scraped 229 items (at 229 items/min)
2023-10-24 10:23:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:23:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:23:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:23:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:23:13 [scrapy.extensions.telnet] INFO: Telnet Password: 571cb283f90d40dc
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:23:13 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:23:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:23:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:23:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:23:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3918,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 569993,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 9.62345,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 23, 23, 583826),
 'httpcompression/response_bytes': 2165669,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 23, 13, 960376)}
2023-10-24 10:23:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:24:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:24:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:24:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:24:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:24:00 [scrapy.extensions.telnet] INFO: Telnet Password: b53501c3a2f9893f
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:24:00 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:24:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:24:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:24:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:24:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4089,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 570306,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 12.990369,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 24, 13, 682219),
 'httpcompression/response_bytes': 2168330,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 24, 0, 691850)}
2023-10-24 10:24:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:25:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:25:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:25:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:25:55 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:25:55 [scrapy.extensions.telnet] INFO: Telnet Password: 324543728220dbcf
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:25:55 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:25:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:25:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:26:09 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:26:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4080,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 569607,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 14.55146,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 26, 9, 969568),
 'httpcompression/response_bytes': 2169036,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 25, 55, 418108)}
2023-10-24 10:26:09 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:27:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:27:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:27:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:27:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:27:10 [scrapy.extensions.telnet] INFO: Telnet Password: cd76dcc8a46a4104
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:27:10 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:27:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:27:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:27:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:27:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4080,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 569601,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 22.934795,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 27, 32, 952879),
 'httpcompression/response_bytes': 2169037,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 27, 10, 18084)}
2023-10-24 10:27:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:27:54 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:27:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:27:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:27:54 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:27:54 [scrapy.extensions.telnet] INFO: Telnet Password: 9bbacce20417f3ce
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:27:54 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:27:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:27:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:34:08 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 6 items (at 6 items/min)
2023-10-24 10:34:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:34:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 5,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 5,
 'downloader/request_bytes': 6618,
 'downloader/request_count': 13,
 'downloader/request_method_count/GET': 13,
 'downloader/response_bytes': 569156,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 380.159192,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 34, 14, 610642),
 'httpcompression/response_bytes': 2168872,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'retry/count': 5,
 'retry/reason_count/twisted.internet.error.TimeoutError': 5,
 'scheduler/dequeued': 13,
 'scheduler/dequeued/memory': 13,
 'scheduler/enqueued': 13,
 'scheduler/enqueued/memory': 13,
 'start_time': datetime.datetime(2023, 10, 24, 10, 27, 54, 451450)}
2023-10-24 10:34:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:35:27 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:35:27 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:35:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:35:27 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:35:27 [scrapy.extensions.telnet] INFO: Telnet Password: 413502182a6ee1b9
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:35:27 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:35:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:35:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:35:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:35:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:35:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:35:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:35:40 [scrapy.extensions.telnet] INFO: Telnet Password: 587b0e44efef08d0
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:35:40 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:35:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:35:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:36:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:36:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 30679,
 'downloader/request_count': 59,
 'downloader/request_method_count/GET': 59,
 'downloader/response_bytes': 4000932,
 'downloader/response_count': 59,
 'downloader/response_status_count/200': 59,
 'elapsed_time_seconds': 27.54845,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 36, 8, 332580),
 'httpcompression/response_bytes': 13950895,
 'httpcompression/response_count': 59,
 'item_scraped_count': 106,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 59,
 'scheduler/dequeued': 59,
 'scheduler/dequeued/memory': 59,
 'scheduler/enqueued': 59,
 'scheduler/enqueued/memory': 59,
 'start_time': datetime.datetime(2023, 10, 24, 10, 35, 40, 784130)}
2023-10-24 10:36:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:36:19 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:36:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:36:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:36:19 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:36:19 [scrapy.extensions.telnet] INFO: Telnet Password: 8a5f2a9eaf9d82ca
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:36:19 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:36:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:36:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:37:19 [scrapy.extensions.logstats] INFO: Crawled 125 pages (at 125 pages/min), scraped 153 items (at 153 items/min)
2023-10-24 10:37:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:37:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 88994,
 'downloader/request_count': 154,
 'downloader/request_method_count/GET': 154,
 'downloader/response_bytes': 9700278,
 'downloader/response_count': 154,
 'downloader/response_status_count/200': 154,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 71.267328,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 37, 30, 462733),
 'httpcompression/response_bytes': 33316379,
 'httpcompression/response_count': 154,
 'item_scraped_count': 168,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 22,
 'response_received_count': 154,
 'scheduler/dequeued': 154,
 'scheduler/dequeued/memory': 154,
 'scheduler/enqueued': 154,
 'scheduler/enqueued/memory': 154,
 'start_time': datetime.datetime(2023, 10, 24, 10, 36, 19, 195405)}
2023-10-24 10:37:30 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:39:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:39:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:39:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:39:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:39:16 [scrapy.extensions.telnet] INFO: Telnet Password: 3d043904a64121f2
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:39:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:39:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:39:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:40:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2789063&minPrice=2778517&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2789063&minPrice=2767970&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 10:40:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1702735&minPrice=1697463&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1702735&minPrice=1692189&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2767969&minPrice=2762697&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2767969&minPrice=2757423&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2741602&minPrice=2736330&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2736330&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1697462&minPrice=1692189&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1702735&minPrice=1692189&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2731056&minPrice=2725783&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2736329&minPrice=2725783&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2741603&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2736330&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2673047&minPrice=2667775&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2673047&minPrice=2662501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2978907&minPrice=2973635&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2978907&minPrice=2968361&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2736329&minPrice=2731057&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2736329&minPrice=2725783&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2973634&minPrice=2968361&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2978907&minPrice=2968361&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2667774&minPrice=2662501&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2673047&minPrice=2662501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2488478&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2483205&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2488477&minPrice=2483205&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2483205&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2752149&minPrice=2746876&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2757422&minPrice=2746876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2778516&minPrice=2773244&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2778516&minPrice=2767970&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2757422&minPrice=2752150&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2757422&minPrice=2746876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2773243&minPrice=2767970&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2778516&minPrice=2767970&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2968360&minPrice=2963088&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2968360&minPrice=2957814&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2963087&minPrice=2957814&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2968360&minPrice=2957814&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2762696&minPrice=2757423&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2767969&minPrice=2757423&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.extensions.logstats] INFO: Crawled 161 pages (at 161 pages/min), scraped 465 items (at 465 items/min)
2023-10-24 10:40:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2483204&minPrice=2472658&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2472658&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2008594&minPrice=1998048&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2008594&minPrice=1987501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=3000000&minPrice=2989455&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=3000000&minPrice=2978908&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lot-233-commercial-hub-impressions-drive-eglinton-wa-6034-504409692> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2704688&minPrice=2683595&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-11-ellen-street-subiaco-wa-6008-504398556> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-68-paramount-drive-wangara-wa-6065-504393728> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1998047&minPrice=1987501&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2008594&minPrice=1987501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-139-stirling-street-perth-wa-6000-504390304> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-3571-great-northern-highway-muchea-wa-6501-503217078> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2704689&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2989454&minPrice=2978908&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=3000000&minPrice=2978908&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2725782&minPrice=2704689&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2704689&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-10-william-street-york-wa-6302-504447160> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-civic-heart-1-mends-street-south-perth-wa-6151-503645862> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2957813&minPrice=2915626&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-61-york-street-subiaco-wa-6008-504353496> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2831250&minPrice=2789064&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-244-york-street-albany-wa-6330-504104755> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2957813&minPrice=2915626&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-8-boyd-cresent-hamilton-hill-wa-6163-504396056> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2831250&minPrice=2789064&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2894532&minPrice=2873439&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2915625&minPrice=2873439&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-94-262-lord-street-perth-wa-6000-504184803> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-238-oxford-street-leederville-wa-6007-504162219> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-48b-48c-glyde-street-mosman-park-wa-6012-504378108> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1776563&minPrice=1734376&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1734376&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2367188&minPrice=2325001&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2409375&minPrice=2325001&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-19-remisko-drive-forrestdale-wa-6112-504290384> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-192-stirling-street-perth-wa-6000-504311512> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2409375&minPrice=2367189&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2409375&minPrice=2325001&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-177-hay-street-subiaco-wa-6008-504421136> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-1-corner-barry-marshall-parade-fiona-wood-road-murdoch-wa-6150-503636870> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-20-22-railway-road-subiaco-wa-6008-504428044> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-10-karratha-street-welshpool-wa-6106-504432036> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2662500&minPrice=2493751&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-37-grisker-road-wanneroo-wa-6065-504447884> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2662500&minPrice=2493751&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-16-hutton-street-osborne-park-wa-6017-504374584> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:26 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:40:26 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x18ff8160a50>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 109899,
 'downloader/request_count': 191,
 'downloader/request_method_count/GET': 191,
 'downloader/response_bytes': 13174857,
 'downloader/response_count': 191,
 'downloader/response_status_count/200': 191,
 'dupefilter/filtered': 19,
 'elapsed_time_seconds': 69.49163,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 40, 26, 435330),
 'httpcompression/response_bytes': 49188249,
 'httpcompression/response_count': 191,
 'item_scraped_count': 465,
 'log_count/ERROR': 53,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 12,
 'response_received_count': 191,
 'scheduler/dequeued': 191,
 'scheduler/dequeued/memory': 191,
 'scheduler/enqueued': 191,
 'scheduler/enqueued/memory': 191,
 'spider_exceptions/BrokenPipeError': 51,
 'spider_exceptions/EOFError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 10, 39, 16, 943700)}
2023-10-24 10:40:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:40:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:40:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:40:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:40:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:40:30 [scrapy.extensions.telnet] INFO: Telnet Password: b6f478ee9e27ff26
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:40:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:40:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:40:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:41:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:41:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:41:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:41:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:41:02 [scrapy.extensions.telnet] INFO: Telnet Password: 523be881adc8837d
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:41:02 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:41:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:41:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:42:03 [scrapy.extensions.logstats] INFO: Crawled 173 pages (at 173 pages/min), scraped 315 items (at 315 items/min)
2023-10-24 10:43:02 [scrapy.extensions.logstats] INFO: Crawled 368 pages (at 195 pages/min), scraped 713 items (at 398 items/min)
2023-10-24 10:43:37 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:43:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 278629,
 'downloader/request_count': 472,
 'downloader/request_method_count/GET': 472,
 'downloader/response_bytes': 30398217,
 'downloader/response_count': 472,
 'downloader/response_status_count/200': 472,
 'dupefilter/filtered': 47,
 'elapsed_time_seconds': 154.695936,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 43, 37, 686444),
 'httpcompression/response_bytes': 107632237,
 'httpcompression/response_count': 472,
 'item_scraped_count': 964,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 472,
 'scheduler/dequeued': 472,
 'scheduler/dequeued/memory': 472,
 'scheduler/enqueued': 472,
 'scheduler/enqueued/memory': 472,
 'start_time': datetime.datetime(2023, 10, 24, 10, 41, 2, 990508)}
2023-10-24 10:43:37 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:48:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:48:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:48:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:48:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:48:12 [scrapy.extensions.telnet] INFO: Telnet Password: ddd852c2e77ce256
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:48:12 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:48:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:48:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:48:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:48:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 72220,
 'downloader/request_count': 103,
 'downloader/request_method_count/GET': 103,
 'downloader/response_bytes': 7067860,
 'downloader/response_count': 103,
 'downloader/response_status_count/200': 103,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 40.666915,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 48, 53, 124810),
 'httpcompression/response_bytes': 25384794,
 'httpcompression/response_count': 103,
 'item_scraped_count': 202,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 103,
 'scheduler/dequeued': 103,
 'scheduler/dequeued/memory': 103,
 'scheduler/enqueued': 103,
 'scheduler/enqueued/memory': 103,
 'start_time': datetime.datetime(2023, 10, 24, 11, 48, 12, 457895)}
2023-10-24 11:48:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:49:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:49:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:49:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:49:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:49:47 [scrapy.extensions.telnet] INFO: Telnet Password: 7132ce4bde7d1f7b
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:49:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:49:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:49:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:49:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:49:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:49:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:49:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:49:56 [scrapy.extensions.telnet] INFO: Telnet Password: 140bc07cabe5de41
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:49:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:49:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:49:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:50:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:50:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:50:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:50:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:50:51 [scrapy.extensions.telnet] INFO: Telnet Password: 96986a35d342a770
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:50:51 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:50:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:50:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:51:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:51:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:51:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:51:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:51:31 [scrapy.extensions.telnet] INFO: Telnet Password: f015fd33bc4b63c7
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:51:31 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:51:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:51:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:51:47 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:51:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18466,
 'downloader/request_count': 28,
 'downloader/request_method_count/GET': 28,
 'downloader/response_bytes': 1861228,
 'downloader/response_count': 28,
 'downloader/response_status_count/200': 28,
 'elapsed_time_seconds': 15.871161,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 51, 47, 135253),
 'httpcompression/response_bytes': 6670823,
 'httpcompression/response_count': 28,
 'item_scraped_count': 71,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 4,
 'response_received_count': 28,
 'scheduler/dequeued': 28,
 'scheduler/dequeued/memory': 28,
 'scheduler/enqueued': 28,
 'scheduler/enqueued/memory': 28,
 'start_time': datetime.datetime(2023, 10, 24, 11, 51, 31, 264092)}
2023-10-24 11:51:47 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:53:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:53:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:53:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:53:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:53:31 [scrapy.extensions.telnet] INFO: Telnet Password: 4a727cad1e134b69
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:53:31 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:53:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:53:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:54:03 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:54:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 44966,
 'downloader/request_count': 72,
 'downloader/request_method_count/GET': 72,
 'downloader/response_bytes': 4909468,
 'downloader/response_count': 72,
 'downloader/response_status_count/200': 72,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 31.970117,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 54, 3, 430649),
 'httpcompression/response_bytes': 17612335,
 'httpcompression/response_count': 72,
 'item_scraped_count': 146,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 8,
 'response_received_count': 72,
 'scheduler/dequeued': 72,
 'scheduler/dequeued/memory': 72,
 'scheduler/enqueued': 72,
 'scheduler/enqueued/memory': 72,
 'start_time': datetime.datetime(2023, 10, 24, 11, 53, 31, 460532)}
2023-10-24 11:54:03 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:55:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:55:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:55:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:55:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:55:00 [scrapy.extensions.telnet] INFO: Telnet Password: 02c6e7951d017f26
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:55:00 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:55:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:55:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:55:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:55:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:55:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:55:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:55:34 [scrapy.extensions.telnet] INFO: Telnet Password: 86ce6210004bc9c1
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:55:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:55:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:55:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:56:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:56:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 31751,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3483857,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 39.467452,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 56, 13, 845336),
 'httpcompression/response_bytes': 12578058,
 'httpcompression/response_count': 51,
 'item_scraped_count': 108,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 51,
 'scheduler/dequeued': 51,
 'scheduler/dequeued/memory': 51,
 'scheduler/enqueued': 51,
 'scheduler/enqueued/memory': 51,
 'start_time': datetime.datetime(2023, 10, 24, 11, 55, 34, 377884)}
2023-10-24 11:56:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:58:01 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:58:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:58:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:58:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:58:01 [scrapy.extensions.telnet] INFO: Telnet Password: a4ce8d8cf2682c5b
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:58:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:58:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:58:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:58:26 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:58:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 19081,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 1936216,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'elapsed_time_seconds': 25.84936,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 58, 26, 887029),
 'httpcompression/response_bytes': 6983801,
 'httpcompression/response_count': 29,
 'item_scraped_count': 76,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 4,
 'response_received_count': 29,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 29,
 'scheduler/enqueued/memory': 29,
 'start_time': datetime.datetime(2023, 10, 24, 11, 58, 1, 37669)}
2023-10-24 11:58:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:59:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:59:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:59:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:59:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:59:35 [scrapy.extensions.telnet] INFO: Telnet Password: 8aaeec1f2b1f1d8b
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:59:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:59:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:59:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:59:57 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:59:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 32230,
 'downloader/request_count': 53,
 'downloader/request_method_count/GET': 53,
 'downloader/response_bytes': 3609894,
 'downloader/response_count': 53,
 'downloader/response_status_count/200': 53,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 21.789925,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 59, 57, 644268),
 'httpcompression/response_bytes': 12907468,
 'httpcompression/response_count': 53,
 'item_scraped_count': 112,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 53,
 'scheduler/dequeued': 53,
 'scheduler/dequeued/memory': 53,
 'scheduler/enqueued': 53,
 'scheduler/enqueued/memory': 53,
 'start_time': datetime.datetime(2023, 10, 24, 11, 59, 35, 854343)}
2023-10-24 11:59:57 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:01:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:01:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:01:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:01:07 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:01:07 [scrapy.extensions.telnet] INFO: Telnet Password: 54baeeb7ce4bf63d
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:01:07 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:01:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:01:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:01:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:01:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 69954,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7509366,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 46.257346,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 1, 54, 109255),
 'httpcompression/response_bytes': 26890428,
 'httpcompression/response_count': 110,
 'item_scraped_count': 216,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 12, 1, 7, 851909)}
2023-10-24 12:01:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:05:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:05:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:05:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:05:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:05:00 [scrapy.extensions.telnet] INFO: Telnet Password: 82295b8eb8c8f2c8
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:05:00 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:05:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:05:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:05:19 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:05:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 19081,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 1933749,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'elapsed_time_seconds': 18.490486,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 5, 19, 316231),
 'httpcompression/response_bytes': 6983602,
 'httpcompression/response_count': 29,
 'item_scraped_count': 76,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 4,
 'response_received_count': 29,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 29,
 'scheduler/enqueued/memory': 29,
 'start_time': datetime.datetime(2023, 10, 24, 12, 5, 0, 825745)}
2023-10-24 12:05:19 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:09:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:09:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:09:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:09:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:09:44 [scrapy.extensions.telnet] INFO: Telnet Password: ea0958833a9d206c
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:09:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:09:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:09:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:09:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:09:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:09:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:09:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:09:51 [scrapy.extensions.telnet] INFO: Telnet Password: 950c8e665807995c
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:09:51 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:09:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:09:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:10:34 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:10:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 63499,
 'downloader/request_count': 104,
 'downloader/request_method_count/GET': 104,
 'downloader/response_bytes': 7066387,
 'downloader/response_count': 104,
 'downloader/response_status_count/200': 104,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 43.3282,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 10, 34, 561697),
 'httpcompression/response_bytes': 25163347,
 'httpcompression/response_count': 104,
 'item_scraped_count': 204,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 104,
 'scheduler/dequeued': 104,
 'scheduler/dequeued/memory': 104,
 'scheduler/enqueued': 104,
 'scheduler/enqueued/memory': 104,
 'start_time': datetime.datetime(2023, 10, 24, 12, 9, 51, 233497)}
2023-10-24 12:10:34 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:11:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:11:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:11:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:11:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:11:48 [scrapy.extensions.telnet] INFO: Telnet Password: 2cf9133659e012d9
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:11:48 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:11:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:11:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:12:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 57, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-35-york-street-wingfield-sa-5013-504445036> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-146-148-marion-road-west-richmond-sa-5033-504421128> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-6-4-aldenhoven-road-lonsdale-sa-5160-504406632> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-9-347-main-south-road-morphett-vale-sa-5162-504330904> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-3-6-charlotte-street-smithfield-sa-5114-504402924> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-16-4-aldenhoven-road-lonsdale-sa-5160-504428548> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-1003-north-east-road-modbury-sa-5092-504347232> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-15-17-19-buchanan-court-hindmarsh-valley-sa-5211-504393220> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-28-racecourse-road-whyalla-norrie-sa-5608-504379364> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-174-gray-street-adelaide-sa-5000-504438032> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-3-32-simper-crescent-mount-barker-sa-5251-504436976> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-224-mount-gambier-road-millicent-sa-5280-504390660> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-3-5-enterprise-court-lonsdale-sa-5160-504431556> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-unit-2-16-proper-bay-road-port-lincoln-sa-5606-500083726> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-410-regency-road-prospect-sa-5082-504420592> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-6-57-61-west-avenue-edinburgh-sa-5111-504431148> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-cub-lonsdale-105-o-sullivans-beach-road-lonsdale-sa-5160-503981286?listingType=rent> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-2b-16-20-birmingham-street-mile-end-south-sa-5031-504430388> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:12:12 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1f96b5f3d90>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 139, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18626,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 2052147,
 'downloader/response_count': 31,
 'downloader/response_status_count/200': 31,
 'elapsed_time_seconds': 23.664229,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 12, 12, 381104),
 'httpcompression/response_bytes': 7172429,
 'httpcompression/response_count': 31,
 'item_scraped_count': 37,
 'log_count/ERROR': 20,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 3,
 'response_received_count': 31,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'spider_exceptions/BrokenPipeError': 18,
 'spider_exceptions/EOFError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 12, 11, 48, 716875)}
2023-10-24 12:12:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:16:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:16:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:16:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:16:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:16:46 [scrapy.extensions.telnet] INFO: Telnet Password: beb5c2ad9128da38
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:16:46 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:16:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:16:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:17:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:17:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 26721,
 'downloader/request_count': 41,
 'downloader/request_method_count/GET': 41,
 'downloader/response_bytes': 2782684,
 'downloader/response_count': 41,
 'downloader/response_status_count/200': 41,
 'elapsed_time_seconds': 22.605334,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 17, 8, 902372),
 'httpcompression/response_bytes': 10115221,
 'httpcompression/response_count': 41,
 'item_scraped_count': 126,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 41,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2023, 10, 24, 12, 16, 46, 297038)}
2023-10-24 12:17:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:23:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:23:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:23:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:23:57 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:23:57 [scrapy.extensions.telnet] INFO: Telnet Password: f2e0c6f29e8a3f08
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:23:57 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:23:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:23:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:24:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:24:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 42490,
 'downloader/request_count': 70,
 'downloader/request_method_count/GET': 70,
 'downloader/response_bytes': 4746555,
 'downloader/response_count': 70,
 'downloader/response_status_count/200': 70,
 'elapsed_time_seconds': 34.412474,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 24, 32, 332044),
 'httpcompression/response_bytes': 16765156,
 'httpcompression/response_count': 70,
 'item_scraped_count': 126,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 7,
 'response_received_count': 70,
 'scheduler/dequeued': 70,
 'scheduler/dequeued/memory': 70,
 'scheduler/enqueued': 70,
 'scheduler/enqueued/memory': 70,
 'start_time': datetime.datetime(2023, 10, 24, 12, 23, 57, 919570)}
2023-10-24 12:24:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:26:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:26:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:26:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:26:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:26:10 [scrapy.extensions.telnet] INFO: Telnet Password: 8412ba9897ce9a8b
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:26:10 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:26:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:26:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:27:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:27:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 106513,
 'downloader/request_count': 153,
 'downloader/request_method_count/GET': 153,
 'downloader/response_bytes': 9774105,
 'downloader/response_count': 153,
 'downloader/response_status_count/200': 153,
 'elapsed_time_seconds': 56.386589,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 27, 7, 329647),
 'httpcompression/response_bytes': 34453676,
 'httpcompression/response_count': 153,
 'item_scraped_count': 296,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 9,
 'response_received_count': 153,
 'scheduler/dequeued': 153,
 'scheduler/dequeued/memory': 153,
 'scheduler/enqueued': 153,
 'scheduler/enqueued/memory': 153,
 'start_time': datetime.datetime(2023, 10, 24, 12, 26, 10, 943058)}
2023-10-24 12:27:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:39:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:39:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:39:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:39:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:39:14 [scrapy.extensions.telnet] INFO: Telnet Password: 8110a1161c3c97e0
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:39:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:39:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:39:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:39:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/bourke-place-109>: HTTP status code is not handled or not allowed
2023-10-24 12:40:14 [scrapy.extensions.logstats] INFO: Crawled 132 pages (at 132 pages/min), scraped 248 items (at 248 items/min)
2023-10-24 12:40:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/388-queen-st-134>: HTTP status code is not handled or not allowed
2023-10-24 12:41:14 [scrapy.extensions.logstats] INFO: Crawled 311 pages (at 179 pages/min), scraped 569 items (at 321 items/min)
2023-10-24 12:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/smartstores@metroplex-133>: HTTP status code is not handled or not allowed
2023-10-24 12:42:14 [scrapy.extensions.logstats] INFO: Crawled 488 pages (at 177 pages/min), scraped 1043 items (at 474 items/min)
2023-10-24 12:42:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-24 12:43:14 [scrapy.extensions.logstats] INFO: Crawled 653 pages (at 165 pages/min), scraped 1450 items (at 407 items/min)
2023-10-24 12:43:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/adc-house-92>: HTTP status code is not handled or not allowed
2023-10-24 12:44:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/melbourne-central-tower-131>: HTTP status code is not handled or not allowed
2023-10-24 12:44:14 [scrapy.extensions.logstats] INFO: Crawled 820 pages (at 167 pages/min), scraped 2254 items (at 804 items/min)
2023-10-24 12:45:14 [scrapy.extensions.logstats] INFO: Crawled 1010 pages (at 190 pages/min), scraped 2804 items (at 550 items/min)
2023-10-24 12:46:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:46:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:46:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:46:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:46:28 [scrapy.extensions.telnet] INFO: Telnet Password: c49108c0181345d8
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:46:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:46:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:46:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:47:28 [scrapy.extensions.logstats] INFO: Crawled 131 pages (at 131 pages/min), scraped 239 items (at 239 items/min)
2023-10-24 12:48:28 [scrapy.extensions.logstats] INFO: Crawled 278 pages (at 147 pages/min), scraped 551 items (at 312 items/min)
2023-10-24 12:49:28 [scrapy.extensions.logstats] INFO: Crawled 487 pages (at 209 pages/min), scraped 1555 items (at 1004 items/min)
2023-10-24 12:50:28 [scrapy.extensions.logstats] INFO: Crawled 670 pages (at 183 pages/min), scraped 2455 items (at 900 items/min)
2023-10-24 12:51:28 [scrapy.extensions.logstats] INFO: Crawled 854 pages (at 184 pages/min), scraped 3616 items (at 1161 items/min)
2023-10-24 12:52:28 [scrapy.extensions.logstats] INFO: Crawled 1052 pages (at 198 pages/min), scraped 5158 items (at 1542 items/min)
2023-10-24 12:53:28 [scrapy.extensions.logstats] INFO: Crawled 1248 pages (at 196 pages/min), scraped 6452 items (at 1294 items/min)
2023-10-24 12:54:28 [scrapy.extensions.logstats] INFO: Crawled 1454 pages (at 206 pages/min), scraped 7772 items (at 1320 items/min)
2023-10-24 12:55:28 [scrapy.extensions.logstats] INFO: Crawled 1640 pages (at 186 pages/min), scraped 9189 items (at 1417 items/min)
2023-10-24 12:56:28 [scrapy.extensions.logstats] INFO: Crawled 1839 pages (at 199 pages/min), scraped 10507 items (at 1318 items/min)
2023-10-24 12:57:28 [scrapy.extensions.logstats] INFO: Crawled 2012 pages (at 173 pages/min), scraped 11694 items (at 1187 items/min)
2023-10-24 12:58:28 [scrapy.extensions.logstats] INFO: Crawled 2194 pages (at 182 pages/min), scraped 13003 items (at 1309 items/min)
2023-10-24 12:59:28 [scrapy.extensions.logstats] INFO: Crawled 2393 pages (at 199 pages/min), scraped 14361 items (at 1358 items/min)
2023-10-24 13:00:28 [scrapy.extensions.logstats] INFO: Crawled 2584 pages (at 191 pages/min), scraped 15516 items (at 1155 items/min)
2023-10-24 13:01:28 [scrapy.extensions.logstats] INFO: Crawled 2760 pages (at 176 pages/min), scraped 15516 items (at 0 items/min)
2023-10-24 13:02:28 [scrapy.extensions.logstats] INFO: Crawled 2953 pages (at 193 pages/min), scraped 15516 items (at 0 items/min)
2023-10-24 13:03:20 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-24 13:03:20 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-24 13:03:20 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=334963&maxPrice=335938. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=234376&maxPrice=236329. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=3&minPrice=78126&maxPrice=93750. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=333986&maxPrice=334962. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:03:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:03:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:03:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:03:32 [scrapy.extensions.telnet] INFO: Telnet Password: c1b93145316faa9b
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:03:32 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:03:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:03:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:04:32 [scrapy.extensions.logstats] INFO: Crawled 119 pages (at 119 pages/min), scraped 227 items (at 227 items/min)
2023-10-24 13:05:32 [scrapy.extensions.logstats] INFO: Crawled 275 pages (at 156 pages/min), scraped 500 items (at 273 items/min)
2023-10-24 13:05:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/243-sherbrooke-129>: HTTP status code is not handled or not allowed
2023-10-24 13:06:32 [scrapy.extensions.logstats] INFO: Crawled 400 pages (at 125 pages/min), scraped 861 items (at 361 items/min)
2023-10-24 13:07:32 [scrapy.extensions.logstats] INFO: Crawled 549 pages (at 149 pages/min), scraped 1702 items (at 841 items/min)
2023-10-24 13:08:32 [scrapy.extensions.logstats] INFO: Crawled 720 pages (at 171 pages/min), scraped 2496 items (at 794 items/min)
2023-10-24 13:09:32 [scrapy.extensions.logstats] INFO: Crawled 920 pages (at 200 pages/min), scraped 3318 items (at 822 items/min)
2023-10-24 13:10:32 [scrapy.extensions.logstats] INFO: Crawled 1096 pages (at 176 pages/min), scraped 4311 items (at 993 items/min)
2023-10-24 13:11:32 [scrapy.extensions.logstats] INFO: Crawled 1263 pages (at 167 pages/min), scraped 5252 items (at 941 items/min)
2023-10-24 13:12:32 [scrapy.extensions.logstats] INFO: Crawled 1463 pages (at 200 pages/min), scraped 6932 items (at 1680 items/min)
2023-10-24 13:13:32 [scrapy.extensions.logstats] INFO: Crawled 1657 pages (at 194 pages/min), scraped 8287 items (at 1355 items/min)
2023-10-24 13:14:32 [scrapy.extensions.logstats] INFO: Crawled 1836 pages (at 179 pages/min), scraped 9531 items (at 1244 items/min)
2023-10-24 13:15:32 [scrapy.extensions.logstats] INFO: Crawled 2034 pages (at 198 pages/min), scraped 11159 items (at 1628 items/min)
2023-10-24 13:16:32 [scrapy.extensions.logstats] INFO: Crawled 2235 pages (at 201 pages/min), scraped 12135 items (at 976 items/min)
2023-10-24 13:17:32 [scrapy.extensions.logstats] INFO: Crawled 2409 pages (at 174 pages/min), scraped 12495 items (at 360 items/min)
2023-10-24 13:18:32 [scrapy.extensions.logstats] INFO: Crawled 2630 pages (at 221 pages/min), scraped 13913 items (at 1418 items/min)
2023-10-24 13:19:32 [scrapy.extensions.logstats] INFO: Crawled 2826 pages (at 196 pages/min), scraped 14870 items (at 957 items/min)
2023-10-24 13:20:32 [scrapy.extensions.logstats] INFO: Crawled 3041 pages (at 215 pages/min), scraped 16027 items (at 1157 items/min)
2023-10-24 13:21:32 [scrapy.extensions.logstats] INFO: Crawled 3244 pages (at 203 pages/min), scraped 17825 items (at 1798 items/min)
2023-10-24 13:22:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:22:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:22:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:22:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:22:30 [scrapy.extensions.telnet] INFO: Telnet Password: c9592027828388a1
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:22:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:22:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:22:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:23:30 [scrapy.extensions.logstats] INFO: Crawled 133 pages (at 133 pages/min), scraped 251 items (at 251 items/min)
2023-10-24 13:24:30 [scrapy.extensions.logstats] INFO: Crawled 347 pages (at 214 pages/min), scraped 468 items (at 217 items/min)
2023-10-24 13:25:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:25:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:25:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:25:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:25:45 [scrapy.extensions.telnet] INFO: Telnet Password: 47337ba8503812e6
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:25:45 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:25:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:25:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:26:45 [scrapy.extensions.logstats] INFO: Crawled 122 pages (at 122 pages/min), scraped 230 items (at 230 items/min)
2023-10-24 13:27:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:27:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:27:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:27:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:27:12 [scrapy.extensions.telnet] INFO: Telnet Password: 16e183578e54197d
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:27:12 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:27:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:27:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:28:12 [scrapy.extensions.logstats] INFO: Crawled 136 pages (at 136 pages/min), scraped 273 items (at 273 items/min)
2023-10-24 13:29:12 [scrapy.extensions.logstats] INFO: Crawled 364 pages (at 228 pages/min), scraped 601 items (at 328 items/min)
2023-10-24 13:30:12 [scrapy.extensions.logstats] INFO: Crawled 588 pages (at 224 pages/min), scraped 921 items (at 320 items/min)
2023-10-24 13:31:12 [scrapy.extensions.logstats] INFO: Crawled 825 pages (at 237 pages/min), scraped 1460 items (at 539 items/min)
2023-10-24 13:32:12 [scrapy.extensions.logstats] INFO: Crawled 1054 pages (at 229 pages/min), scraped 1999 items (at 539 items/min)
2023-10-24 13:33:12 [scrapy.extensions.logstats] INFO: Crawled 1266 pages (at 212 pages/min), scraped 2487 items (at 488 items/min)
2023-10-24 13:34:12 [scrapy.extensions.logstats] INFO: Crawled 1504 pages (at 238 pages/min), scraped 3017 items (at 530 items/min)
2023-10-24 13:35:12 [scrapy.extensions.logstats] INFO: Crawled 1723 pages (at 219 pages/min), scraped 3429 items (at 412 items/min)
2023-10-24 13:36:12 [scrapy.extensions.logstats] INFO: Crawled 1963 pages (at 240 pages/min), scraped 3808 items (at 379 items/min)
2023-10-24 13:37:12 [scrapy.extensions.logstats] INFO: Crawled 2206 pages (at 243 pages/min), scraped 4195 items (at 387 items/min)
2023-10-24 13:38:12 [scrapy.extensions.logstats] INFO: Crawled 2430 pages (at 224 pages/min), scraped 4587 items (at 392 items/min)
2023-10-24 13:39:04 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 13:39:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1864283,
 'downloader/request_count': 2623,
 'downloader/request_method_count/GET': 2623,
 'downloader/response_bytes': 167137809,
 'downloader/response_count': 2623,
 'downloader/response_status_count/200': 2623,
 'elapsed_time_seconds': 711.856315,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 13, 39, 4, 37770),
 'httpcompression/response_bytes': 589506680,
 'httpcompression/response_count': 2623,
 'item_scraped_count': 4769,
 'log_count/INFO': 21,
 'log_count/WARNING': 1,
 'request_depth_max': 59,
 'response_received_count': 2623,
 'scheduler/dequeued': 2623,
 'scheduler/dequeued/memory': 2623,
 'scheduler/enqueued': 2623,
 'scheduler/enqueued/memory': 2623,
 'start_time': datetime.datetime(2023, 10, 24, 13, 27, 12, 181455)}
2023-10-24 13:39:04 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-25 21:07:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-25 21:07:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-25 21:07:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-25 21:07:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-25 21:07:02 [scrapy.extensions.telnet] INFO: Telnet Password: 809c7a3f649b7194
2023-10-25 21:07:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-25 21:07:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-25 21:07:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-25 21:07:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-25 21:07:02 [scrapy.core.engine] INFO: Spider opened
2023-10-25 21:07:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-25 21:07:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-25 21:07:25 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-25 21:07:25 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x11401777c90>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 163, in close
    items['data'] = data
NameError: name 'data' is not defined
2023-10-25 21:07:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 13743,
 'downloader/request_count': 23,
 'downloader/request_method_count/GET': 23,
 'downloader/response_bytes': 1445993,
 'downloader/response_count': 23,
 'downloader/response_status_count/200': 23,
 'elapsed_time_seconds': 23.136439,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 2, 7, 25, 514060),
 'httpcompression/response_bytes': 5052025,
 'httpcompression/response_count': 23,
 'item_scraped_count': 42,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 5,
 'response_received_count': 23,
 'scheduler/dequeued': 23,
 'scheduler/dequeued/memory': 23,
 'scheduler/enqueued': 23,
 'scheduler/enqueued/memory': 23,
 'start_time': datetime.datetime(2023, 10, 26, 2, 7, 2, 377621)}
2023-10-25 21:07:25 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-25 21:10:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-25 21:10:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-25 21:10:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-25 21:10:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-25 21:10:20 [scrapy.extensions.telnet] INFO: Telnet Password: caee39ab2a634728
2023-10-25 21:10:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-25 21:10:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-25 21:10:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-25 21:10:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-25 21:10:20 [scrapy.core.engine] INFO: Spider opened
2023-10-25 21:10:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-25 21:10:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-25 21:10:42 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-25 21:10:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 15025,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 1491456,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'elapsed_time_seconds': 22.654531,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 2, 10, 42, 842636),
 'httpcompression/response_bytes': 5112527,
 'httpcompression/response_count': 24,
 'item_scraped_count': 35,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 5,
 'response_received_count': 24,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2023, 10, 26, 2, 10, 20, 188105)}
2023-10-25 21:10:42 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-25 22:24:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-25 22:24:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-25 22:24:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-25 22:24:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-25 22:24:51 [scrapy.extensions.telnet] INFO: Telnet Password: d0784909cef4b255
2023-10-25 22:24:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-25 22:24:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-25 22:24:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-25 22:24:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-25 22:24:51 [scrapy.core.engine] INFO: Spider opened
2023-10-25 22:24:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-25 22:24:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-25 22:25:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-25 22:25:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-25 22:25:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-25 22:25:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-25 22:25:46 [scrapy.extensions.telnet] INFO: Telnet Password: 07b5cc2b2a3bd8b5
2023-10-25 22:25:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-25 22:25:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-25 22:25:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-25 22:25:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-25 22:25:46 [scrapy.core.engine] INFO: Spider opened
2023-10-25 22:25:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-25 22:25:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-25 22:25:51 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 25 pages/min), scraped 52 items (at 52 items/min)
2023-10-25 22:26:46 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 4 pages/min), scraped 13 items (at 13 items/min)
2023-10-25 22:26:51 [scrapy.extensions.logstats] INFO: Crawled 54 pages (at 29 pages/min), scraped 108 items (at 56 items/min)
2023-10-25 22:27:46 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 15 pages/min), scraped 46 items (at 33 items/min)
2023-10-25 22:27:51 [scrapy.extensions.logstats] INFO: Crawled 65 pages (at 11 pages/min), scraped 128 items (at 20 items/min)
2023-10-25 22:28:46 [scrapy.extensions.logstats] INFO: Crawled 37 pages (at 18 pages/min), scraped 73 items (at 27 items/min)
2023-10-25 22:28:51 [scrapy.extensions.logstats] INFO: Crawled 83 pages (at 18 pages/min), scraped 173 items (at 45 items/min)
2023-10-25 22:29:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-106-medway-street-rocklea-qld-4106-504450168> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-shop-1b-2-8-brisbane-street-surry-hills-nsw-2010-504385640> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-51-queen-victoria-street-fremantle-wa-6160-504214504> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-17-mason-street-newport-vic-3015-504428980> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-100-govt-backed-investment-60-64-wells-street-frankston-vic-3199-504428952> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-600-chapel-street-south-yarra-vic-3141-504439284> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-1-76-reid-parade-hastings-vic-3915-504423548> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-shore-studios-13-13-hickson-road-sydney-nsw-2000-504385632> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:46 [scrapy.extensions.logstats] INFO: Crawled 53 pages (at 16 pages/min), scraped 90 items (at 17 items/min)
2023-10-25 22:29:51 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 26 pages/min), scraped 208 items (at 35 items/min)
2023-10-25 22:29:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 57, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-8b-herries-street-east-toowoomba-qld-4350-504418288> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-25 22:29:56 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1b7e0654ed0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 163, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-25 22:29:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 28094,
 'downloader/request_count': 56,
 'downloader/request_method_count/GET': 56,
 'downloader/response_bytes': 3775876,
 'downloader/response_count': 55,
 'downloader/response_status_count/200': 55,
 'elapsed_time_seconds': 250.214943,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 3, 29, 56, 942215),
 'httpcompression/response_bytes': 13478416,
 'httpcompression/response_count': 55,
 'item_scraped_count': 90,
 'log_count/ERROR': 11,
 'log_count/INFO': 14,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 55,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 1,
 'scheduler/dequeued': 56,
 'scheduler/dequeued/memory': 56,
 'scheduler/enqueued': 56,
 'scheduler/enqueued/memory': 56,
 'spider_exceptions/BrokenPipeError': 10,
 'start_time': datetime.datetime(2023, 10, 26, 3, 25, 46, 727272)}
2023-10-25 22:29:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-25 22:30:51 [scrapy.extensions.logstats] INFO: Crawled 140 pages (at 31 pages/min), scraped 266 items (at 58 items/min)
2023-10-25 22:31:51 [scrapy.extensions.logstats] INFO: Crawled 166 pages (at 26 pages/min), scraped 319 items (at 53 items/min)
2023-10-25 22:32:51 [scrapy.extensions.logstats] INFO: Crawled 190 pages (at 24 pages/min), scraped 361 items (at 42 items/min)
2023-10-25 22:33:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/243-sherbrooke-129>: HTTP status code is not handled or not allowed
2023-10-25 22:33:51 [scrapy.extensions.logstats] INFO: Crawled 221 pages (at 31 pages/min), scraped 418 items (at 57 items/min)
2023-10-25 22:34:51 [scrapy.extensions.logstats] INFO: Crawled 245 pages (at 24 pages/min), scraped 460 items (at 42 items/min)
2023-10-25 22:35:51 [scrapy.extensions.logstats] INFO: Crawled 269 pages (at 24 pages/min), scraped 502 items (at 42 items/min)
2023-10-25 22:36:51 [scrapy.extensions.logstats] INFO: Crawled 289 pages (at 20 pages/min), scraped 540 items (at 38 items/min)
2023-10-25 22:37:51 [scrapy.extensions.logstats] INFO: Crawled 313 pages (at 24 pages/min), scraped 591 items (at 51 items/min)
2023-10-25 22:38:51 [scrapy.extensions.logstats] INFO: Crawled 337 pages (at 24 pages/min), scraped 651 items (at 60 items/min)
2023-10-25 22:39:51 [scrapy.extensions.logstats] INFO: Crawled 353 pages (at 16 pages/min), scraped 685 items (at 34 items/min)
2023-10-25 22:40:51 [scrapy.extensions.logstats] INFO: Crawled 386 pages (at 33 pages/min), scraped 754 items (at 69 items/min)
2023-10-25 22:41:51 [scrapy.extensions.logstats] INFO: Crawled 407 pages (at 21 pages/min), scraped 853 items (at 99 items/min)
2023-10-25 22:42:51 [scrapy.extensions.logstats] INFO: Crawled 426 pages (at 19 pages/min), scraped 953 items (at 100 items/min)
2023-10-25 22:43:51 [scrapy.extensions.logstats] INFO: Crawled 450 pages (at 24 pages/min), scraped 1047 items (at 94 items/min)
2023-10-25 22:44:51 [scrapy.extensions.logstats] INFO: Crawled 469 pages (at 19 pages/min), scraped 1112 items (at 65 items/min)
2023-10-25 22:45:51 [scrapy.extensions.logstats] INFO: Crawled 503 pages (at 34 pages/min), scraped 1194 items (at 82 items/min)
2023-10-25 22:46:51 [scrapy.extensions.logstats] INFO: Crawled 530 pages (at 27 pages/min), scraped 1258 items (at 64 items/min)
2023-10-25 22:47:51 [scrapy.extensions.logstats] INFO: Crawled 554 pages (at 24 pages/min), scraped 1303 items (at 45 items/min)
2023-10-25 22:48:51 [scrapy.extensions.logstats] INFO: Crawled 578 pages (at 24 pages/min), scraped 1349 items (at 46 items/min)
2023-10-25 22:49:51 [scrapy.extensions.logstats] INFO: Crawled 603 pages (at 25 pages/min), scraped 1407 items (at 58 items/min)
2023-10-25 22:50:51 [scrapy.extensions.logstats] INFO: Crawled 613 pages (at 10 pages/min), scraped 1436 items (at 29 items/min)
2023-10-25 22:51:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-25 22:51:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-25 22:51:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-25 22:51:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-25 22:51:48 [scrapy.extensions.telnet] INFO: Telnet Password: 1f5813fa51d23d10
2023-10-25 22:51:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-25 22:51:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-25 22:51:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-25 22:51:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-25 22:51:48 [scrapy.core.engine] INFO: Spider opened
2023-10-25 22:51:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-25 22:51:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-25 22:51:51 [scrapy.extensions.logstats] INFO: Crawled 615 pages (at 2 pages/min), scraped 1440 items (at 4 items/min)
2023-10-25 22:52:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&tenure=vacant> (failed 3 times): DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-25 22:52:23 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&tenure=vacant>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-25 22:52:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-25 22:52:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 903,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 34.687583,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 3, 52, 23, 175085),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 26, 3, 51, 48, 487502)}
2023-10-25 22:52:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-25 22:52:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-25 22:52:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-25 22:52:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-25 22:52:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-25 22:52:40 [scrapy.extensions.telnet] INFO: Telnet Password: 0ddb4482e081cf13
2023-10-25 22:52:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-25 22:52:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-25 22:52:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-25 22:52:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-25 22:52:40 [scrapy.core.engine] INFO: Spider opened
2023-10-25 22:52:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-25 22:52:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-25 22:52:51 [scrapy.extensions.logstats] INFO: Crawled 619 pages (at 4 pages/min), scraped 1456 items (at 16 items/min)
2023-10-25 22:53:40 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 10 items (at 10 items/min)
2023-10-25 22:53:51 [scrapy.extensions.logstats] INFO: Crawled 634 pages (at 15 pages/min), scraped 1507 items (at 51 items/min)
2023-10-25 22:54:40 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 20 pages/min), scraped 48 items (at 38 items/min)
2023-10-25 22:54:51 [scrapy.extensions.logstats] INFO: Crawled 643 pages (at 9 pages/min), scraped 1543 items (at 36 items/min)
2023-10-25 22:55:40 [scrapy.extensions.logstats] INFO: Crawled 36 pages (at 15 pages/min), scraped 81 items (at 33 items/min)
2023-10-25 22:55:51 [scrapy.extensions.logstats] INFO: Crawled 648 pages (at 5 pages/min), scraped 1561 items (at 18 items/min)
2023-10-25 22:56:40 [scrapy.extensions.logstats] INFO: Crawled 50 pages (at 14 pages/min), scraped 104 items (at 23 items/min)
2023-10-25 22:56:51 [scrapy.extensions.logstats] INFO: Crawled 657 pages (at 9 pages/min), scraped 1595 items (at 34 items/min)
2023-10-25 22:57:40 [scrapy.extensions.logstats] INFO: Crawled 63 pages (at 13 pages/min), scraped 126 items (at 22 items/min)
2023-10-25 22:57:51 [scrapy.extensions.logstats] INFO: Crawled 665 pages (at 8 pages/min), scraped 1628 items (at 33 items/min)
2023-10-25 22:58:40 [scrapy.extensions.logstats] INFO: Crawled 64 pages (at 1 pages/min), scraped 127 items (at 1 items/min)
2023-10-25 22:58:51 [scrapy.extensions.logstats] INFO: Crawled 667 pages (at 2 pages/min), scraped 1636 items (at 8 items/min)
2023-10-25 22:59:40 [scrapy.extensions.logstats] INFO: Crawled 73 pages (at 9 pages/min), scraped 145 items (at 18 items/min)
2023-10-25 22:59:51 [scrapy.extensions.logstats] INFO: Crawled 672 pages (at 5 pages/min), scraped 1656 items (at 20 items/min)
2023-10-25 23:00:40 [scrapy.extensions.logstats] INFO: Crawled 76 pages (at 3 pages/min), scraped 157 items (at 12 items/min)
2023-10-25 23:00:51 [scrapy.extensions.logstats] INFO: Crawled 674 pages (at 2 pages/min), scraped 1664 items (at 8 items/min)
2023-10-25 23:01:40 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 12 pages/min), scraped 169 items (at 12 items/min)
2023-10-25 23:01:51 [scrapy.extensions.logstats] INFO: Crawled 685 pages (at 11 pages/min), scraped 1708 items (at 44 items/min)
2023-10-25 23:02:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/243-sherbrooke-129>: HTTP status code is not handled or not allowed
2023-10-25 23:02:40 [scrapy.extensions.logstats] INFO: Crawled 100 pages (at 12 pages/min), scraped 198 items (at 29 items/min)
2023-10-25 23:02:51 [scrapy.extensions.logstats] INFO: Crawled 694 pages (at 9 pages/min), scraped 1744 items (at 36 items/min)
2023-10-25 23:03:40 [scrapy.extensions.logstats] INFO: Crawled 118 pages (at 18 pages/min), scraped 225 items (at 27 items/min)
2023-10-25 23:03:51 [scrapy.extensions.logstats] INFO: Crawled 712 pages (at 18 pages/min), scraped 1816 items (at 72 items/min)
2023-10-25 23:04:40 [scrapy.extensions.logstats] INFO: Crawled 143 pages (at 25 pages/min), scraped 277 items (at 52 items/min)
2023-10-25 23:04:51 [scrapy.extensions.logstats] INFO: Crawled 737 pages (at 25 pages/min), scraped 1916 items (at 100 items/min)
2023-10-25 23:05:40 [scrapy.extensions.logstats] INFO: Crawled 168 pages (at 25 pages/min), scraped 320 items (at 43 items/min)
2023-10-25 23:05:51 [scrapy.extensions.logstats] INFO: Crawled 761 pages (at 24 pages/min), scraped 2013 items (at 97 items/min)
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-105-1-erskineville-road-newtown-nsw-2042-504411828. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=61672975&maxPrice=61676026. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-280-282-illawarra-road-marrickville-nsw-2204-504449484. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=74975587&maxPrice=74981690. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-505-64-castlereagh-street-sydney-nsw-2000-504422920. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-infinity-towers-unit-433-1-anthony-rolfe-avenue-gungahlin-act-2912-504443932. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-1-28-1205-koo-wee-rup-road-pakenham-vic-3810-504379572. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-47-birdwood-avenue-doonside-nsw-2767-504428048. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:36 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&tenure=vacant&page=19. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-25 23:06:40 [scrapy.extensions.logstats] INFO: Crawled 201 pages (at 33 pages/min), scraped 370 items (at 50 items/min)
2023-10-25 23:06:51 [scrapy.extensions.logstats] INFO: Crawled 870 pages (at 109 pages/min), scraped 2365 items (at 352 items/min)
2023-10-25 23:07:40 [scrapy.extensions.logstats] INFO: Crawled 756 pages (at 555 pages/min), scraped 1412 items (at 1042 items/min)
2023-10-25 23:07:51 [scrapy.extensions.logstats] INFO: Crawled 1324 pages (at 454 pages/min), scraped 4020 items (at 1655 items/min)
2023-10-25 23:08:40 [scrapy.extensions.logstats] INFO: Crawled 1361 pages (at 605 pages/min), scraped 2086 items (at 674 items/min)
2023-10-25 23:08:51 [scrapy.extensions.logstats] INFO: Crawled 1766 pages (at 442 pages/min), scraped 5680 items (at 1660 items/min)
2023-10-25 23:09:40 [scrapy.extensions.logstats] INFO: Crawled 1976 pages (at 615 pages/min), scraped 2792 items (at 706 items/min)
2023-10-25 23:09:51 [scrapy.extensions.logstats] INFO: Crawled 2212 pages (at 446 pages/min), scraped 7039 items (at 1359 items/min)
2023-10-25 23:10:51 [scrapy.extensions.logstats] INFO: Crawled 2617 pages (at 405 pages/min), scraped 8492 items (at 1453 items/min)
2023-10-25 23:11:51 [scrapy.extensions.logstats] INFO: Crawled 3021 pages (at 404 pages/min), scraped 9940 items (at 1448 items/min)
2023-10-25 23:12:51 [scrapy.extensions.logstats] INFO: Crawled 3425 pages (at 404 pages/min), scraped 11703 items (at 1763 items/min)
2023-10-25 23:13:51 [scrapy.extensions.logstats] INFO: Crawled 3850 pages (at 425 pages/min), scraped 13248 items (at 1545 items/min)
2023-10-25 23:14:51 [scrapy.extensions.logstats] INFO: Crawled 4293 pages (at 443 pages/min), scraped 14515 items (at 1267 items/min)
2023-10-25 23:15:51 [scrapy.extensions.logstats] INFO: Crawled 4708 pages (at 415 pages/min), scraped 16769 items (at 2254 items/min)
2023-10-25 23:16:51 [scrapy.extensions.logstats] INFO: Crawled 5127 pages (at 419 pages/min), scraped 18341 items (at 1572 items/min)
2023-10-25 23:17:51 [scrapy.extensions.logstats] INFO: Crawled 5541 pages (at 414 pages/min), scraped 20710 items (at 2369 items/min)
2023-10-25 23:18:51 [scrapy.extensions.logstats] INFO: Crawled 5948 pages (at 407 pages/min), scraped 22821 items (at 2111 items/min)
2023-10-25 23:19:51 [scrapy.extensions.logstats] INFO: Crawled 6351 pages (at 403 pages/min), scraped 24785 items (at 1964 items/min)
2023-10-25 23:20:51 [scrapy.extensions.logstats] INFO: Crawled 6756 pages (at 405 pages/min), scraped 26341 items (at 1556 items/min)
2023-10-25 23:21:51 [scrapy.extensions.logstats] INFO: Crawled 7153 pages (at 397 pages/min), scraped 27655 items (at 1314 items/min)
2023-10-25 23:22:51 [scrapy.extensions.logstats] INFO: Crawled 7563 pages (at 410 pages/min), scraped 28636 items (at 981 items/min)
2023-10-25 23:23:51 [scrapy.extensions.logstats] INFO: Crawled 7978 pages (at 415 pages/min), scraped 29745 items (at 1109 items/min)
2023-10-25 23:24:51 [scrapy.extensions.logstats] INFO: Crawled 8348 pages (at 370 pages/min), scraped 30676 items (at 931 items/min)
2023-10-25 23:25:51 [scrapy.extensions.logstats] INFO: Crawled 8740 pages (at 392 pages/min), scraped 31812 items (at 1136 items/min)
2023-10-25 23:26:51 [scrapy.extensions.logstats] INFO: Crawled 9186 pages (at 446 pages/min), scraped 33169 items (at 1357 items/min)
2023-10-25 23:27:51 [scrapy.extensions.logstats] INFO: Crawled 9590 pages (at 404 pages/min), scraped 34973 items (at 1804 items/min)
2023-10-25 23:28:51 [scrapy.extensions.logstats] INFO: Crawled 9980 pages (at 390 pages/min), scraped 36667 items (at 1694 items/min)
2023-10-25 23:29:51 [scrapy.extensions.logstats] INFO: Crawled 10386 pages (at 406 pages/min), scraped 38227 items (at 1560 items/min)
2023-10-25 23:30:51 [scrapy.extensions.logstats] INFO: Crawled 10793 pages (at 407 pages/min), scraped 39945 items (at 1718 items/min)
2023-10-25 23:31:51 [scrapy.extensions.logstats] INFO: Crawled 11191 pages (at 398 pages/min), scraped 41217 items (at 1272 items/min)
2023-10-25 23:32:51 [scrapy.extensions.logstats] INFO: Crawled 11593 pages (at 402 pages/min), scraped 42381 items (at 1164 items/min)
2023-10-25 23:33:51 [scrapy.extensions.logstats] INFO: Crawled 12002 pages (at 409 pages/min), scraped 43647 items (at 1266 items/min)
2023-10-25 23:34:51 [scrapy.extensions.logstats] INFO: Crawled 12400 pages (at 398 pages/min), scraped 44728 items (at 1081 items/min)
2023-10-25 23:35:51 [scrapy.extensions.logstats] INFO: Crawled 12811 pages (at 411 pages/min), scraped 45255 items (at 527 items/min)
2023-10-25 23:36:51 [scrapy.extensions.logstats] INFO: Crawled 13215 pages (at 404 pages/min), scraped 45690 items (at 435 items/min)
2023-10-25 23:37:51 [scrapy.extensions.logstats] INFO: Crawled 13598 pages (at 383 pages/min), scraped 47156 items (at 1466 items/min)
2023-10-25 23:38:51 [scrapy.extensions.logstats] INFO: Crawled 13960 pages (at 362 pages/min), scraped 48623 items (at 1467 items/min)
2023-10-25 23:39:51 [scrapy.extensions.logstats] INFO: Crawled 14357 pages (at 397 pages/min), scraped 50432 items (at 1809 items/min)
2023-10-25 23:40:51 [scrapy.extensions.logstats] INFO: Crawled 14737 pages (at 380 pages/min), scraped 52236 items (at 1804 items/min)
2023-10-25 23:41:51 [scrapy.extensions.logstats] INFO: Crawled 15129 pages (at 392 pages/min), scraped 54138 items (at 1902 items/min)
2023-10-25 23:42:51 [scrapy.extensions.logstats] INFO: Crawled 15497 pages (at 368 pages/min), scraped 55232 items (at 1094 items/min)
2023-10-25 23:43:51 [scrapy.extensions.logstats] INFO: Crawled 15894 pages (at 397 pages/min), scraped 55626 items (at 394 items/min)
2023-10-25 23:44:51 [scrapy.extensions.logstats] INFO: Crawled 16272 pages (at 378 pages/min), scraped 56164 items (at 538 items/min)
2023-10-25 23:45:51 [scrapy.extensions.logstats] INFO: Crawled 16629 pages (at 357 pages/min), scraped 56744 items (at 580 items/min)
2023-10-25 23:46:51 [scrapy.extensions.logstats] INFO: Crawled 17002 pages (at 373 pages/min), scraped 57301 items (at 557 items/min)
2023-10-25 23:47:51 [scrapy.extensions.logstats] INFO: Crawled 17402 pages (at 400 pages/min), scraped 58918 items (at 1617 items/min)
2023-10-25 23:48:51 [scrapy.extensions.logstats] INFO: Crawled 17791 pages (at 389 pages/min), scraped 60308 items (at 1390 items/min)
2023-10-25 23:49:51 [scrapy.extensions.logstats] INFO: Crawled 18174 pages (at 383 pages/min), scraped 62802 items (at 2494 items/min)
2023-10-25 23:50:51 [scrapy.extensions.logstats] INFO: Crawled 18546 pages (at 372 pages/min), scraped 65856 items (at 3054 items/min)
2023-10-25 23:51:51 [scrapy.extensions.logstats] INFO: Crawled 18928 pages (at 382 pages/min), scraped 68882 items (at 3026 items/min)
2023-10-25 23:52:51 [scrapy.extensions.logstats] INFO: Crawled 19323 pages (at 395 pages/min), scraped 72011 items (at 3129 items/min)
2023-10-25 23:53:51 [scrapy.extensions.logstats] INFO: Crawled 19712 pages (at 389 pages/min), scraped 75128 items (at 3117 items/min)
2023-10-25 23:54:51 [scrapy.extensions.logstats] INFO: Crawled 20098 pages (at 386 pages/min), scraped 78216 items (at 3088 items/min)
2023-10-25 23:55:51 [scrapy.extensions.logstats] INFO: Crawled 20471 pages (at 373 pages/min), scraped 80597 items (at 2381 items/min)
2023-10-25 23:56:51 [scrapy.extensions.logstats] INFO: Crawled 20856 pages (at 385 pages/min), scraped 82941 items (at 2344 items/min)
2023-10-25 23:57:51 [scrapy.extensions.logstats] INFO: Crawled 21243 pages (at 387 pages/min), scraped 85695 items (at 2754 items/min)
2023-10-25 23:58:51 [scrapy.extensions.logstats] INFO: Crawled 21629 pages (at 386 pages/min), scraped 88222 items (at 2527 items/min)
2023-10-25 23:59:51 [scrapy.extensions.logstats] INFO: Crawled 22012 pages (at 383 pages/min), scraped 91466 items (at 3244 items/min)
2023-10-26 00:00:51 [scrapy.extensions.logstats] INFO: Crawled 22377 pages (at 365 pages/min), scraped 93952 items (at 2486 items/min)
2023-10-26 00:01:51 [scrapy.extensions.logstats] INFO: Crawled 22749 pages (at 372 pages/min), scraped 96759 items (at 2807 items/min)
2023-10-26 00:02:51 [scrapy.extensions.logstats] INFO: Crawled 23140 pages (at 391 pages/min), scraped 98719 items (at 1960 items/min)
2023-10-26 00:03:51 [scrapy.extensions.logstats] INFO: Crawled 23516 pages (at 376 pages/min), scraped 101766 items (at 3047 items/min)
2023-10-26 00:04:51 [scrapy.extensions.logstats] INFO: Crawled 23893 pages (at 377 pages/min), scraped 105325 items (at 3559 items/min)
2023-10-26 00:05:51 [scrapy.extensions.logstats] INFO: Crawled 24271 pages (at 378 pages/min), scraped 108639 items (at 3314 items/min)
2023-10-26 00:07:50 [scrapy.extensions.logstats] INFO: Crawled 24630 pages (at 359 pages/min), scraped 111363 items (at 2724 items/min)
2023-10-26 00:07:51 [scrapy.extensions.logstats] INFO: Crawled 24630 pages (at 0 pages/min), scraped 111363 items (at 0 items/min)
2023-10-26 00:08:51 [scrapy.extensions.logstats] INFO: Crawled 24766 pages (at 136 pages/min), scraped 112622 items (at 1259 items/min)
2023-10-26 00:09:51 [scrapy.extensions.logstats] INFO: Crawled 25125 pages (at 359 pages/min), scraped 115803 items (at 3181 items/min)
2023-10-26 00:11:22 [scrapy.extensions.logstats] INFO: Crawled 25303 pages (at 178 pages/min), scraped 117210 items (at 1407 items/min)
2023-10-26 00:11:51 [scrapy.extensions.logstats] INFO: Crawled 25375 pages (at 72 pages/min), scraped 117819 items (at 609 items/min)
2023-10-26 00:12:51 [scrapy.extensions.logstats] INFO: Crawled 25604 pages (at 229 pages/min), scraped 119712 items (at 1893 items/min)
2023-10-26 00:13:51 [scrapy.extensions.logstats] INFO: Crawled 25937 pages (at 333 pages/min), scraped 122636 items (at 2924 items/min)
2023-10-26 00:14:53 [scrapy.extensions.logstats] INFO: Crawled 25937 pages (at 0 pages/min), scraped 122636 items (at 0 items/min)
2023-10-26 00:15:51 [scrapy.extensions.logstats] INFO: Crawled 26096 pages (at 159 pages/min), scraped 124052 items (at 1416 items/min)
2023-10-26 00:16:51 [scrapy.extensions.logstats] INFO: Crawled 26359 pages (at 263 pages/min), scraped 126602 items (at 2550 items/min)
2023-10-26 00:18:25 [scrapy.extensions.logstats] INFO: Crawled 26534 pages (at 175 pages/min), scraped 128179 items (at 1577 items/min)
2023-10-26 00:18:51 [scrapy.extensions.logstats] INFO: Crawled 26598 pages (at 64 pages/min), scraped 128740 items (at 561 items/min)
2023-10-26 00:19:51 [scrapy.extensions.logstats] INFO: Crawled 26910 pages (at 312 pages/min), scraped 131649 items (at 2909 items/min)
2023-10-26 00:20:51 [scrapy.extensions.logstats] INFO: Crawled 27258 pages (at 348 pages/min), scraped 135042 items (at 3393 items/min)
2023-10-26 00:21:57 [scrapy.extensions.logstats] INFO: Crawled 27282 pages (at 24 pages/min), scraped 135278 items (at 236 items/min)
2023-10-26 00:22:51 [scrapy.extensions.logstats] INFO: Crawled 27513 pages (at 231 pages/min), scraped 137520 items (at 2242 items/min)
2023-10-26 00:23:51 [scrapy.extensions.logstats] INFO: Crawled 27873 pages (at 360 pages/min), scraped 140788 items (at 3268 items/min)
2023-10-26 00:25:29 [scrapy.extensions.logstats] INFO: Crawled 28084 pages (at 211 pages/min), scraped 142764 items (at 1976 items/min)
2023-10-26 00:25:51 [scrapy.extensions.logstats] INFO: Crawled 28118 pages (at 34 pages/min), scraped 143102 items (at 338 items/min)
2023-10-26 00:26:51 [scrapy.extensions.logstats] INFO: Crawled 28354 pages (at 236 pages/min), scraped 145173 items (at 2071 items/min)
2023-10-26 00:27:51 [scrapy.extensions.logstats] INFO: Crawled 28637 pages (at 283 pages/min), scraped 147820 items (at 2647 items/min)
2023-10-26 00:29:02 [scrapy.extensions.logstats] INFO: Crawled 28691 pages (at 54 pages/min), scraped 148291 items (at 471 items/min)
2023-10-26 00:29:51 [scrapy.extensions.logstats] INFO: Crawled 28834 pages (at 143 pages/min), scraped 149462 items (at 1171 items/min)
2023-10-26 00:30:51 [scrapy.extensions.logstats] INFO: Crawled 29089 pages (at 255 pages/min), scraped 151329 items (at 1867 items/min)
2023-10-26 00:32:33 [scrapy.extensions.logstats] INFO: Crawled 29329 pages (at 240 pages/min), scraped 153233 items (at 1904 items/min)
2023-10-26 00:32:51 [scrapy.extensions.logstats] INFO: Crawled 29359 pages (at 30 pages/min), scraped 153479 items (at 246 items/min)
2023-10-26 00:33:51 [scrapy.extensions.logstats] INFO: Crawled 29699 pages (at 340 pages/min), scraped 156087 items (at 2608 items/min)
2023-10-26 00:34:51 [scrapy.extensions.logstats] INFO: Crawled 30040 pages (at 341 pages/min), scraped 158605 items (at 2518 items/min)
2023-10-26 00:36:04 [scrapy.extensions.logstats] INFO: Crawled 30122 pages (at 82 pages/min), scraped 159206 items (at 601 items/min)
2023-10-26 00:36:51 [scrapy.extensions.logstats] INFO: Crawled 30272 pages (at 150 pages/min), scraped 160286 items (at 1080 items/min)
2023-10-26 00:37:51 [scrapy.extensions.logstats] INFO: Crawled 30543 pages (at 271 pages/min), scraped 162474 items (at 2188 items/min)
2023-10-26 00:39:36 [scrapy.extensions.logstats] INFO: Crawled 30791 pages (at 248 pages/min), scraped 164626 items (at 2152 items/min)
2023-10-26 00:39:51 [scrapy.extensions.logstats] INFO: Crawled 30796 pages (at 5 pages/min), scraped 164668 items (at 42 items/min)
2023-10-26 00:40:51 [scrapy.extensions.logstats] INFO: Crawled 30932 pages (at 136 pages/min), scraped 165836 items (at 1168 items/min)
2023-10-26 00:41:51 [scrapy.extensions.logstats] INFO: Crawled 31033 pages (at 101 pages/min), scraped 166676 items (at 840 items/min)
2023-10-26 00:43:08 [scrapy.extensions.logstats] INFO: Crawled 31066 pages (at 33 pages/min), scraped 166951 items (at 275 items/min)
2023-10-26 00:43:51 [scrapy.extensions.logstats] INFO: Crawled 31252 pages (at 186 pages/min), scraped 168488 items (at 1537 items/min)
2023-10-26 00:44:51 [scrapy.extensions.logstats] INFO: Crawled 31606 pages (at 354 pages/min), scraped 171469 items (at 2981 items/min)
2023-10-26 00:46:38 [scrapy.extensions.logstats] INFO: Crawled 31834 pages (at 228 pages/min), scraped 173436 items (at 1967 items/min)
2023-10-26 00:46:51 [scrapy.extensions.logstats] INFO: Crawled 31840 pages (at 6 pages/min), scraped 173491 items (at 55 items/min)
2023-10-26 00:47:51 [scrapy.extensions.logstats] INFO: Crawled 32089 pages (at 249 pages/min), scraped 175605 items (at 2114 items/min)
2023-10-26 00:48:51 [scrapy.extensions.logstats] INFO: Crawled 32430 pages (at 341 pages/min), scraped 178465 items (at 2860 items/min)
2023-10-26 00:50:10 [scrapy.extensions.logstats] INFO: Crawled 32534 pages (at 104 pages/min), scraped 179341 items (at 876 items/min)
2023-10-26 00:50:51 [scrapy.extensions.logstats] INFO: Crawled 32554 pages (at 20 pages/min), scraped 179523 items (at 182 items/min)
2023-10-26 00:51:51 [scrapy.extensions.logstats] INFO: Crawled 32588 pages (at 34 pages/min), scraped 179816 items (at 293 items/min)
2023-10-26 00:52:51 [scrapy.extensions.logstats] INFO: Crawled 32879 pages (at 291 pages/min), scraped 182166 items (at 2350 items/min)
2023-10-26 00:53:51 [scrapy.extensions.logstats] INFO: Crawled 33268 pages (at 389 pages/min), scraped 185457 items (at 3291 items/min)
2023-10-26 00:55:12 [scrapy.extensions.logstats] INFO: Crawled 33387 pages (at 119 pages/min), scraped 186496 items (at 1039 items/min)
2023-10-26 00:55:51 [scrapy.extensions.logstats] INFO: Crawled 33430 pages (at 43 pages/min), scraped 186845 items (at 349 items/min)
2023-10-26 00:56:51 [scrapy.extensions.logstats] INFO: Crawled 33604 pages (at 174 pages/min), scraped 188363 items (at 1518 items/min)
2023-10-26 00:58:45 [scrapy.extensions.logstats] INFO: Crawled 33855 pages (at 251 pages/min), scraped 190505 items (at 2142 items/min)
2023-10-26 00:58:51 [scrapy.extensions.logstats] INFO: Crawled 33855 pages (at 0 pages/min), scraped 190505 items (at 0 items/min)
2023-10-26 00:59:51 [scrapy.extensions.logstats] INFO: Crawled 34102 pages (at 247 pages/min), scraped 192337 items (at 1832 items/min)
2023-10-26 01:00:51 [scrapy.extensions.logstats] INFO: Crawled 34331 pages (at 229 pages/min), scraped 194291 items (at 1954 items/min)
2023-10-26 01:02:15 [scrapy.extensions.logstats] INFO: Crawled 34432 pages (at 101 pages/min), scraped 195169 items (at 878 items/min)
2023-10-26 01:02:51 [scrapy.extensions.logstats] INFO: Crawled 34543 pages (at 111 pages/min), scraped 196137 items (at 968 items/min)
2023-10-26 01:03:51 [scrapy.extensions.logstats] INFO: Crawled 34782 pages (at 239 pages/min), scraped 198158 items (at 2021 items/min)
2023-10-26 01:05:47 [scrapy.extensions.logstats] INFO: Crawled 35061 pages (at 279 pages/min), scraped 200535 items (at 2377 items/min)
2023-10-26 01:05:51 [scrapy.extensions.logstats] INFO: Crawled 35061 pages (at 0 pages/min), scraped 200535 items (at 0 items/min)
2023-10-26 01:06:51 [scrapy.extensions.logstats] INFO: Crawled 35232 pages (at 171 pages/min), scraped 201967 items (at 1432 items/min)
2023-10-26 01:07:51 [scrapy.extensions.logstats] INFO: Crawled 35386 pages (at 154 pages/min), scraped 203271 items (at 1304 items/min)
2023-10-26 01:09:16 [scrapy.extensions.logstats] INFO: Crawled 35411 pages (at 25 pages/min), scraped 203488 items (at 217 items/min)
2023-10-26 01:09:51 [scrapy.extensions.logstats] INFO: Crawled 35419 pages (at 8 pages/min), scraped 203558 items (at 70 items/min)
2023-10-26 01:10:51 [scrapy.extensions.logstats] INFO: Crawled 35452 pages (at 33 pages/min), scraped 203828 items (at 270 items/min)
2023-10-26 01:11:51 [scrapy.extensions.logstats] INFO: Crawled 35532 pages (at 80 pages/min), scraped 204508 items (at 680 items/min)
2023-10-26 01:12:51 [scrapy.extensions.logstats] INFO: Crawled 35633 pages (at 101 pages/min), scraped 205362 items (at 854 items/min)
2023-10-26 01:14:18 [scrapy.extensions.logstats] INFO: Crawled 35664 pages (at 31 pages/min), scraped 205627 items (at 265 items/min)
2023-10-26 01:14:51 [scrapy.extensions.logstats] INFO: Crawled 35664 pages (at 0 pages/min), scraped 205627 items (at 0 items/min)
2023-10-26 01:15:51 [scrapy.extensions.logstats] INFO: Crawled 35664 pages (at 0 pages/min), scraped 205627 items (at 0 items/min)
2023-10-26 01:17:05 [scrapy.extensions.logstats] INFO: Crawled 35664 pages (at 0 pages/min), scraped 205627 items (at 0 items/min)
2023-10-26 01:17:51 [scrapy.extensions.logstats] INFO: Crawled 35664 pages (at 0 pages/min), scraped 205627 items (at 0 items/min)
2023-10-26 01:18:51 [scrapy.extensions.logstats] INFO: Crawled 35664 pages (at 0 pages/min), scraped 205627 items (at 0 items/min)
2023-10-26 01:19:51 [scrapy.extensions.logstats] INFO: Crawled 35666 pages (at 2 pages/min), scraped 205647 items (at 20 items/min)
2023-10-26 01:20:51 [scrapy.extensions.logstats] INFO: Crawled 35666 pages (at 0 pages/min), scraped 205647 items (at 0 items/min)
2023-10-26 01:21:51 [scrapy.extensions.logstats] INFO: Crawled 35666 pages (at 0 pages/min), scraped 205647 items (at 0 items/min)
2023-10-26 01:22:51 [scrapy.extensions.logstats] INFO: Crawled 35666 pages (at 0 pages/min), scraped 205647 items (at 0 items/min)
2023-10-26 01:23:51 [scrapy.extensions.logstats] INFO: Crawled 35672 pages (at 6 pages/min), scraped 205693 items (at 46 items/min)
2023-10-26 01:24:51 [scrapy.extensions.logstats] INFO: Crawled 35702 pages (at 30 pages/min), scraped 205941 items (at 248 items/min)
2023-10-26 01:25:51 [scrapy.extensions.logstats] INFO: Crawled 35851 pages (at 149 pages/min), scraped 207354 items (at 1413 items/min)
2023-10-26 01:26:51 [scrapy.extensions.logstats] INFO: Crawled 36011 pages (at 160 pages/min), scraped 208776 items (at 1422 items/min)
2023-10-26 01:27:51 [scrapy.extensions.logstats] INFO: Crawled 36184 pages (at 173 pages/min), scraped 210245 items (at 1469 items/min)
2023-10-26 01:28:51 [scrapy.extensions.logstats] INFO: Crawled 36356 pages (at 172 pages/min), scraped 211720 items (at 1475 items/min)
2023-10-26 01:29:51 [scrapy.extensions.logstats] INFO: Crawled 36589 pages (at 233 pages/min), scraped 213564 items (at 1844 items/min)
2023-10-26 01:30:51 [scrapy.extensions.logstats] INFO: Crawled 36767 pages (at 178 pages/min), scraped 215195 items (at 1631 items/min)
2023-10-26 01:31:51 [scrapy.extensions.logstats] INFO: Crawled 36874 pages (at 107 pages/min), scraped 215963 items (at 768 items/min)
2023-10-26 01:32:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 01:32:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 01:32:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 01:32:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 01:32:31 [scrapy.extensions.telnet] INFO: Telnet Password: cd765bf1554295b9
2023-10-26 01:32:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 01:32:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 01:32:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 01:32:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 01:32:31 [scrapy.core.engine] INFO: Spider opened
2023-10-26 01:32:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 01:32:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 01:32:51 [scrapy.extensions.logstats] INFO: Crawled 36955 pages (at 81 pages/min), scraped 216468 items (at 505 items/min)
2023-10-26 01:33:51 [scrapy.extensions.logstats] INFO: Crawled 37009 pages (at 54 pages/min), scraped 216798 items (at 330 items/min)
2023-10-26 01:34:51 [scrapy.extensions.logstats] INFO: Crawled 37216 pages (at 207 pages/min), scraped 218271 items (at 1473 items/min)
2023-10-26 01:35:51 [scrapy.extensions.logstats] INFO: Crawled 37480 pages (at 264 pages/min), scraped 220179 items (at 1908 items/min)
2023-10-26 01:36:51 [scrapy.extensions.logstats] INFO: Crawled 37771 pages (at 291 pages/min), scraped 222329 items (at 2150 items/min)
2023-10-26 01:37:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 01:37:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 01:37:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 01:37:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 01:37:09 [scrapy.extensions.telnet] INFO: Telnet Password: 246707df3cad068b
2023-10-26 01:37:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 01:37:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 01:37:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 01:37:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 01:37:09 [scrapy.core.engine] INFO: Spider opened
2023-10-26 01:37:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 01:37:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 01:37:51 [scrapy.extensions.logstats] INFO: Crawled 37942 pages (at 171 pages/min), scraped 223909 items (at 1580 items/min)
2023-10-26 01:38:09 [scrapy.extensions.logstats] INFO: Crawled 166 pages (at 166 pages/min), scraped 310 items (at 310 items/min)
2023-10-26 01:38:51 [scrapy.extensions.logstats] INFO: Crawled 38195 pages (at 253 pages/min), scraped 226063 items (at 2154 items/min)
2023-10-26 01:39:09 [scrapy.extensions.logstats] INFO: Crawled 411 pages (at 245 pages/min), scraped 862 items (at 552 items/min)
2023-10-26 01:39:51 [scrapy.extensions.logstats] INFO: Crawled 38275 pages (at 80 pages/min), scraped 226710 items (at 647 items/min)
2023-10-26 01:40:09 [scrapy.extensions.logstats] INFO: Crawled 507 pages (at 96 pages/min), scraped 1298 items (at 436 items/min)
2023-10-26 01:40:51 [scrapy.extensions.logstats] INFO: Crawled 38323 pages (at 48 pages/min), scraped 227003 items (at 293 items/min)
2023-10-26 01:41:09 [scrapy.extensions.logstats] INFO: Crawled 551 pages (at 44 pages/min), scraped 1402 items (at 104 items/min)
2023-10-26 01:41:51 [scrapy.extensions.logstats] INFO: Crawled 38377 pages (at 54 pages/min), scraped 227326 items (at 323 items/min)
2023-10-26 01:42:09 [scrapy.extensions.logstats] INFO: Crawled 754 pages (at 203 pages/min), scraped 1925 items (at 523 items/min)
2023-10-26 01:42:51 [scrapy.extensions.logstats] INFO: Crawled 38477 pages (at 100 pages/min), scraped 227880 items (at 554 items/min)
2023-10-26 01:43:09 [scrapy.extensions.logstats] INFO: Crawled 919 pages (at 165 pages/min), scraped 2185 items (at 260 items/min)
2023-10-26 01:43:51 [scrapy.extensions.logstats] INFO: Crawled 38545 pages (at 68 pages/min), scraped 228284 items (at 404 items/min)
2023-10-26 01:44:09 [scrapy.extensions.logstats] INFO: Crawled 1061 pages (at 142 pages/min), scraped 2537 items (at 352 items/min)
2023-10-26 01:44:51 [scrapy.extensions.logstats] INFO: Crawled 38612 pages (at 67 pages/min), scraped 228688 items (at 404 items/min)
2023-10-26 01:45:09 [scrapy.extensions.logstats] INFO: Crawled 1153 pages (at 92 pages/min), scraped 2814 items (at 277 items/min)
2023-10-26 01:45:51 [scrapy.extensions.logstats] INFO: Crawled 38665 pages (at 53 pages/min), scraped 229080 items (at 392 items/min)
2023-10-26 01:46:09 [scrapy.extensions.logstats] INFO: Crawled 1292 pages (at 139 pages/min), scraped 3155 items (at 341 items/min)
2023-10-26 01:46:51 [scrapy.extensions.logstats] INFO: Crawled 38817 pages (at 152 pages/min), scraped 230052 items (at 972 items/min)
2023-10-26 01:47:09 [scrapy.extensions.logstats] INFO: Crawled 1473 pages (at 181 pages/min), scraped 3755 items (at 600 items/min)
2023-10-26 01:47:51 [scrapy.extensions.logstats] INFO: Crawled 38965 pages (at 148 pages/min), scraped 231265 items (at 1213 items/min)
2023-10-26 01:48:09 [scrapy.extensions.logstats] INFO: Crawled 1581 pages (at 108 pages/min), scraped 4399 items (at 644 items/min)
2023-10-26 01:48:51 [scrapy.extensions.logstats] INFO: Crawled 39078 pages (at 113 pages/min), scraped 232170 items (at 905 items/min)
2023-10-26 01:49:09 [scrapy.extensions.logstats] INFO: Crawled 1754 pages (at 173 pages/min), scraped 5848 items (at 1449 items/min)
2023-10-26 01:49:51 [scrapy.extensions.logstats] INFO: Crawled 39197 pages (at 119 pages/min), scraped 233089 items (at 919 items/min)
2023-10-26 01:50:09 [scrapy.extensions.logstats] INFO: Crawled 1923 pages (at 169 pages/min), scraped 7267 items (at 1419 items/min)
2023-10-26 01:50:51 [scrapy.extensions.logstats] INFO: Crawled 39323 pages (at 126 pages/min), scraped 234097 items (at 1008 items/min)
2023-10-26 01:51:09 [scrapy.extensions.logstats] INFO: Crawled 2073 pages (at 150 pages/min), scraped 8501 items (at 1234 items/min)
2023-10-26 01:51:51 [scrapy.extensions.logstats] INFO: Crawled 39487 pages (at 164 pages/min), scraped 235425 items (at 1328 items/min)
2023-10-26 01:52:09 [scrapy.extensions.logstats] INFO: Crawled 2267 pages (at 194 pages/min), scraped 10083 items (at 1582 items/min)
2023-10-26 01:52:51 [scrapy.extensions.logstats] INFO: Crawled 39669 pages (at 182 pages/min), scraped 236846 items (at 1421 items/min)
2023-10-26 01:53:09 [scrapy.extensions.logstats] INFO: Crawled 2463 pages (at 196 pages/min), scraped 11702 items (at 1619 items/min)
2023-10-26 01:53:51 [scrapy.extensions.logstats] INFO: Crawled 39814 pages (at 145 pages/min), scraped 238018 items (at 1172 items/min)
2023-10-26 01:54:09 [scrapy.extensions.logstats] INFO: Crawled 2569 pages (at 106 pages/min), scraped 12611 items (at 909 items/min)
2023-10-26 01:54:51 [scrapy.extensions.logstats] INFO: Crawled 39925 pages (at 111 pages/min), scraped 238909 items (at 891 items/min)
2023-10-26 01:55:09 [scrapy.extensions.logstats] INFO: Crawled 2727 pages (at 158 pages/min), scraped 13922 items (at 1311 items/min)
2023-10-26 01:55:51 [scrapy.extensions.logstats] INFO: Crawled 40076 pages (at 151 pages/min), scraped 240117 items (at 1208 items/min)
2023-10-26 01:56:09 [scrapy.extensions.logstats] INFO: Crawled 2876 pages (at 149 pages/min), scraped 15193 items (at 1271 items/min)
2023-10-26 01:56:51 [scrapy.extensions.logstats] INFO: Crawled 40139 pages (at 63 pages/min), scraped 240621 items (at 504 items/min)
2023-10-26 01:57:09 [scrapy.extensions.logstats] INFO: Crawled 2932 pages (at 56 pages/min), scraped 15655 items (at 462 items/min)
2023-10-26 01:57:51 [scrapy.extensions.logstats] INFO: Crawled 40204 pages (at 65 pages/min), scraped 241141 items (at 520 items/min)
2023-10-26 01:58:09 [scrapy.extensions.logstats] INFO: Crawled 3012 pages (at 80 pages/min), scraped 16337 items (at 682 items/min)
2023-10-26 01:58:51 [scrapy.extensions.logstats] INFO: Crawled 40275 pages (at 71 pages/min), scraped 241709 items (at 568 items/min)
2023-10-26 01:59:09 [scrapy.extensions.logstats] INFO: Crawled 3095 pages (at 83 pages/min), scraped 17074 items (at 737 items/min)
2023-10-26 01:59:51 [scrapy.extensions.logstats] INFO: Crawled 40459 pages (at 184 pages/min), scraped 243183 items (at 1474 items/min)
2023-10-26 02:00:09 [scrapy.extensions.logstats] INFO: Crawled 3330 pages (at 235 pages/min), scraped 19187 items (at 2113 items/min)
2023-10-26 02:00:51 [scrapy.extensions.logstats] INFO: Crawled 40590 pages (at 131 pages/min), scraped 244219 items (at 1036 items/min)
2023-10-26 02:01:09 [scrapy.extensions.logstats] INFO: Crawled 3438 pages (at 108 pages/min), scraped 20139 items (at 952 items/min)
2023-10-26 02:01:51 [scrapy.extensions.logstats] INFO: Crawled 40736 pages (at 146 pages/min), scraped 245395 items (at 1176 items/min)
2023-10-26 02:02:09 [scrapy.extensions.logstats] INFO: Crawled 3632 pages (at 194 pages/min), scraped 21806 items (at 1667 items/min)
2023-10-26 02:02:51 [scrapy.extensions.logstats] INFO: Crawled 40941 pages (at 205 pages/min), scraped 247039 items (at 1644 items/min)
2023-10-26 02:03:09 [scrapy.extensions.logstats] INFO: Crawled 3778 pages (at 146 pages/min), scraped 23075 items (at 1269 items/min)
2023-10-26 02:03:51 [scrapy.extensions.logstats] INFO: Crawled 41017 pages (at 76 pages/min), scraped 247647 items (at 608 items/min)
2023-10-26 02:04:09 [scrapy.extensions.logstats] INFO: Crawled 3841 pages (at 63 pages/min), scraped 23639 items (at 564 items/min)
2023-10-26 02:04:51 [scrapy.extensions.logstats] INFO: Crawled 41155 pages (at 138 pages/min), scraped 248751 items (at 1104 items/min)
2023-10-26 02:05:09 [scrapy.extensions.logstats] INFO: Crawled 3984 pages (at 143 pages/min), scraped 24892 items (at 1253 items/min)
2023-10-26 02:05:51 [scrapy.extensions.logstats] INFO: Crawled 41257 pages (at 102 pages/min), scraped 249564 items (at 813 items/min)
2023-10-26 02:06:09 [scrapy.extensions.logstats] INFO: Crawled 4028 pages (at 44 pages/min), scraped 25288 items (at 396 items/min)
2023-10-26 02:06:51 [scrapy.extensions.logstats] INFO: Crawled 41335 pages (at 78 pages/min), scraped 250196 items (at 632 items/min)
2023-10-26 02:07:09 [scrapy.extensions.logstats] INFO: Crawled 4121 pages (at 93 pages/min), scraped 26116 items (at 828 items/min)
2023-10-26 02:07:51 [scrapy.extensions.logstats] INFO: Crawled 41389 pages (at 54 pages/min), scraped 250630 items (at 434 items/min)
2023-10-26 02:08:09 [scrapy.extensions.logstats] INFO: Crawled 4159 pages (at 38 pages/min), scraped 26467 items (at 351 items/min)
2023-10-26 02:08:51 [scrapy.extensions.logstats] INFO: Crawled 41453 pages (at 64 pages/min), scraped 251137 items (at 507 items/min)
2023-10-26 02:09:09 [scrapy.extensions.logstats] INFO: Crawled 4206 pages (at 47 pages/min), scraped 26890 items (at 423 items/min)
2023-10-26 02:09:51 [scrapy.extensions.logstats] INFO: Crawled 41527 pages (at 74 pages/min), scraped 251739 items (at 602 items/min)
2023-10-26 02:10:09 [scrapy.extensions.logstats] INFO: Crawled 4252 pages (at 46 pages/min), scraped 27303 items (at 413 items/min)
2023-10-26 02:10:51 [scrapy.extensions.logstats] INFO: Crawled 41560 pages (at 33 pages/min), scraped 252005 items (at 266 items/min)
2023-10-26 02:11:09 [scrapy.extensions.logstats] INFO: Crawled 4300 pages (at 48 pages/min), scraped 27720 items (at 417 items/min)
2023-10-26 02:11:51 [scrapy.extensions.logstats] INFO: Crawled 41641 pages (at 81 pages/min), scraped 252653 items (at 648 items/min)
2023-10-26 02:12:09 [scrapy.extensions.logstats] INFO: Crawled 4417 pages (at 117 pages/min), scraped 28749 items (at 1029 items/min)
2023-10-26 02:12:51 [scrapy.extensions.logstats] INFO: Crawled 41758 pages (at 117 pages/min), scraped 253561 items (at 908 items/min)
2023-10-26 02:13:09 [scrapy.extensions.logstats] INFO: Crawled 4643 pages (at 226 pages/min), scraped 30678 items (at 1929 items/min)
2023-10-26 02:13:51 [scrapy.extensions.logstats] INFO: Crawled 41860 pages (at 102 pages/min), scraped 254343 items (at 782 items/min)
2023-10-26 02:14:09 [scrapy.extensions.logstats] INFO: Crawled 4669 pages (at 26 pages/min), scraped 30907 items (at 229 items/min)
2023-10-26 02:14:51 [scrapy.extensions.logstats] INFO: Crawled 41917 pages (at 57 pages/min), scraped 254815 items (at 472 items/min)
2023-10-26 02:15:09 [scrapy.extensions.logstats] INFO: Crawled 4758 pages (at 89 pages/min), scraped 31696 items (at 789 items/min)
2023-10-26 02:15:51 [scrapy.extensions.logstats] INFO: Crawled 42020 pages (at 103 pages/min), scraped 255654 items (at 839 items/min)
2023-10-26 02:16:09 [scrapy.extensions.logstats] INFO: Crawled 4849 pages (at 91 pages/min), scraped 32497 items (at 801 items/min)
2023-10-26 02:16:51 [scrapy.extensions.logstats] INFO: Crawled 42225 pages (at 205 pages/min), scraped 257442 items (at 1788 items/min)
2023-10-26 02:17:09 [scrapy.extensions.logstats] INFO: Crawled 5082 pages (at 233 pages/min), scraped 34561 items (at 2064 items/min)
2023-10-26 02:17:51 [scrapy.extensions.logstats] INFO: Crawled 42441 pages (at 216 pages/min), scraped 259272 items (at 1830 items/min)
2023-10-26 02:18:09 [scrapy.extensions.logstats] INFO: Crawled 5321 pages (at 239 pages/min), scraped 36718 items (at 2157 items/min)
2023-10-26 02:18:51 [scrapy.extensions.logstats] INFO: Crawled 42649 pages (at 208 pages/min), scraped 261128 items (at 1856 items/min)
2023-10-26 02:19:09 [scrapy.extensions.logstats] INFO: Crawled 5497 pages (at 176 pages/min), scraped 38293 items (at 1575 items/min)
2023-10-26 02:19:51 [scrapy.extensions.logstats] INFO: Crawled 42704 pages (at 55 pages/min), scraped 261595 items (at 467 items/min)
2023-10-26 02:20:09 [scrapy.extensions.logstats] INFO: Crawled 5553 pages (at 56 pages/min), scraped 38806 items (at 513 items/min)
2023-10-26 02:21:37 [scrapy.extensions.logstats] INFO: Crawled 42734 pages (at 30 pages/min), scraped 261855 items (at 260 items/min)
2023-10-26 02:21:37 [scrapy.extensions.logstats] INFO: Crawled 5560 pages (at 7 pages/min), scraped 38869 items (at 63 items/min)
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=47460939&maxPrice=47467042. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=43591310&maxPrice=43597413. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=46963503&maxPrice=46966554. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=45227052&maxPrice=45233155. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=48132326&maxPrice=48144532. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=42492677&maxPrice=42495728. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=44760134&maxPrice=44763184. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=37661745&maxPrice=37664796. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=47467043&maxPrice=47473145. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=47531130&maxPrice=47534180. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:21:51 [scrapy.extensions.logstats] INFO: Crawled 42734 pages (at 0 pages/min), scraped 261855 items (at 0 items/min)
2023-10-26 02:22:09 [scrapy.extensions.logstats] INFO: Crawled 5572 pages (at 12 pages/min), scraped 38966 items (at 97 items/min)
2023-10-26 02:22:51 [scrapy.extensions.logstats] INFO: Crawled 42754 pages (at 20 pages/min), scraped 262023 items (at 168 items/min)
2023-10-26 02:23:09 [scrapy.extensions.logstats] INFO: Crawled 5580 pages (at 8 pages/min), scraped 39028 items (at 62 items/min)
2023-10-26 02:23:51 [scrapy.extensions.logstats] INFO: Crawled 42755 pages (at 1 pages/min), scraped 262031 items (at 8 items/min)
2023-10-26 02:24:09 [scrapy.extensions.logstats] INFO: Crawled 5580 pages (at 0 pages/min), scraped 39028 items (at 0 items/min)
2023-10-26 02:24:51 [scrapy.extensions.logstats] INFO: Crawled 42755 pages (at 0 pages/min), scraped 262031 items (at 0 items/min)
2023-10-26 02:25:09 [scrapy.extensions.logstats] INFO: Crawled 5582 pages (at 2 pages/min), scraped 39046 items (at 18 items/min)
2023-10-26 02:25:51 [scrapy.extensions.logstats] INFO: Crawled 42802 pages (at 47 pages/min), scraped 262423 items (at 392 items/min)
2023-10-26 02:26:09 [scrapy.extensions.logstats] INFO: Crawled 5646 pages (at 64 pages/min), scraped 39579 items (at 533 items/min)
2023-10-26 02:26:51 [scrapy.extensions.logstats] INFO: Crawled 42854 pages (at 52 pages/min), scraped 262865 items (at 442 items/min)
2023-10-26 02:28:07 [scrapy.extensions.logstats] INFO: Crawled 5694 pages (at 48 pages/min), scraped 39963 items (at 384 items/min)
2023-10-26 02:28:07 [scrapy.extensions.logstats] INFO: Crawled 42870 pages (at 16 pages/min), scraped 262998 items (at 133 items/min)
2023-10-26 02:28:09 [scrapy.extensions.logstats] INFO: Crawled 5694 pages (at 0 pages/min), scraped 39963 items (at 0 items/min)
2023-10-26 02:28:11 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=43685915&maxPrice=43688965. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:28:11 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=34271242&maxPrice=34277344. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:28:11 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=42602541&maxPrice=42614747. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:28:11 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=34234621&maxPrice=34240723. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:28:11 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=33935548&maxPrice=33947754. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:28:11 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=37634279&maxPrice=37637330. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:28:51 [scrapy.extensions.logstats] INFO: Crawled 42871 pages (at 1 pages/min), scraped 263006 items (at 8 items/min)
2023-10-26 02:29:09 [scrapy.extensions.logstats] INFO: Crawled 5696 pages (at 2 pages/min), scraped 39979 items (at 16 items/min)
2023-10-26 02:29:51 [scrapy.extensions.logstats] INFO: Crawled 42871 pages (at 0 pages/min), scraped 263006 items (at 0 items/min)
2023-10-26 02:30:09 [scrapy.extensions.logstats] INFO: Crawled 5697 pages (at 1 pages/min), scraped 39987 items (at 8 items/min)
2023-10-26 02:30:51 [scrapy.extensions.logstats] INFO: Crawled 42871 pages (at 0 pages/min), scraped 263006 items (at 0 items/min)
2023-10-26 02:31:09 [scrapy.extensions.logstats] INFO: Crawled 5697 pages (at 0 pages/min), scraped 39987 items (at 0 items/min)
2023-10-26 02:31:51 [scrapy.extensions.logstats] INFO: Crawled 42871 pages (at 0 pages/min), scraped 263006 items (at 0 items/min)
2023-10-26 02:33:08 [scrapy.extensions.logstats] INFO: Crawled 42872 pages (at 1 pages/min), scraped 263015 items (at 9 items/min)
2023-10-26 02:33:08 [scrapy.extensions.logstats] INFO: Crawled 5701 pages (at 4 pages/min), scraped 40019 items (at 32 items/min)
2023-10-26 02:33:09 [scrapy.extensions.logstats] INFO: Crawled 5701 pages (at 0 pages/min), scraped 40019 items (at 0 items/min)
2023-10-26 02:33:51 [scrapy.extensions.logstats] INFO: Crawled 42872 pages (at 0 pages/min), scraped 263015 items (at 0 items/min)
2023-10-26 02:34:09 [scrapy.extensions.logstats] INFO: Crawled 5701 pages (at 0 pages/min), scraped 40019 items (at 0 items/min)
2023-10-26 02:34:51 [scrapy.extensions.logstats] INFO: Crawled 42872 pages (at 0 pages/min), scraped 263015 items (at 0 items/min)
2023-10-26 02:35:09 [scrapy.extensions.logstats] INFO: Crawled 5702 pages (at 1 pages/min), scraped 40028 items (at 9 items/min)
2023-10-26 02:35:51 [scrapy.extensions.logstats] INFO: Crawled 42872 pages (at 0 pages/min), scraped 263015 items (at 0 items/min)
2023-10-26 02:36:09 [scrapy.extensions.logstats] INFO: Crawled 5702 pages (at 0 pages/min), scraped 40028 items (at 0 items/min)
2023-10-26 02:36:51 [scrapy.extensions.logstats] INFO: Crawled 42872 pages (at 0 pages/min), scraped 263015 items (at 0 items/min)
2023-10-26 02:37:09 [scrapy.extensions.logstats] INFO: Crawled 5706 pages (at 4 pages/min), scraped 40063 items (at 35 items/min)
2023-10-26 02:38:39 [scrapy.extensions.logstats] INFO: Crawled 42882 pages (at 10 pages/min), scraped 263096 items (at 81 items/min)
2023-10-26 02:38:39 [scrapy.extensions.logstats] INFO: Crawled 5716 pages (at 10 pages/min), scraped 40150 items (at 87 items/min)
2023-10-26 02:38:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=41748048&maxPrice=41754151. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:38:51 [scrapy.extensions.logstats] INFO: Crawled 42882 pages (at 0 pages/min), scraped 263096 items (at 0 items/min)
2023-10-26 02:39:09 [scrapy.extensions.logstats] INFO: Crawled 5716 pages (at 0 pages/min), scraped 40150 items (at 0 items/min)
2023-10-26 02:39:51 [scrapy.extensions.logstats] INFO: Crawled 42882 pages (at 0 pages/min), scraped 263096 items (at 0 items/min)
2023-10-26 02:40:09 [scrapy.extensions.logstats] INFO: Crawled 5716 pages (at 0 pages/min), scraped 40150 items (at 0 items/min)
2023-10-26 02:40:51 [scrapy.extensions.logstats] INFO: Crawled 42939 pages (at 57 pages/min), scraped 263292 items (at 196 items/min)
2023-10-26 02:41:09 [scrapy.extensions.logstats] INFO: Crawled 5748 pages (at 32 pages/min), scraped 40437 items (at 287 items/min)
2023-10-26 02:42:41 [scrapy.extensions.logstats] INFO: Crawled 5780 pages (at 32 pages/min), scraped 40716 items (at 279 items/min)
2023-10-26 02:42:41 [scrapy.extensions.logstats] INFO: Crawled 43075 pages (at 136 pages/min), scraped 264093 items (at 801 items/min)
2023-10-26 02:42:51 [scrapy.extensions.logstats] INFO: Crawled 43075 pages (at 0 pages/min), scraped 264093 items (at 0 items/min)
2023-10-26 02:43:09 [scrapy.extensions.logstats] INFO: Crawled 5780 pages (at 0 pages/min), scraped 40716 items (at 0 items/min)
2023-10-26 02:43:51 [scrapy.extensions.logstats] INFO: Crawled 43097 pages (at 22 pages/min), scraped 264250 items (at 157 items/min)
2023-10-26 02:44:09 [scrapy.extensions.logstats] INFO: Crawled 5787 pages (at 7 pages/min), scraped 40779 items (at 63 items/min)
2023-10-26 02:44:51 [scrapy.extensions.logstats] INFO: Crawled 43099 pages (at 2 pages/min), scraped 264277 items (at 27 items/min)
2023-10-26 02:45:09 [scrapy.extensions.logstats] INFO: Crawled 5812 pages (at 25 pages/min), scraped 40989 items (at 210 items/min)
2023-10-26 02:46:13 [scrapy.extensions.logstats] INFO: Crawled 43109 pages (at 10 pages/min), scraped 264367 items (at 90 items/min)
2023-10-26 02:46:13 [scrapy.extensions.logstats] INFO: Crawled 5813 pages (at 1 pages/min), scraped 40999 items (at 10 items/min)
2023-10-26 02:46:17 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=39721681&maxPrice=39746094. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:46:17 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=40795900&maxPrice=40798951. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:46:17 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=40640261&maxPrice=40643312. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:46:17 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=39648439&maxPrice=39672852. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 02:46:51 [scrapy.extensions.logstats] INFO: Crawled 43109 pages (at 0 pages/min), scraped 264367 items (at 0 items/min)
2023-10-26 02:47:09 [scrapy.extensions.logstats] INFO: Crawled 5814 pages (at 1 pages/min), scraped 41007 items (at 8 items/min)
2023-10-26 02:47:51 [scrapy.extensions.logstats] INFO: Crawled 43118 pages (at 9 pages/min), scraped 264384 items (at 17 items/min)
2023-10-26 02:48:09 [scrapy.extensions.logstats] INFO: Crawled 5839 pages (at 25 pages/min), scraped 41218 items (at 211 items/min)
2023-10-26 02:48:51 [scrapy.extensions.logstats] INFO: Crawled 43142 pages (at 24 pages/min), scraped 264504 items (at 120 items/min)
2023-10-26 02:49:09 [scrapy.extensions.logstats] INFO: Crawled 5862 pages (at 23 pages/min), scraped 41412 items (at 194 items/min)
2023-10-26 02:50:15 [scrapy.extensions.logstats] INFO: Crawled 43142 pages (at 0 pages/min), scraped 264504 items (at 0 items/min)
2023-10-26 02:50:16 [scrapy.extensions.logstats] INFO: Crawled 5862 pages (at 0 pages/min), scraped 41412 items (at 0 items/min)
2023-10-26 02:50:51 [scrapy.extensions.logstats] INFO: Crawled 43148 pages (at 6 pages/min), scraped 264511 items (at 7 items/min)
2023-10-26 02:51:09 [scrapy.extensions.logstats] INFO: Crawled 5862 pages (at 0 pages/min), scraped 41412 items (at 0 items/min)
2023-10-26 02:51:51 [scrapy.extensions.logstats] INFO: Crawled 43148 pages (at 0 pages/min), scraped 264511 items (at 0 items/min)
2023-10-26 02:52:09 [scrapy.extensions.logstats] INFO: Crawled 5863 pages (at 1 pages/min), scraped 41421 items (at 9 items/min)
2023-10-26 02:52:51 [scrapy.extensions.logstats] INFO: Crawled 43150 pages (at 2 pages/min), scraped 264519 items (at 8 items/min)
2023-10-26 02:53:09 [scrapy.extensions.logstats] INFO: Crawled 5870 pages (at 7 pages/min), scraped 41472 items (at 51 items/min)
2023-10-26 02:54:18 [scrapy.extensions.logstats] INFO: Crawled 5870 pages (at 0 pages/min), scraped 41472 items (at 0 items/min)
2023-10-26 02:54:18 [scrapy.extensions.logstats] INFO: Crawled 43154 pages (at 4 pages/min), scraped 264524 items (at 5 items/min)
2023-10-26 02:54:51 [scrapy.extensions.logstats] INFO: Crawled 43154 pages (at 0 pages/min), scraped 264524 items (at 0 items/min)
2023-10-26 02:55:09 [scrapy.extensions.logstats] INFO: Crawled 5871 pages (at 1 pages/min), scraped 41473 items (at 1 items/min)
2023-10-26 02:55:51 [scrapy.extensions.logstats] INFO: Crawled 43154 pages (at 0 pages/min), scraped 264524 items (at 0 items/min)
2023-10-26 02:56:09 [scrapy.extensions.logstats] INFO: Crawled 5904 pages (at 33 pages/min), scraped 41514 items (at 41 items/min)
2023-10-26 02:56:51 [scrapy.extensions.logstats] INFO: Crawled 43179 pages (at 25 pages/min), scraped 264549 items (at 25 items/min)
2023-10-26 02:57:09 [scrapy.extensions.logstats] INFO: Crawled 5927 pages (at 23 pages/min), scraped 41541 items (at 27 items/min)
2023-10-26 02:57:51 [scrapy.extensions.logstats] INFO: Crawled 43296 pages (at 117 pages/min), scraped 264665 items (at 116 items/min)
2023-10-26 02:58:51 [scrapy.extensions.logstats] INFO: Crawled 43571 pages (at 275 pages/min), scraped 265677 items (at 1012 items/min)
2023-10-26 02:59:51 [scrapy.extensions.logstats] INFO: Crawled 43922 pages (at 351 pages/min), scraped 267852 items (at 2175 items/min)
2023-10-26 03:00:51 [scrapy.extensions.logstats] INFO: Crawled 44174 pages (at 252 pages/min), scraped 269907 items (at 2055 items/min)
2023-10-26 03:01:51 [scrapy.extensions.logstats] INFO: Crawled 44377 pages (at 203 pages/min), scraped 271661 items (at 1754 items/min)
2023-10-26 03:02:51 [scrapy.extensions.logstats] INFO: Crawled 44725 pages (at 348 pages/min), scraped 274605 items (at 2944 items/min)
2023-10-26 03:03:51 [scrapy.extensions.logstats] INFO: Crawled 45052 pages (at 327 pages/min), scraped 277363 items (at 2758 items/min)
2023-10-26 03:04:51 [scrapy.extensions.logstats] INFO: Crawled 45294 pages (at 242 pages/min), scraped 278456 items (at 1093 items/min)
2023-10-26 03:04:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/130-little-collins-113>: HTTP status code is not handled or not allowed
2023-10-26 03:05:51 [scrapy.extensions.logstats] INFO: Crawled 45627 pages (at 333 pages/min), scraped 280410 items (at 1954 items/min)
2023-10-26 03:06:51 [scrapy.extensions.logstats] INFO: Crawled 45918 pages (at 291 pages/min), scraped 282369 items (at 1959 items/min)
2023-10-26 03:07:51 [scrapy.extensions.logstats] INFO: Crawled 46370 pages (at 452 pages/min), scraped 284287 items (at 1918 items/min)
2023-10-26 03:08:51 [scrapy.extensions.logstats] INFO: Crawled 46849 pages (at 479 pages/min), scraped 286896 items (at 2609 items/min)
2023-10-26 03:09:51 [scrapy.extensions.logstats] INFO: Crawled 47426 pages (at 577 pages/min), scraped 290884 items (at 3988 items/min)
2023-10-26 03:10:51 [scrapy.extensions.logstats] INFO: Crawled 48005 pages (at 579 pages/min), scraped 295457 items (at 4573 items/min)
2023-10-26 03:11:51 [scrapy.extensions.logstats] INFO: Crawled 48563 pages (at 558 pages/min), scraped 300461 items (at 5004 items/min)
2023-10-26 03:12:51 [scrapy.extensions.logstats] INFO: Crawled 49161 pages (at 598 pages/min), scraped 306073 items (at 5612 items/min)
2023-10-26 03:13:51 [scrapy.extensions.logstats] INFO: Crawled 49765 pages (at 604 pages/min), scraped 311725 items (at 5652 items/min)
2023-10-26 03:14:51 [scrapy.extensions.logstats] INFO: Crawled 50372 pages (at 607 pages/min), scraped 317262 items (at 5537 items/min)
2023-10-26 03:15:51 [scrapy.extensions.logstats] INFO: Crawled 50897 pages (at 525 pages/min), scraped 322252 items (at 4990 items/min)
2023-10-26 03:16:51 [scrapy.extensions.logstats] INFO: Crawled 51414 pages (at 517 pages/min), scraped 327053 items (at 4801 items/min)
2023-10-26 03:17:51 [scrapy.extensions.logstats] INFO: Crawled 51995 pages (at 581 pages/min), scraped 332644 items (at 5591 items/min)
2023-10-26 03:18:51 [scrapy.extensions.logstats] INFO: Crawled 52474 pages (at 479 pages/min), scraped 337277 items (at 4633 items/min)
2023-10-26 03:19:51 [scrapy.extensions.logstats] INFO: Crawled 52915 pages (at 441 pages/min), scraped 341327 items (at 4050 items/min)
2023-10-26 03:20:12 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6&minPrice=4837038&maxPrice=4840089. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 03:20:12 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6&minPrice=4867556&maxPrice=4870606. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 03:20:51 [scrapy.extensions.logstats] INFO: Crawled 53327 pages (at 412 pages/min), scraped 345141 items (at 3814 items/min)
2023-10-26 03:21:51 [scrapy.extensions.logstats] INFO: Crawled 53700 pages (at 373 pages/min), scraped 348529 items (at 3388 items/min)
2023-10-26 03:22:51 [scrapy.extensions.logstats] INFO: Crawled 54086 pages (at 386 pages/min), scraped 352236 items (at 3707 items/min)
2023-10-26 03:23:51 [scrapy.extensions.logstats] INFO: Crawled 54493 pages (at 407 pages/min), scraped 356054 items (at 3818 items/min)
2023-10-26 03:24:51 [scrapy.extensions.logstats] INFO: Crawled 54809 pages (at 316 pages/min), scraped 359010 items (at 2956 items/min)
2023-10-26 03:25:33 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4&minPrice=6842043&maxPrice=6845094. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 03:25:51 [scrapy.extensions.logstats] INFO: Crawled 55022 pages (at 213 pages/min), scraped 360978 items (at 1968 items/min)
2023-10-26 03:26:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:26:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:26:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:26:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:26:10 [scrapy.extensions.telnet] INFO: Telnet Password: 8a59b9d9f35349a4
2023-10-26 03:26:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:26:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:26:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:26:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:26:10 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:26:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:26:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:26:51 [scrapy.extensions.logstats] INFO: Crawled 55328 pages (at 306 pages/min), scraped 363849 items (at 2871 items/min)
2023-10-26 03:26:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-8-prima-court-ravenhall-vic-3023-504375704> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-26 03:26:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-1-28-hasler-road-osborne-park-wa-6017-504412964> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-the-corso-12-south-sea-islander-way-maroochydore-qld-4558-504433832> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-154-rathdowne-street-carlton-vic-3053-504450560> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-365-brunswick-street-fitzroy-vic-3065-504450552> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-64-old-maitland-road-hexham-nsw-2322-504429380> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-217-mcallisters-road-bilambil-heights-nsw-2486-504450512> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lot-3-55-kenyon-street-eagle-farm-qld-4009-504434116> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-retail-1-2-51-albany-street-crows-nest-nsw-2065-504442936> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-6-33-salamanca-pl-hobart-tas-7000-504412880> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-19-buontempo-road-carrum-downs-vic-3201-504450568> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 57, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-jutland-business-park-53-jutland-way-epping-vic-3076-504444856> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-16-18-ceylon-street-nunawading-vic-3131-504444964> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:57 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 03:26:57 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1f740f25b90>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 163, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:26:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 85070,
 'downloader/request_count': 167,
 'downloader/request_method_count/GET': 167,
 'downloader/response_bytes': 11477185,
 'downloader/response_count': 167,
 'downloader/response_status_count/200': 167,
 'elapsed_time_seconds': 47.248424,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 8, 26, 57, 776255),
 'httpcompression/response_bytes': 41089314,
 'httpcompression/response_count': 167,
 'item_scraped_count': 288,
 'log_count/ERROR': 15,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 16,
 'response_received_count': 167,
 'scheduler/dequeued': 167,
 'scheduler/dequeued/memory': 167,
 'scheduler/enqueued': 167,
 'scheduler/enqueued/memory': 167,
 'spider_exceptions/BrokenPipeError': 14,
 'start_time': datetime.datetime(2023, 10, 26, 8, 26, 10, 527831)}
2023-10-26 03:26:57 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 03:27:15 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:27:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:27:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:27:15 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:27:15 [scrapy.extensions.telnet] INFO: Telnet Password: 1d2c5ef3713c651a
2023-10-26 03:27:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:27:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:27:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:27:15 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:27:15 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:27:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:27:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:27:51 [scrapy.extensions.logstats] INFO: Crawled 55668 pages (at 340 pages/min), scraped 367002 items (at 3153 items/min)
2023-10-26 03:28:51 [scrapy.extensions.logstats] INFO: Crawled 56036 pages (at 368 pages/min), scraped 370447 items (at 3445 items/min)
2023-10-26 03:29:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:29:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:29:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:29:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:29:30 [scrapy.extensions.telnet] INFO: Telnet Password: 741c8117ba02cabc
2023-10-26 03:29:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:29:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:29:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:29:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:29:30 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:29:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:29:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:29:51 [scrapy.extensions.logstats] INFO: Crawled 56423 pages (at 387 pages/min), scraped 374044 items (at 3597 items/min)
2023-10-26 03:30:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:30:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:30:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:30:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:30:22 [scrapy.extensions.telnet] INFO: Telnet Password: 28570053df493f52
2023-10-26 03:30:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:30:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:30:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:30:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:30:22 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:30:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:30:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:30:51 [scrapy.extensions.logstats] INFO: Crawled 56816 pages (at 393 pages/min), scraped 377706 items (at 3662 items/min)
2023-10-26 03:31:51 [scrapy.extensions.logstats] INFO: Crawled 57249 pages (at 433 pages/min), scraped 381781 items (at 4075 items/min)
2023-10-26 03:32:51 [scrapy.extensions.logstats] INFO: Crawled 57582 pages (at 333 pages/min), scraped 384911 items (at 3130 items/min)
2023-10-26 03:33:51 [scrapy.extensions.logstats] INFO: Crawled 58004 pages (at 422 pages/min), scraped 388834 items (at 3923 items/min)
2023-10-26 03:34:29 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:34:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:34:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:34:29 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:34:29 [scrapy.extensions.telnet] INFO: Telnet Password: df43c9a9640090ef
2023-10-26 03:34:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:34:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:34:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:34:29 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:34:29 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:34:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:34:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:34:51 [scrapy.extensions.logstats] INFO: Crawled 58260 pages (at 256 pages/min), scraped 391214 items (at 2380 items/min)
2023-10-26 03:35:51 [scrapy.extensions.logstats] INFO: Crawled 58443 pages (at 183 pages/min), scraped 392920 items (at 1706 items/min)
2023-10-26 03:36:51 [scrapy.extensions.logstats] INFO: Crawled 58643 pages (at 200 pages/min), scraped 394787 items (at 1867 items/min)
2023-10-26 03:37:51 [scrapy.extensions.logstats] INFO: Crawled 58868 pages (at 225 pages/min), scraped 396919 items (at 2132 items/min)
2023-10-26 03:38:51 [scrapy.extensions.logstats] INFO: Crawled 59068 pages (at 200 pages/min), scraped 398833 items (at 1914 items/min)
2023-10-26 03:39:51 [scrapy.extensions.logstats] INFO: Crawled 59434 pages (at 366 pages/min), scraped 402259 items (at 3426 items/min)
2023-10-26 03:40:51 [scrapy.extensions.logstats] INFO: Crawled 59833 pages (at 399 pages/min), scraped 405882 items (at 3623 items/min)
2023-10-26 03:41:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:41:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:41:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:41:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:41:32 [scrapy.extensions.telnet] INFO: Telnet Password: 683324c9dc96edb4
2023-10-26 03:41:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:41:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:41:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:41:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:41:32 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:41:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:41:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:41:51 [scrapy.extensions.logstats] INFO: Crawled 60256 pages (at 423 pages/min), scraped 409770 items (at 3888 items/min)
2023-10-26 03:41:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:41:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:41:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:41:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:41:51 [scrapy.extensions.telnet] INFO: Telnet Password: 235f3b4bd8b8444a
2023-10-26 03:41:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:41:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:41:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:41:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:41:51 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:41:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:41:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:42:04 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 03:42:04 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 03:42:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 03:42:04 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 03:42:04 [scrapy.extensions.telnet] INFO: Telnet Password: 5bd1f35031115c40
2023-10-26 03:42:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 03:42:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 03:42:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 03:42:04 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 03:42:04 [scrapy.core.engine] INFO: Spider opened
2023-10-26 03:42:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 03:42:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 03:42:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 57, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-26 03:42:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-51-edward-street-brisbane-city-qld-4000-504413004> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:42:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-20-kleinhans-court-lowood-qld-4311-504413016> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:42:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-ruby-house-bay-street-double-bay-nsw-2028-504412936> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:42:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-8-yazaki-way-carrum-downs-vic-3201-504408904> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 156, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:42:34 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 03:42:34 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x18b5ba89410>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 163, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 03:42:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 62523,
 'downloader/request_count': 123,
 'downloader/request_method_count/GET': 123,
 'downloader/response_bytes': 8465747,
 'downloader/response_count': 123,
 'downloader/response_status_count/200': 123,
 'elapsed_time_seconds': 29.70754,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 8, 42, 34, 315075),
 'httpcompression/response_bytes': 30379169,
 'httpcompression/response_count': 123,
 'item_scraped_count': 217,
 'log_count/ERROR': 6,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 12,
 'response_received_count': 123,
 'scheduler/dequeued': 123,
 'scheduler/dequeued/memory': 123,
 'scheduler/enqueued': 123,
 'scheduler/enqueued/memory': 123,
 'spider_exceptions/BrokenPipeError': 5,
 'start_time': datetime.datetime(2023, 10, 26, 8, 42, 4, 607535)}
2023-10-26 03:42:34 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 03:42:51 [scrapy.extensions.logstats] INFO: Crawled 60590 pages (at 334 pages/min), scraped 412912 items (at 3142 items/min)
2023-10-26 03:43:51 [scrapy.extensions.logstats] INFO: Crawled 60975 pages (at 385 pages/min), scraped 416369 items (at 3457 items/min)
2023-10-26 03:44:51 [scrapy.extensions.logstats] INFO: Crawled 61240 pages (at 265 pages/min), scraped 418831 items (at 2462 items/min)
2023-10-26 03:45:51 [scrapy.extensions.logstats] INFO: Crawled 61502 pages (at 262 pages/min), scraped 421235 items (at 2404 items/min)
2023-10-26 03:46:51 [scrapy.extensions.logstats] INFO: Crawled 61912 pages (at 410 pages/min), scraped 425043 items (at 3808 items/min)
2023-10-26 03:47:51 [scrapy.extensions.logstats] INFO: Crawled 62329 pages (at 417 pages/min), scraped 428860 items (at 3817 items/min)
2023-10-26 03:48:51 [scrapy.extensions.logstats] INFO: Crawled 62718 pages (at 389 pages/min), scraped 431439 items (at 2579 items/min)
2023-10-26 03:49:51 [scrapy.extensions.logstats] INFO: Crawled 63081 pages (at 363 pages/min), scraped 434639 items (at 3200 items/min)
2023-10-26 03:50:51 [scrapy.extensions.logstats] INFO: Crawled 63403 pages (at 322 pages/min), scraped 437402 items (at 2763 items/min)
2023-10-26 03:51:51 [scrapy.extensions.logstats] INFO: Crawled 63690 pages (at 287 pages/min), scraped 439979 items (at 2577 items/min)
2023-10-26 03:52:51 [scrapy.extensions.logstats] INFO: Crawled 64083 pages (at 393 pages/min), scraped 443614 items (at 3635 items/min)
2023-10-26 03:53:51 [scrapy.extensions.logstats] INFO: Crawled 64495 pages (at 412 pages/min), scraped 447497 items (at 3883 items/min)
2023-10-26 03:54:51 [scrapy.extensions.logstats] INFO: Crawled 64829 pages (at 334 pages/min), scraped 450584 items (at 3087 items/min)
2023-10-26 03:55:51 [scrapy.extensions.logstats] INFO: Crawled 65236 pages (at 407 pages/min), scraped 454468 items (at 3884 items/min)
2023-10-26 03:56:51 [scrapy.extensions.logstats] INFO: Crawled 65601 pages (at 365 pages/min), scraped 457841 items (at 3373 items/min)
2023-10-26 03:57:51 [scrapy.extensions.logstats] INFO: Crawled 66049 pages (at 448 pages/min), scraped 461965 items (at 4124 items/min)
2023-10-26 03:58:51 [scrapy.extensions.logstats] INFO: Crawled 66454 pages (at 405 pages/min), scraped 465673 items (at 3708 items/min)
2023-10-26 03:59:51 [scrapy.extensions.logstats] INFO: Crawled 66853 pages (at 399 pages/min), scraped 469355 items (at 3682 items/min)
2023-10-26 04:00:51 [scrapy.extensions.logstats] INFO: Crawled 67290 pages (at 437 pages/min), scraped 473367 items (at 4012 items/min)
2023-10-26 04:01:51 [scrapy.extensions.logstats] INFO: Crawled 67541 pages (at 251 pages/min), scraped 475557 items (at 2190 items/min)
2023-10-26 04:02:51 [scrapy.extensions.logstats] INFO: Crawled 67916 pages (at 375 pages/min), scraped 478943 items (at 3386 items/min)
2023-10-26 04:03:51 [scrapy.extensions.logstats] INFO: Crawled 68364 pages (at 448 pages/min), scraped 482862 items (at 3919 items/min)
2023-10-26 04:04:51 [scrapy.extensions.logstats] INFO: Crawled 68743 pages (at 379 pages/min), scraped 486286 items (at 3424 items/min)
2023-10-26 04:05:21 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:05:21 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:05:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:05:21 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:05:21 [scrapy.extensions.telnet] INFO: Telnet Password: b707e731d316341c
2023-10-26 04:05:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:05:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:05:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:05:21 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:05:21 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:05:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:05:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:05:42 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:05:42 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:05:42 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:05:42 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:05:42 [scrapy.extensions.telnet] INFO: Telnet Password: 9825fbccfdef4178
2023-10-26 04:05:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:05:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:05:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:05:42 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:05:42 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:05:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:05:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:05:51 [scrapy.extensions.logstats] INFO: Crawled 68833 pages (at 90 pages/min), scraped 487100 items (at 814 items/min)
2023-10-26 04:05:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:05:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:06:51 [scrapy.extensions.logstats] INFO: Crawled 68855 pages (at 22 pages/min), scraped 487300 items (at 200 items/min)
2023-10-26 04:07:51 [scrapy.extensions.logstats] INFO: Crawled 68970 pages (at 115 pages/min), scraped 488330 items (at 1030 items/min)
2023-10-26 04:08:51 [scrapy.extensions.logstats] INFO: Crawled 69126 pages (at 156 pages/min), scraped 489756 items (at 1426 items/min)
2023-10-26 04:09:21 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:09:21 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:09:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:09:21 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:09:21 [scrapy.extensions.telnet] INFO: Telnet Password: 4556703e17dc0f29
2023-10-26 04:09:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:09:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:09:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:09:21 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:09:21 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:09:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:09:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:09:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:09:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:09:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:09:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:09:46 [scrapy.extensions.telnet] INFO: Telnet Password: 3979784927796f4a
2023-10-26 04:09:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:09:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:09:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:09:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:09:46 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:09:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:09:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:09:51 [scrapy.extensions.logstats] INFO: Crawled 69319 pages (at 193 pages/min), scraped 491450 items (at 1694 items/min)
2023-10-26 04:10:33 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:10:33 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:10:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:10:33 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:10:33 [scrapy.extensions.telnet] INFO: Telnet Password: 1dfe2641cd55abd9
2023-10-26 04:10:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:10:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:10:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:10:33 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:10:33 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:10:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:10:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:10:39 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 04:10:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5086,
 'downloader/request_count': 10,
 'downloader/request_method_count/GET': 10,
 'downloader/response_bytes': 701660,
 'downloader/response_count': 10,
 'downloader/response_status_count/200': 10,
 'elapsed_time_seconds': 5.800006,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 9, 10, 39, 680760),
 'httpcompression/response_bytes': 2601270,
 'httpcompression/response_count': 10,
 'item_scraped_count': 16,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 10,
 'scheduler/dequeued': 10,
 'scheduler/dequeued/memory': 10,
 'scheduler/enqueued': 10,
 'scheduler/enqueued/memory': 10,
 'start_time': datetime.datetime(2023, 10, 26, 9, 10, 33, 880754)}
2023-10-26 04:10:39 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 04:10:51 [scrapy.extensions.logstats] INFO: Crawled 69362 pages (at 43 pages/min), scraped 491814 items (at 364 items/min)
2023-10-26 04:11:51 [scrapy.extensions.logstats] INFO: Crawled 69382 pages (at 20 pages/min), scraped 492006 items (at 192 items/min)
2023-10-26 04:12:51 [scrapy.extensions.logstats] INFO: Crawled 69399 pages (at 17 pages/min), scraped 492136 items (at 130 items/min)
2023-10-26 04:13:51 [scrapy.extensions.logstats] INFO: Crawled 69472 pages (at 73 pages/min), scraped 492774 items (at 638 items/min)
2023-10-26 04:14:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:14:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:14:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:14:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:14:46 [scrapy.extensions.telnet] INFO: Telnet Password: 159be2f488372d24
2023-10-26 04:14:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:14:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:14:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:14:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:14:46 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:14:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:14:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:14:51 [scrapy.extensions.logstats] INFO: Crawled 69523 pages (at 51 pages/min), scraped 493196 items (at 422 items/min)
2023-10-26 04:15:46 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 8 items (at 8 items/min)
2023-10-26 04:15:51 [scrapy.extensions.logstats] INFO: Crawled 69527 pages (at 4 pages/min), scraped 493225 items (at 29 items/min)
2023-10-26 04:16:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 04:16:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 6441,
 'downloader/request_count': 13,
 'downloader/request_method_count/GET': 13,
 'downloader/response_bytes': 701505,
 'downloader/response_count': 10,
 'downloader/response_status_count/200': 10,
 'elapsed_time_seconds': 108.607703,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 9, 16, 35, 551018),
 'httpcompression/response_bytes': 2601237,
 'httpcompression/response_count': 10,
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 10,
 'retry/count': 3,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'scheduler/dequeued': 13,
 'scheduler/dequeued/memory': 13,
 'scheduler/enqueued': 13,
 'scheduler/enqueued/memory': 13,
 'start_time': datetime.datetime(2023, 10, 26, 9, 14, 46, 943315)}
2023-10-26 04:16:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 04:16:51 [scrapy.extensions.logstats] INFO: Crawled 69559 pages (at 32 pages/min), scraped 493519 items (at 294 items/min)
2023-10-26 04:17:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:17:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:17:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:17:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:17:14 [scrapy.extensions.telnet] INFO: Telnet Password: ca37a7362b8cfccb
2023-10-26 04:17:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:17:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:17:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:17:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:17:14 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:17:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:17:51 [scrapy.extensions.logstats] INFO: Crawled 69574 pages (at 15 pages/min), scraped 493669 items (at 150 items/min)
2023-10-26 04:18:14 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 12 pages/min), scraped 30 items (at 30 items/min)
2023-10-26 04:18:51 [scrapy.extensions.logstats] INFO: Crawled 69710 pages (at 136 pages/min), scraped 494907 items (at 1238 items/min)
2023-10-26 04:19:14 [scrapy.extensions.logstats] INFO: Crawled 156 pages (at 144 pages/min), scraped 291 items (at 261 items/min)
2023-10-26 04:19:51 [scrapy.extensions.logstats] INFO: Crawled 69925 pages (at 215 pages/min), scraped 496859 items (at 1952 items/min)
2023-10-26 04:20:14 [scrapy.extensions.logstats] INFO: Crawled 385 pages (at 229 pages/min), scraped 752 items (at 461 items/min)
2023-10-26 04:20:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-26 04:20:51 [scrapy.extensions.logstats] INFO: Crawled 70156 pages (at 231 pages/min), scraped 498755 items (at 1896 items/min)
2023-10-26 04:21:14 [scrapy.extensions.logstats] INFO: Crawled 454 pages (at 69 pages/min), scraped 944 items (at 192 items/min)
2023-10-26 04:21:51 [scrapy.extensions.logstats] INFO: Crawled 70217 pages (at 61 pages/min), scraped 499159 items (at 404 items/min)
2023-10-26 04:22:51 [scrapy.extensions.logstats] INFO: Crawled 70250 pages (at 33 pages/min), scraped 499352 items (at 193 items/min)
2023-10-26 04:23:51 [scrapy.extensions.logstats] INFO: Crawled 70308 pages (at 58 pages/min), scraped 499750 items (at 398 items/min)
2023-10-26 04:24:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 04:24:51 [scrapy.extensions.logstats] INFO: Crawled 70492 pages (at 184 pages/min), scraped 500656 items (at 906 items/min)
2023-10-26 04:25:51 [scrapy.extensions.logstats] INFO: Crawled 70907 pages (at 415 pages/min), scraped 503364 items (at 2708 items/min)
2023-10-26 04:26:51 [scrapy.extensions.logstats] INFO: Crawled 71296 pages (at 389 pages/min), scraped 506298 items (at 2934 items/min)
2023-10-26 04:27:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/smartstores@metroplex-133>: HTTP status code is not handled or not allowed
2023-10-26 04:27:51 [scrapy.extensions.logstats] INFO: Crawled 71532 pages (at 236 pages/min), scraped 506932 items (at 634 items/min)
2023-10-26 04:28:51 [scrapy.extensions.logstats] INFO: Crawled 71738 pages (at 206 pages/min), scraped 508240 items (at 1308 items/min)
2023-10-26 04:29:51 [scrapy.extensions.logstats] INFO: Crawled 72031 pages (at 293 pages/min), scraped 510106 items (at 1866 items/min)
2023-10-26 04:30:51 [scrapy.extensions.logstats] INFO: Crawled 72426 pages (at 395 pages/min), scraped 512056 items (at 1950 items/min)
2023-10-26 04:31:51 [scrapy.extensions.logstats] INFO: Crawled 72900 pages (at 474 pages/min), scraped 514413 items (at 2357 items/min)
2023-10-26 04:32:51 [scrapy.extensions.logstats] INFO: Crawled 73247 pages (at 347 pages/min), scraped 515579 items (at 1166 items/min)
2023-10-26 04:33:51 [scrapy.extensions.logstats] INFO: Crawled 73512 pages (at 265 pages/min), scraped 516844 items (at 1265 items/min)
2023-10-26 04:34:51 [scrapy.extensions.logstats] INFO: Crawled 73611 pages (at 99 pages/min), scraped 517245 items (at 401 items/min)
2023-10-26 04:35:51 [scrapy.extensions.logstats] INFO: Crawled 73986 pages (at 375 pages/min), scraped 519274 items (at 2029 items/min)
2023-10-26 04:36:51 [scrapy.extensions.logstats] INFO: Crawled 74201 pages (at 215 pages/min), scraped 520037 items (at 763 items/min)
2023-10-26 04:37:51 [scrapy.extensions.logstats] INFO: Crawled 74630 pages (at 429 pages/min), scraped 522848 items (at 2811 items/min)
2023-10-26 04:38:51 [scrapy.extensions.logstats] INFO: Crawled 74791 pages (at 161 pages/min), scraped 523672 items (at 824 items/min)
2023-10-26 04:39:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:39:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:39:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:39:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:39:14 [scrapy.extensions.telnet] INFO: Telnet Password: 66b3d3ed000415b5
2023-10-26 04:39:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:39:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:39:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:39:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:39:14 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:39:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:39:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:39:51 [scrapy.extensions.logstats] INFO: Crawled 74794 pages (at 3 pages/min), scraped 523693 items (at 21 items/min)
2023-10-26 04:40:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:40:51 [scrapy.extensions.logstats] INFO: Crawled 74798 pages (at 4 pages/min), scraped 523724 items (at 31 items/min)
2023-10-26 04:41:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:41:51 [scrapy.extensions.logstats] INFO: Crawled 74803 pages (at 5 pages/min), scraped 523756 items (at 32 items/min)
2023-10-26 04:42:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:42:51 [scrapy.extensions.logstats] INFO: Crawled 74810 pages (at 7 pages/min), scraped 523815 items (at 59 items/min)
2023-10-26 04:42:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 04:43:14 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 10 pages/min), scraped 27 items (at 27 items/min)
2023-10-26 04:43:51 [scrapy.extensions.logstats] INFO: Crawled 74817 pages (at 7 pages/min), scraped 523873 items (at 58 items/min)
2023-10-26 04:44:14 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 12 pages/min), scraped 48 items (at 21 items/min)
2023-10-26 04:44:51 [scrapy.extensions.logstats] INFO: Crawled 74829 pages (at 12 pages/min), scraped 523963 items (at 90 items/min)
2023-10-26 04:45:14 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 66 pages/min), scraped 176 items (at 128 items/min)
2023-10-26 04:45:42 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-26 04:45:42 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-26 04:45:42 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-26 04:45:42 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-26 04:45:42 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-26 04:45:42 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-615-derrimut-road-tarneit-vic-3029-504429692. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:45:42 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-2996-logan-road-underwood-qld-4119-504366872. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:45:51 [scrapy.extensions.logstats] INFO: Crawled 74990 pages (at 161 pages/min), scraped 525030 items (at 1067 items/min)
2023-10-26 04:46:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 04:46:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 04:46:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 04:46:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 04:46:00 [scrapy.extensions.telnet] INFO: Telnet Password: b42bb784f542972d
2023-10-26 04:46:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 04:46:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 04:46:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 04:46:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 04:46:00 [scrapy.core.engine] INFO: Spider opened
2023-10-26 04:46:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:46:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-g25-14-18-ethel-avenue-brookvale-nsw-2100-504406328. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-7-2756-albany-highway-kelmscott-wa-6111-504402392. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=33571&maxPrice=36621. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=39674&maxPrice=42725. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19&minPrice=292970&maxPrice=390625. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17&minPrice=1269533&maxPrice=1367188. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:40 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-18-george-street-morwell-vic-3840-504404676. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 04:46:51 [scrapy.extensions.logstats] INFO: Crawled 74993 pages (at 3 pages/min), scraped 525033 items (at 3 items/min)
2023-10-26 04:47:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:47:51 [scrapy.extensions.logstats] INFO: Crawled 74993 pages (at 0 pages/min), scraped 525033 items (at 0 items/min)
2023-10-26 04:48:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 04:48:51 [scrapy.extensions.logstats] INFO: Crawled 74993 pages (at 0 pages/min), scraped 525033 items (at 0 items/min)
2023-10-26 04:49:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 448,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 233,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 10,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 177,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 22,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 6,
 'downloader/request_bytes': 47212969,
 'downloader/request_count': 75441,
 'downloader/request_method_count/GET': 75441,
 'downloader/response_bytes': 5363851477,
 'downloader/response_count': 74993,
 'downloader/response_status_count/200': 74988,
 'downloader/response_status_count/404': 5,
 'dupefilter/filtered': 8,
 'elapsed_time_seconds': 23089.083323,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2023, 10, 26, 9, 49, 40, 225511),
 'httpcompression/response_bytes': 21716583725,
 'httpcompression/response_count': 74993,
 'httperror/response_ignored_count': 5,
 'httperror/response_ignored_status_count/404': 5,
 'item_scraped_count': 525033,
 'log_count/INFO': 400,
 'log_count/WARNING': 23,
 'request_depth_max': 479,
 'response_received_count': 74993,
 'retry/count': 448,
 'retry/reason_count/twisted.internet.error.ConnectError': 233,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 10,
 'retry/reason_count/twisted.internet.error.TimeoutError': 177,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 22,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 6,
 'scheduler/dequeued': 75441,
 'scheduler/dequeued/memory': 75441,
 'scheduler/enqueued': 76130,
 'scheduler/enqueued/memory': 76130,
 'start_time': datetime.datetime(2023, 10, 26, 3, 24, 51, 142188)}
2023-10-26 04:49:40 [scrapy.core.engine] INFO: Spider closed (shutdown)
2023-10-26 05:00:52 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:00:52 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:00:52 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:00:52 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:00:52 [scrapy.extensions.telnet] INFO: Telnet Password: d0d54b38cdd2dd43
2023-10-26 05:00:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:00:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:00:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:00:52 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:00:52 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:00:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:00:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:14:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:14:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:14:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:14:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:14:58 [scrapy.extensions.telnet] INFO: Telnet Password: 84da62bdd5441b24
2023-10-26 05:14:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:14:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:14:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:14:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:14:58 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:14:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:14:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:15:58 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 15 pages/min), scraped 42 items (at 42 items/min)
2023-10-26 05:16:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 05:16:58 [scrapy.extensions.logstats] INFO: Crawled 116 pages (at 101 pages/min), scraped 223 items (at 181 items/min)
2023-10-26 05:17:58 [scrapy.extensions.logstats] INFO: Crawled 161 pages (at 45 pages/min), scraped 304 items (at 81 items/min)
2023-10-26 05:18:58 [scrapy.extensions.logstats] INFO: Crawled 215 pages (at 54 pages/min), scraped 403 items (at 99 items/min)
2023-10-26 05:19:58 [scrapy.extensions.logstats] INFO: Crawled 291 pages (at 76 pages/min), scraped 542 items (at 139 items/min)
2023-10-26 05:20:58 [scrapy.extensions.logstats] INFO: Crawled 337 pages (at 46 pages/min), scraped 642 items (at 100 items/min)
2023-10-26 05:21:58 [scrapy.extensions.logstats] INFO: Crawled 353 pages (at 16 pages/min), scraped 685 items (at 43 items/min)
2023-10-26 05:22:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-405-thynne-road-morningside-qld-4170-504356928> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:22:58 [scrapy.extensions.logstats] INFO: Crawled 365 pages (at 12 pages/min), scraped 723 items (at 38 items/min)
2023-10-26 05:22:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-2-6-station-street-penrith-nsw-2750-504352800> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-cobbitty-nsw-2570-503927082> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-torrumbarry-vic-3562-504361360> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-bella-vista-morris-road-euston-nsw-2737-504362388> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-41-campbell-street-bowen-hills-qld-4006-504362220> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6&minPrice=50000001&maxPrice=100000000> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-90-handford-lane-officer-vic-3809-504303256> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-cheltenham-vic-3192-504312556> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-1860-princes-freeway-avalon-vic-3212-504323800> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:23:58 [scrapy.extensions.logstats] INFO: Crawled 375 pages (at 10 pages/min), scraped 723 items (at 0 items/min)
2023-10-26 05:24:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-burmah-aggregation-burmah-road-graman-nsw-2360-504325600> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:24:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-16-18-wentworth-street-parramatta-nsw-2150-504325668> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-54-miller-street-north-sydney-nsw-2060-504324724> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:24:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-885-889-mamre-rd-kemps-creek-nsw-2178-504327108> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:24:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-13a-union-st-pyrmont-nsw-2009-504332100> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:24:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-borambil-station-lachlan-valley-way-condobolin-nsw-2877-504325596> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5&minPrice=50000001&maxPrice=100000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:24:23 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-26 05:24:23 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-26 05:24:24 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-26 05:24:24 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-191-miller-road-chester-hill-nsw-2162-504276984. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:24:24 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-339-coronation-drive-milton-qld-4064-504351104. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:24:24 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-nightingale-health-innovation-precinct-7005-wellness-way-springfield-central-qld-4300-504254164. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:24:24 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au//for-sale/property-2-48-new-street-south-kingsville-vic-3015-504367408. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:24:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:24:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:24:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:24:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:24:37 [scrapy.extensions.telnet] INFO: Telnet Password: 1ab41b970f96216b
2023-10-26 05:24:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:24:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:24:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:24:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:24:37 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:24:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:24:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:25:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding> (failed 3 times): DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-26 05:25:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-26 05:25:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 05:25:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 861,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 34.375425,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 10, 25, 11, 821451),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 26, 10, 24, 37, 446026)}
2023-10-26 05:25:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 05:25:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:25:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:25:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:25:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:25:30 [scrapy.extensions.telnet] INFO: Telnet Password: a9b4b0a2961d78ae
2023-10-26 05:25:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:25:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:25:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:25:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:25:30 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:25:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:25:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:28:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:28:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:28:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:28:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:28:56 [scrapy.extensions.telnet] INFO: Telnet Password: 423f149bde890676
2023-10-26 05:28:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:28:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:28:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:28:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:28:56 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:28:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:28:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:29:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 05:29:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:29:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:29:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:29:25 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:29:25 [scrapy.extensions.telnet] INFO: Telnet Password: 3b03a4e3d6223c20
2023-10-26 05:29:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:29:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:29:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:29:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:29:25 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:29:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:29:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:29:54 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:30:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:31:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:32:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:32:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding> (failed 3 times): User timeout caused connection failure: Getting https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding took longer than 180.0 seconds..
2023-10-26 05:32:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 398, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding took longer than 180.0 seconds..
2023-10-26 05:32:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 05:32:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 861,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 208.684717,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 10, 32, 54, 314190),
 'log_count/ERROR': 2,
 'log_count/INFO': 13,
 'log_count/WARNING': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 26, 10, 29, 25, 629473)}
2023-10-26 05:32:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 05:36:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:36:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:36:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:36:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:36:34 [scrapy.extensions.telnet] INFO: Telnet Password: 2088d67d0776ebe9
2023-10-26 05:36:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:36:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:36:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:36:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:36:34 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:36:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:36:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:36:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 05:37:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-124-bridport-street-albert-park-vic-3206-504429412> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-26 05:37:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-1-6-32-french-avenue-brendale-qld-4500-504425972> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:37:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-the-north-melbourne-corner-queensberry-st-laurens-st-munster-tce-north-melbourne-vic-3051-504418692> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:37:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-newman-s-horseradish-262-lake-plains-road-langhorne-creek-sa-5255-504407404> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:37:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:37:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-195-wellington-road-clayton-vic-3168-504407396> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:37:20 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 05:37:20 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x2c4f4354ad0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 165, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:37:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 86644,
 'downloader/request_count': 154,
 'downloader/request_method_count/GET': 154,
 'downloader/response_bytes': 10567685,
 'downloader/response_count': 154,
 'downloader/response_status_count/200': 153,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 45.719975,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 10, 37, 20, 189960),
 'httpcompression/response_bytes': 37792790,
 'httpcompression/response_count': 154,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 273,
 'log_count/ERROR': 7,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 15,
 'response_received_count': 154,
 'scheduler/dequeued': 154,
 'scheduler/dequeued/memory': 154,
 'scheduler/enqueued': 154,
 'scheduler/enqueued/memory': 154,
 'spider_exceptions/BrokenPipeError': 6,
 'start_time': datetime.datetime(2023, 10, 26, 10, 36, 34, 469985)}
2023-10-26 05:37:20 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 05:39:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 05:39:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 05:39:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 05:39:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 05:39:46 [scrapy.extensions.telnet] INFO: Telnet Password: 100840d493ba620c
2023-10-26 05:39:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 05:39:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 05:39:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 05:39:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 05:39:46 [scrapy.core.engine] INFO: Spider opened
2023-10-26 05:39:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 05:39:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 05:39:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 05:40:46 [scrapy.extensions.logstats] INFO: Crawled 207 pages (at 207 pages/min), scraped 385 items (at 385 items/min)
2023-10-26 05:41:46 [scrapy.extensions.logstats] INFO: Crawled 414 pages (at 207 pages/min), scraped 876 items (at 491 items/min)
2023-10-26 05:42:05 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92968751&maxPrice=93750000. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:42:05 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89062501&maxPrice=90625000. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:42:05 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92187501&maxPrice=92968750. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:42:05 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94531251&maxPrice=95312500. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 05:42:46 [scrapy.extensions.logstats] INFO: Crawled 550 pages (at 136 pages/min), scraped 1393 items (at 517 items/min)
2023-10-26 05:43:46 [scrapy.extensions.logstats] INFO: Crawled 555 pages (at 5 pages/min), scraped 1416 items (at 23 items/min)
2023-10-26 05:44:46 [scrapy.extensions.logstats] INFO: Crawled 559 pages (at 4 pages/min), scraped 1424 items (at 8 items/min)
2023-10-26 05:45:46 [scrapy.extensions.logstats] INFO: Crawled 559 pages (at 0 pages/min), scraped 1424 items (at 0 items/min)
2023-10-26 05:46:46 [scrapy.extensions.logstats] INFO: Crawled 606 pages (at 47 pages/min), scraped 1424 items (at 0 items/min)
2023-10-26 05:47:46 [scrapy.extensions.logstats] INFO: Crawled 657 pages (at 51 pages/min), scraped 1424 items (at 0 items/min)
2023-10-26 05:48:46 [scrapy.extensions.logstats] INFO: Crawled 712 pages (at 55 pages/min), scraped 1424 items (at 0 items/min)
2023-10-26 05:49:46 [scrapy.extensions.logstats] INFO: Crawled 745 pages (at 33 pages/min), scraped 1424 items (at 0 items/min)
2023-10-26 05:50:10 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-26 05:50:10 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-26 05:50:40 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x19e1f5a4e50>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\task.py", line 526, in _oneWorkUnit
    result = next(self._iterator)
             ^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 165, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 05:50:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 13,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 8,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 4,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 469351,
 'downloader/request_count': 782,
 'downloader/request_method_count/GET': 782,
 'downloader/response_bytes': 52032968,
 'downloader/response_count': 769,
 'downloader/response_status_count/200': 768,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 654.057892,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2023, 10, 26, 10, 50, 40, 899741),
 'httpcompression/response_bytes': 189057955,
 'httpcompression/response_count': 769,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 1424,
 'log_count/ERROR': 1,
 'log_count/INFO': 22,
 'log_count/WARNING': 5,
 'request_depth_max': 58,
 'response_received_count': 769,
 'retry/count': 13,
 'retry/reason_count/twisted.internet.error.TimeoutError': 8,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 4,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'scheduler/dequeued': 782,
 'scheduler/dequeued/memory': 782,
 'scheduler/enqueued': 946,
 'scheduler/enqueued/memory': 946,
 'start_time': datetime.datetime(2023, 10, 26, 10, 39, 46, 841849)}
2023-10-26 05:50:40 [scrapy.core.engine] INFO: Spider closed (shutdown)
2023-10-26 05:50:41 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-26 06:01:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 06:01:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 06:01:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 06:01:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 06:01:58 [scrapy.extensions.telnet] INFO: Telnet Password: 628f70ed60781664
2023-10-26 06:01:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 06:01:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 06:01:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 06:01:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 06:01:58 [scrapy.core.engine] INFO: Spider opened
2023-10-26 06:01:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 06:01:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 06:02:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 06:02:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-26 06:02:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-126-federation-way-telegraph-point-nsw-2441-504368128> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 06:02:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-metro-petroleum-38-morayfield-road-caboolture-south-qld-4510-504429728> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 06:02:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 06:02:14 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1c2de845250>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 165, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-26 06:02:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 11425,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 1478347,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 16.070413,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 11, 2, 14, 103578),
 'httpcompression/response_bytes': 5368787,
 'httpcompression/response_count': 22,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 36,
 'log_count/ERROR': 4,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 3,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/BrokenPipeError': 2,
 'spider_exceptions/EOFError': 1,
 'start_time': datetime.datetime(2023, 10, 26, 11, 1, 58, 33165)}
2023-10-26 06:02:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 06:15:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 06:15:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 06:15:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 06:15:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 06:15:31 [scrapy.extensions.telnet] INFO: Telnet Password: d4e91cd12061fa7c
2023-10-26 06:15:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 06:15:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 06:15:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 06:15:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 06:15:31 [scrapy.core.engine] INFO: Spider opened
2023-10-26 06:15:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 06:15:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 06:16:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 06:17:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 06:18:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 06:18:31 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 16 items (at 16 items/min)
2023-10-26 06:19:31 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 1 pages/min), scraped 26 items (at 10 items/min)
2023-10-26 06:20:31 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 17 pages/min), scraped 52 items (at 26 items/min)
2023-10-26 06:21:31 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 12 pages/min), scraped 73 items (at 21 items/min)
2023-10-26 06:22:31 [scrapy.extensions.logstats] INFO: Crawled 61 pages (at 23 pages/min), scraped 123 items (at 50 items/min)
2023-10-26 06:23:31 [scrapy.extensions.logstats] INFO: Crawled 179 pages (at 118 pages/min), scraped 339 items (at 216 items/min)
2023-10-26 06:24:31 [scrapy.extensions.logstats] INFO: Crawled 264 pages (at 85 pages/min), scraped 488 items (at 149 items/min)
2023-10-26 06:25:31 [scrapy.extensions.logstats] INFO: Crawled 359 pages (at 95 pages/min), scraped 691 items (at 203 items/min)
2023-10-26 06:26:31 [scrapy.extensions.logstats] INFO: Crawled 699 pages (at 340 pages/min), scraped 1423 items (at 732 items/min)
2023-10-26 06:27:31 [scrapy.extensions.logstats] INFO: Crawled 888 pages (at 189 pages/min), scraped 1612 items (at 189 items/min)
2023-10-26 06:28:31 [scrapy.extensions.logstats] INFO: Crawled 1000 pages (at 112 pages/min), scraped 1723 items (at 111 items/min)
2023-10-26 06:29:31 [scrapy.extensions.logstats] INFO: Crawled 1131 pages (at 131 pages/min), scraped 1855 items (at 132 items/min)
2023-10-26 06:30:31 [scrapy.extensions.logstats] INFO: Crawled 1183 pages (at 52 pages/min), scraped 1907 items (at 52 items/min)
2023-10-26 06:31:31 [scrapy.extensions.logstats] INFO: Crawled 1470 pages (at 287 pages/min), scraped 2192 items (at 285 items/min)
2023-10-26 06:32:31 [scrapy.extensions.logstats] INFO: Crawled 1852 pages (at 382 pages/min), scraped 2650 items (at 458 items/min)
2023-10-26 06:33:31 [scrapy.extensions.logstats] INFO: Crawled 2285 pages (at 433 pages/min), scraped 4575 items (at 1925 items/min)
2023-10-26 06:34:31 [scrapy.extensions.logstats] INFO: Crawled 2648 pages (at 363 pages/min), scraped 6007 items (at 1432 items/min)
2023-10-26 06:35:31 [scrapy.extensions.logstats] INFO: Crawled 3044 pages (at 396 pages/min), scraped 7363 items (at 1356 items/min)
2023-10-26 06:36:31 [scrapy.extensions.logstats] INFO: Crawled 3395 pages (at 351 pages/min), scraped 8049 items (at 686 items/min)
2023-10-26 06:37:31 [scrapy.extensions.logstats] INFO: Crawled 3827 pages (at 432 pages/min), scraped 8720 items (at 671 items/min)
2023-10-26 06:38:31 [scrapy.extensions.logstats] INFO: Crawled 4259 pages (at 432 pages/min), scraped 9461 items (at 741 items/min)
2023-10-26 06:39:31 [scrapy.extensions.logstats] INFO: Crawled 4636 pages (at 377 pages/min), scraped 9964 items (at 503 items/min)
2023-10-26 06:40:31 [scrapy.extensions.logstats] INFO: Crawled 4921 pages (at 285 pages/min), scraped 10250 items (at 286 items/min)
2023-10-26 06:41:31 [scrapy.extensions.logstats] INFO: Crawled 5164 pages (at 243 pages/min), scraped 10493 items (at 243 items/min)
2023-10-26 06:42:31 [scrapy.extensions.logstats] INFO: Crawled 5522 pages (at 358 pages/min), scraped 10851 items (at 358 items/min)
2023-10-26 06:43:31 [scrapy.extensions.logstats] INFO: Crawled 5935 pages (at 413 pages/min), scraped 11264 items (at 413 items/min)
2023-10-26 06:44:31 [scrapy.extensions.logstats] INFO: Crawled 6376 pages (at 441 pages/min), scraped 11794 items (at 530 items/min)
2023-10-26 06:45:31 [scrapy.extensions.logstats] INFO: Crawled 6812 pages (at 436 pages/min), scraped 12419 items (at 625 items/min)
2023-10-26 06:46:31 [scrapy.extensions.logstats] INFO: Crawled 7258 pages (at 446 pages/min), scraped 13294 items (at 875 items/min)
2023-10-26 06:47:31 [scrapy.extensions.logstats] INFO: Crawled 7508 pages (at 250 pages/min), scraped 13832 items (at 538 items/min)
2023-10-26 06:48:49 [scrapy.extensions.logstats] INFO: Crawled 7605 pages (at 97 pages/min), scraped 14030 items (at 198 items/min)
2023-10-26 06:49:31 [scrapy.extensions.logstats] INFO: Crawled 7961 pages (at 356 pages/min), scraped 14733 items (at 703 items/min)
2023-10-26 06:50:31 [scrapy.extensions.logstats] INFO: Crawled 8488 pages (at 527 pages/min), scraped 15780 items (at 1047 items/min)
2023-10-26 06:52:20 [scrapy.extensions.logstats] INFO: Crawled 8953 pages (at 465 pages/min), scraped 16618 items (at 838 items/min)
2023-10-26 06:52:31 [scrapy.extensions.logstats] INFO: Crawled 8958 pages (at 5 pages/min), scraped 16624 items (at 6 items/min)
2023-10-26 06:53:31 [scrapy.extensions.logstats] INFO: Crawled 9109 pages (at 151 pages/min), scraped 16813 items (at 189 items/min)
2023-10-26 06:54:31 [scrapy.extensions.logstats] INFO: Crawled 9317 pages (at 208 pages/min), scraped 17106 items (at 293 items/min)
2023-10-26 06:55:52 [scrapy.extensions.logstats] INFO: Crawled 9470 pages (at 153 pages/min), scraped 17344 items (at 238 items/min)
2023-10-26 06:56:31 [scrapy.extensions.logstats] INFO: Crawled 9672 pages (at 202 pages/min), scraped 17717 items (at 373 items/min)
2023-10-26 06:57:31 [scrapy.extensions.logstats] INFO: Crawled 10099 pages (at 427 pages/min), scraped 19992 items (at 2275 items/min)
2023-10-26 06:59:23 [scrapy.extensions.logstats] INFO: Crawled 10526 pages (at 427 pages/min), scraped 21639 items (at 1647 items/min)
2023-10-26 06:59:31 [scrapy.extensions.logstats] INFO: Crawled 10537 pages (at 11 pages/min), scraped 21665 items (at 26 items/min)
2023-10-26 07:00:31 [scrapy.extensions.logstats] INFO: Crawled 11014 pages (at 477 pages/min), scraped 23763 items (at 2098 items/min)
2023-10-26 07:01:31 [scrapy.extensions.logstats] INFO: Crawled 11520 pages (at 506 pages/min), scraped 25887 items (at 2124 items/min)
2023-10-26 07:02:54 [scrapy.extensions.logstats] INFO: Crawled 11685 pages (at 165 pages/min), scraped 26877 items (at 990 items/min)
2023-10-26 07:03:31 [scrapy.extensions.logstats] INFO: Crawled 11873 pages (at 188 pages/min), scraped 27994 items (at 1117 items/min)
2023-10-26 07:04:31 [scrapy.extensions.logstats] INFO: Crawled 12274 pages (at 401 pages/min), scraped 30355 items (at 2361 items/min)
2023-10-26 07:06:26 [scrapy.extensions.logstats] INFO: Crawled 12685 pages (at 411 pages/min), scraped 32788 items (at 2433 items/min)
2023-10-26 07:06:31 [scrapy.extensions.logstats] INFO: Crawled 12685 pages (at 0 pages/min), scraped 32788 items (at 0 items/min)
2023-10-26 07:07:31 [scrapy.extensions.logstats] INFO: Crawled 12808 pages (at 123 pages/min), scraped 33491 items (at 703 items/min)
2023-10-26 07:08:31 [scrapy.extensions.logstats] INFO: Crawled 12920 pages (at 112 pages/min), scraped 34119 items (at 628 items/min)
2023-10-26 07:09:58 [scrapy.extensions.logstats] INFO: Crawled 13070 pages (at 150 pages/min), scraped 35009 items (at 890 items/min)
2023-10-26 07:10:06 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=51965334&maxPrice=51977540. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:10:06 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=51953126&maxPrice=51965333. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:10:31 [scrapy.extensions.logstats] INFO: Crawled 13126 pages (at 56 pages/min), scraped 35348 items (at 339 items/min)
2023-10-26 07:11:31 [scrapy.extensions.logstats] INFO: Crawled 13364 pages (at 238 pages/min), scraped 36764 items (at 1416 items/min)
2023-10-26 07:13:30 [scrapy.extensions.logstats] INFO: Crawled 13597 pages (at 233 pages/min), scraped 38160 items (at 1396 items/min)
2023-10-26 07:13:31 [scrapy.extensions.logstats] INFO: Crawled 13597 pages (at 0 pages/min), scraped 38160 items (at 0 items/min)
2023-10-26 07:13:33 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=54574587&maxPrice=54577637. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:13:33 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=56234743&maxPrice=56237793. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:14:31 [scrapy.extensions.logstats] INFO: Crawled 13724 pages (at 127 pages/min), scraped 38759 items (at 599 items/min)
2023-10-26 07:15:31 [scrapy.extensions.logstats] INFO: Crawled 13874 pages (at 150 pages/min), scraped 39517 items (at 758 items/min)
2023-10-26 07:17:02 [scrapy.extensions.logstats] INFO: Crawled 14102 pages (at 228 pages/min), scraped 40871 items (at 1354 items/min)
2023-10-26 07:17:31 [scrapy.extensions.logstats] INFO: Crawled 14287 pages (at 185 pages/min), scraped 41505 items (at 634 items/min)
2023-10-26 07:18:31 [scrapy.extensions.logstats] INFO: Crawled 14805 pages (at 518 pages/min), scraped 42103 items (at 598 items/min)
2023-10-26 07:19:31 [scrapy.extensions.logstats] INFO: Crawled 15138 pages (at 333 pages/min), scraped 42816 items (at 713 items/min)
2023-10-26 07:20:32 [scrapy.extensions.logstats] INFO: Crawled 15138 pages (at 0 pages/min), scraped 42816 items (at 0 items/min)
2023-10-26 07:21:31 [scrapy.extensions.logstats] INFO: Crawled 15523 pages (at 385 pages/min), scraped 44306 items (at 1490 items/min)
2023-10-26 07:22:31 [scrapy.extensions.logstats] INFO: Crawled 15961 pages (at 438 pages/min), scraped 45464 items (at 1158 items/min)
2023-10-26 07:24:04 [scrapy.extensions.logstats] INFO: Crawled 16235 pages (at 274 pages/min), scraped 46370 items (at 906 items/min)
2023-10-26 07:24:08 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=61224367&maxPrice=61227418. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:24:31 [scrapy.extensions.logstats] INFO: Crawled 16306 pages (at 71 pages/min), scraped 46628 items (at 258 items/min)
2023-10-26 07:25:31 [scrapy.extensions.logstats] INFO: Crawled 16520 pages (at 214 pages/min), scraped 47092 items (at 464 items/min)
2023-10-26 07:26:31 [scrapy.extensions.logstats] INFO: Crawled 16827 pages (at 307 pages/min), scraped 48259 items (at 1167 items/min)
2023-10-26 07:27:35 [scrapy.extensions.logstats] INFO: Crawled 16846 pages (at 19 pages/min), scraped 48332 items (at 73 items/min)
2023-10-26 07:27:39 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85510255&maxPrice=85513306. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:27:39 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=60623171&maxPrice=60626222. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:27:39 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=60620119&maxPrice=60623170. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:28:31 [scrapy.extensions.logstats] INFO: Crawled 17099 pages (at 253 pages/min), scraped 48668 items (at 336 items/min)
2023-10-26 07:29:31 [scrapy.extensions.logstats] INFO: Crawled 17426 pages (at 327 pages/min), scraped 49152 items (at 484 items/min)
2023-10-26 07:30:31 [scrapy.extensions.logstats] INFO: Crawled 17918 pages (at 492 pages/min), scraped 49656 items (at 504 items/min)
2023-10-26 07:30:54 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-26 07:30:54 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-26 07:30:56 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-26 07:30:56 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=97473146&maxPrice=97479249. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:30:56 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=97491457&maxPrice=97497559. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 07:31:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 07:31:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 07:31:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 07:31:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 07:31:14 [scrapy.extensions.telnet] INFO: Telnet Password: c43d76f962c005ad
2023-10-26 07:31:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 07:31:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 07:31:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 07:31:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 07:31:14 [scrapy.core.engine] INFO: Spider opened
2023-10-26 07:31:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 07:31:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 07:31:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 07:32:14 [scrapy.extensions.logstats] INFO: Crawled 249 pages (at 249 pages/min), scraped 455 items (at 455 items/min)
2023-10-26 07:33:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 07:33:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 07:33:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 07:33:07 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 07:33:07 [scrapy.extensions.telnet] INFO: Telnet Password: 8a7f54ec904321d9
2023-10-26 07:33:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 07:33:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 07:33:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 07:33:07 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 07:33:07 [scrapy.core.engine] INFO: Spider opened
2023-10-26 07:33:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 07:33:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-26 07:33:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 07:33:14 [scrapy.extensions.logstats] INFO: Crawled 634 pages (at 385 pages/min), scraped 1476 items (at 1021 items/min)
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-sale/property-84-88-dacmar-road-coolum-beach-qld-4573-504418616>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90029909&maxPrice=90032960>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92541505&maxPrice=92547608>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90026857&maxPrice=90029908>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-sale/property-shops-1-5-168-186-little-collins-street-melbourne-vic-3000-504444928>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92523195&maxPrice=92526246>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92526247&maxPrice=92529297>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68176271&maxPrice=68188477>: HTTP status code is not handled or not allowed
2023-10-26 07:33:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92486574&maxPrice=92492676>: HTTP status code is not handled or not allowed
2023-10-26 07:33:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-sale/property-unit-9-663-victoria-street-abbotsford-vic-3067-504450472>: HTTP status code is not handled or not allowed
2023-10-26 07:33:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92547609&maxPrice=92553711>: HTTP status code is not handled or not allowed
2023-10-26 07:33:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18>: HTTP status code is not handled or not allowed
2023-10-26 07:33:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68200685&maxPrice=68212891>: HTTP status code is not handled or not allowed
2023-10-26 07:33:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68188478&maxPrice=68200684>: HTTP status code is not handled or not allowed
2023-10-26 07:33:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 07:33:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 103700,
 'downloader/request_count': 184,
 'downloader/request_method_count/GET': 184,
 'downloader/response_bytes': 12320624,
 'downloader/response_count': 184,
 'downloader/response_status_count/200': 179,
 'downloader/response_status_count/403': 4,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 44.327008,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 12, 33, 51, 995916),
 'httpcompression/response_bytes': 44080901,
 'httpcompression/response_count': 180,
 'httperror/response_ignored_count': 5,
 'httperror/response_ignored_status_count/403': 4,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 332,
 'log_count/INFO': 15,
 'log_count/WARNING': 1,
 'request_depth_max': 17,
 'response_received_count': 184,
 'scheduler/dequeued': 184,
 'scheduler/dequeued/memory': 184,
 'scheduler/enqueued': 184,
 'scheduler/enqueued/memory': 184,
 'start_time': datetime.datetime(2023, 10, 26, 12, 33, 7, 668908)}
2023-10-26 07:33:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 07:33:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92535402&maxPrice=92538453>: HTTP status code is not handled or not allowed
2023-10-26 07:33:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92517091&maxPrice=92520142>: HTTP status code is not handled or not allowed
2023-10-26 07:33:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92538454&maxPrice=92541504>: HTTP status code is not handled or not allowed
2023-10-26 07:33:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68164064&maxPrice=68176270>: HTTP status code is not handled or not allowed
2023-10-26 07:33:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92520143&maxPrice=92523194>: HTTP status code is not handled or not allowed
2023-10-26 07:33:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92480470&maxPrice=92486573>: HTTP status code is not handled or not allowed
2023-10-26 07:33:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92514040&maxPrice=92517090>: HTTP status code is not handled or not allowed
2023-10-26 07:33:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92510988&maxPrice=92514039>: HTTP status code is not handled or not allowed
2023-10-26 07:33:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92504884&maxPrice=92507935>: HTTP status code is not handled or not allowed
2023-10-26 07:33:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92498781&maxPrice=92504883>: HTTP status code is not handled or not allowed
2023-10-26 07:33:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92492677&maxPrice=92498780>: HTTP status code is not handled or not allowed
2023-10-26 07:33:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90032961&maxPrice=90036012>: HTTP status code is not handled or not allowed
2023-10-26 07:33:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92529298&maxPrice=92535401>: HTTP status code is not handled or not allowed
2023-10-26 07:33:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92852785&maxPrice=92855836>: HTTP status code is not handled or not allowed
2023-10-26 07:33:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92507936&maxPrice=92510987>: HTTP status code is not handled or not allowed
2023-10-26 07:33:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94470216&maxPrice=94476319>: HTTP status code is not handled or not allowed
2023-10-26 07:33:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67337038&maxPrice=67340089>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94464113&maxPrice=94470215>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94458009&maxPrice=94464112>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96624758&maxPrice=96630860>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96618654&maxPrice=96624757>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67333986&maxPrice=67337037>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94348146&maxPrice=94360352>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94335939&maxPrice=94348145>: HTTP status code is not handled or not allowed
2023-10-26 07:33:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96557619&maxPrice=96569825>: HTTP status code is not handled or not allowed
2023-10-26 07:33:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67285158&maxPrice=67333985>: HTTP status code is not handled or not allowed
2023-10-26 07:33:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92834474&maxPrice=92846680>: HTTP status code is not handled or not allowed
2023-10-26 07:33:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77099611&maxPrice=77148438>: HTTP status code is not handled or not allowed
2023-10-26 07:33:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92822267&maxPrice=92834473>: HTTP status code is not handled or not allowed
2023-10-26 07:33:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77050783&maxPrice=77099610>: HTTP status code is not handled or not allowed
2023-10-26 07:33:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94482423&maxPrice=94506836>: HTTP status code is not handled or not allowed
2023-10-26 07:33:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96582033&maxPrice=96606446>: HTTP status code is not handled or not allowed
2023-10-26 07:33:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94506837&maxPrice=94531250>: HTTP status code is not handled or not allowed
2023-10-26 07:33:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94409181&maxPrice=94433594>: HTTP status code is not handled or not allowed
2023-10-26 07:33:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96655275&maxPrice=96679688>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89843751&maxPrice=89892579>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96508791&maxPrice=96533204>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96630861&maxPrice=96655274>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96484376&maxPrice=96508790>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94384767&maxPrice=94409180>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89892580&maxPrice=89941407>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77246095&maxPrice=77343750>: HTTP status code is not handled or not allowed
2023-10-26 07:33:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77148439&maxPrice=77246094>: HTTP status code is not handled or not allowed
2023-10-26 07:33:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90185548&maxPrice=90234375>: HTTP status code is not handled or not allowed
2023-10-26 07:33:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67187501&maxPrice=67285157>: HTTP status code is not handled or not allowed
2023-10-26 07:33:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90136720&maxPrice=90185547>: HTTP status code is not handled or not allowed
2023-10-26 07:33:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67480470&maxPrice=67578125>: HTTP status code is not handled or not allowed
2023-10-26 07:33:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67382814&maxPrice=67480469>: HTTP status code is not handled or not allowed
2023-10-26 07:33:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76953126&maxPrice=77050782>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90234376&maxPrice=90283204>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90332033&maxPrice=90380860>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90380861&maxPrice=90429688>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93310548&maxPrice=93334961>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93164064&maxPrice=93212891>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68554689&maxPrice=68750000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67968751&maxPrice=68164063>: HTTP status code is not handled or not allowed
2023-10-26 07:34:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93212892&maxPrice=93261719>: HTTP status code is not handled or not allowed
2023-10-26 07:34:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67773439&maxPrice=67968750>: HTTP status code is not handled or not allowed
2023-10-26 07:34:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67578126&maxPrice=67773438>: HTTP status code is not handled or not allowed
2023-10-26 07:34:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91162111&maxPrice=91210938>: HTTP status code is not handled or not allowed
2023-10-26 07:34:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93261720&maxPrice=93310547>: HTTP status code is not handled or not allowed
2023-10-26 07:34:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90039064&maxPrice=90136719>: HTTP status code is not handled or not allowed
2023-10-26 07:34:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68359376&maxPrice=68554688>: HTTP status code is not handled or not allowed
2023-10-26 07:34:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91113283&maxPrice=91162110>: HTTP status code is not handled or not allowed
2023-10-26 07:34:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77734376&maxPrice=77929688>: HTTP status code is not handled or not allowed
2023-10-26 07:34:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77929689&maxPrice=78125000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77343751&maxPrice=77539063>: HTTP status code is not handled or not allowed
2023-10-26 07:34:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92919923&maxPrice=92968750>: HTTP status code is not handled or not allowed
2023-10-26 07:34:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92871095&maxPrice=92919922>: HTTP status code is not handled or not allowed
2023-10-26 07:34:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92382814&maxPrice=92480469>: HTTP status code is not handled or not allowed
2023-10-26 07:34:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77539064&maxPrice=77734375>: HTTP status code is not handled or not allowed
2023-10-26 07:34:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90527345&maxPrice=90625000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90429689&maxPrice=90527344>: HTTP status code is not handled or not allowed
2023-10-26 07:34:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92773439&maxPrice=92822266>: HTTP status code is not handled or not allowed
2023-10-26 07:34:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96679689&maxPrice=96777344>: HTTP status code is not handled or not allowed
2023-10-26 07:34:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96777345&maxPrice=96875000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91308595&maxPrice=91406250>: HTTP status code is not handled or not allowed
2023-10-26 07:34:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93066408&maxPrice=93164063>: HTTP status code is not handled or not allowed
2023-10-26 07:34:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92968751&maxPrice=93066407>: HTTP status code is not handled or not allowed
2023-10-26 07:34:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96289064&maxPrice=96386719>: HTTP status code is not handled or not allowed
2023-10-26 07:34:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91015626&maxPrice=91113282>: HTTP status code is not handled or not allowed
2023-10-26 07:34:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88378908&maxPrice=88476563>: HTTP status code is not handled or not allowed
2023-10-26 07:34:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88281251&maxPrice=88378907>: HTTP status code is not handled or not allowed
2023-10-26 07:34:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91210939&maxPrice=91308594>: HTTP status code is not handled or not allowed
2023-10-26 07:34:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96386720&maxPrice=96484375>: HTTP status code is not handled or not allowed
2023-10-26 07:34:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88476564&maxPrice=88574219>: HTTP status code is not handled or not allowed
2023-10-26 07:34:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76562501&maxPrice=76953125>: HTTP status code is not handled or not allowed
2023-10-26 07:34:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92187501&maxPrice=92382813>: HTTP status code is not handled or not allowed
2023-10-26 07:34:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96093751&maxPrice=96289063>: HTTP status code is not handled or not allowed
2023-10-26 07:34:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94140626&maxPrice=94335938>: HTTP status code is not handled or not allowed
2023-10-26 07:34:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88574220&maxPrice=88671875>: HTTP status code is not handled or not allowed
2023-10-26 07:34:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91601564&maxPrice=91796875>: HTTP status code is not handled or not allowed
2023-10-26 07:34:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91406251&maxPrice=91601563>: HTTP status code is not handled or not allowed
2023-10-26 07:34:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91992189&maxPrice=92187500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91796876&maxPrice=91992188>: HTTP status code is not handled or not allowed
2023-10-26 07:34:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92578126&maxPrice=92773438>: HTTP status code is not handled or not allowed
2023-10-26 07:34:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88671876&maxPrice=88867188>: HTTP status code is not handled or not allowed
2023-10-26 07:34:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88867189&maxPrice=89062500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93359376&maxPrice=93554688>: HTTP status code is not handled or not allowed
2023-10-26 07:34:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=71875001&maxPrice=73437500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93554689&maxPrice=93750000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=73437501&maxPrice=75000000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=66406251&maxPrice=67187500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98828126&maxPrice=99218750>: HTTP status code is not handled or not allowed
2023-10-26 07:34:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65625001&maxPrice=66406250>: HTTP status code is not handled or not allowed
2023-10-26 07:34:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90625001&maxPrice=91015625>: HTTP status code is not handled or not allowed
2023-10-26 07:34:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99609376&maxPrice=100000000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98437501&maxPrice=98828125>: HTTP status code is not handled or not allowed
2023-10-26 07:34:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64843751&maxPrice=65625000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99218751&maxPrice=99609375>: HTTP status code is not handled or not allowed
2023-10-26 07:34:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89062501&maxPrice=89453125>: HTTP status code is not handled or not allowed
2023-10-26 07:34:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89453126&maxPrice=89843750>: HTTP status code is not handled or not allowed
2023-10-26 07:34:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93750001&maxPrice=94140625>: HTTP status code is not handled or not allowed
2023-10-26 07:34:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87890626&maxPrice=88281250>: HTTP status code is not handled or not allowed
2023-10-26 07:34:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64062501&maxPrice=64843750>: HTTP status code is not handled or not allowed
2023-10-26 07:34:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87500001&maxPrice=87890625>: HTTP status code is not handled or not allowed
2023-10-26 07:34:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94921876&maxPrice=95312500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94531251&maxPrice=94921875>: HTTP status code is not handled or not allowed
2023-10-26 07:34:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75000001&maxPrice=76562500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79687501&maxPrice=81250000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78125001&maxPrice=79687500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68750001&maxPrice=71875000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82812501&maxPrice=84375000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=50000001&maxPrice=56250000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81250001&maxPrice=82812500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62500001&maxPrice=64062500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12&minPrice=0&maxPrice=50000000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=56250001&maxPrice=62500000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95312501&maxPrice=96093750>: HTTP status code is not handled or not allowed
2023-10-26 07:34:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=97656251&maxPrice=98437500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=84375001&maxPrice=85937500>: HTTP status code is not handled or not allowed
2023-10-26 07:34:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96875001&maxPrice=97656250>: HTTP status code is not handled or not allowed
2023-10-26 07:34:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85937501&maxPrice=87500000>: HTTP status code is not handled or not allowed
2023-10-26 07:34:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-26 07:34:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 626488,
 'downloader/request_count': 1032,
 'downloader/request_method_count/GET': 1032,
 'downloader/response_bytes': 58877531,
 'downloader/response_count': 1032,
 'downloader/response_status_count/200': 888,
 'downloader/response_status_count/403': 143,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 179.163196,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 26, 12, 34, 13, 241109),
 'httpcompression/response_bytes': 208861702,
 'httpcompression/response_count': 889,
 'httperror/response_ignored_count': 144,
 'httperror/response_ignored_status_count/403': 143,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 2180,
 'log_count/INFO': 156,
 'log_count/WARNING': 1,
 'request_depth_max': 54,
 'response_received_count': 1032,
 'scheduler/dequeued': 1032,
 'scheduler/dequeued/memory': 1032,
 'scheduler/enqueued': 1032,
 'scheduler/enqueued/memory': 1032,
 'start_time': datetime.datetime(2023, 10, 26, 12, 31, 14, 77913)}
2023-10-26 07:34:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-26 07:35:08 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-26 07:35:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-26 07:35:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-26 07:35:08 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-26 07:35:08 [scrapy.extensions.telnet] INFO: Telnet Password: 0f3b324b0e4f8fff
2023-10-26 07:35:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-26 07:35:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-26 07:35:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-26 07:35:08 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-26 07:35:08 [scrapy.core.engine] INFO: Spider opened
2023-10-26 07:35:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-26 07:35:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-26 07:35:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-26 07:36:08 [scrapy.extensions.logstats] INFO: Crawled 229 pages (at 229 pages/min), scraped 426 items (at 426 items/min)
2023-10-26 07:37:08 [scrapy.extensions.logstats] INFO: Crawled 373 pages (at 144 pages/min), scraped 723 items (at 297 items/min)
2023-10-26 07:38:08 [scrapy.extensions.logstats] INFO: Crawled 659 pages (at 286 pages/min), scraped 1829 items (at 1106 items/min)
2023-10-26 07:39:08 [scrapy.extensions.logstats] INFO: Crawled 855 pages (at 196 pages/min), scraped 2961 items (at 1132 items/min)
2023-10-26 07:40:08 [scrapy.extensions.logstats] INFO: Crawled 1022 pages (at 167 pages/min), scraped 3850 items (at 889 items/min)
2023-10-26 07:41:08 [scrapy.extensions.logstats] INFO: Crawled 1308 pages (at 286 pages/min), scraped 5501 items (at 1651 items/min)
2023-10-26 07:42:08 [scrapy.extensions.logstats] INFO: Crawled 1629 pages (at 321 pages/min), scraped 6822 items (at 1321 items/min)
2023-10-26 07:43:08 [scrapy.extensions.logstats] INFO: Crawled 1979 pages (at 350 pages/min), scraped 8713 items (at 1891 items/min)
2023-10-26 07:44:08 [scrapy.extensions.logstats] INFO: Crawled 2341 pages (at 362 pages/min), scraped 10701 items (at 1988 items/min)
2023-10-26 07:45:08 [scrapy.extensions.logstats] INFO: Crawled 2664 pages (at 323 pages/min), scraped 12441 items (at 1740 items/min)
2023-10-26 07:46:08 [scrapy.extensions.logstats] INFO: Crawled 2903 pages (at 239 pages/min), scraped 13837 items (at 1396 items/min)
2023-10-26 07:47:08 [scrapy.extensions.logstats] INFO: Crawled 3060 pages (at 157 pages/min), scraped 14743 items (at 906 items/min)
2023-10-26 07:48:08 [scrapy.extensions.logstats] INFO: Crawled 3241 pages (at 181 pages/min), scraped 15717 items (at 974 items/min)
2023-10-26 07:49:08 [scrapy.extensions.logstats] INFO: Crawled 3448 pages (at 207 pages/min), scraped 16951 items (at 1234 items/min)
2023-10-26 07:50:08 [scrapy.extensions.logstats] INFO: Crawled 3590 pages (at 142 pages/min), scraped 17783 items (at 832 items/min)
2023-10-26 07:51:08 [scrapy.extensions.logstats] INFO: Crawled 3733 pages (at 143 pages/min), scraped 18635 items (at 852 items/min)
2023-10-26 07:52:08 [scrapy.extensions.logstats] INFO: Crawled 3885 pages (at 152 pages/min), scraped 19553 items (at 918 items/min)
2023-10-26 07:53:08 [scrapy.extensions.logstats] INFO: Crawled 4095 pages (at 210 pages/min), scraped 20521 items (at 968 items/min)
2023-10-26 07:54:08 [scrapy.extensions.logstats] INFO: Crawled 4304 pages (at 209 pages/min), scraped 21304 items (at 783 items/min)
2023-10-26 07:55:08 [scrapy.extensions.logstats] INFO: Crawled 4484 pages (at 180 pages/min), scraped 22070 items (at 766 items/min)
2023-10-26 07:56:08 [scrapy.extensions.logstats] INFO: Crawled 4613 pages (at 129 pages/min), scraped 22840 items (at 770 items/min)
2023-10-26 07:57:08 [scrapy.extensions.logstats] INFO: Crawled 4781 pages (at 168 pages/min), scraped 23815 items (at 975 items/min)
2023-10-26 07:58:08 [scrapy.extensions.logstats] INFO: Crawled 4950 pages (at 169 pages/min), scraped 24720 items (at 905 items/min)
2023-10-26 07:59:08 [scrapy.extensions.logstats] INFO: Crawled 5133 pages (at 183 pages/min), scraped 25824 items (at 1104 items/min)
2023-10-26 08:00:08 [scrapy.extensions.logstats] INFO: Crawled 5324 pages (at 191 pages/min), scraped 26830 items (at 1006 items/min)
2023-10-26 08:01:08 [scrapy.extensions.logstats] INFO: Crawled 5495 pages (at 171 pages/min), scraped 27445 items (at 615 items/min)
2023-10-26 08:02:08 [scrapy.extensions.logstats] INFO: Crawled 5703 pages (at 208 pages/min), scraped 28088 items (at 643 items/min)
2023-10-26 08:03:08 [scrapy.extensions.logstats] INFO: Crawled 5885 pages (at 182 pages/min), scraped 28652 items (at 564 items/min)
2023-10-26 08:04:08 [scrapy.extensions.logstats] INFO: Crawled 6070 pages (at 185 pages/min), scraped 29112 items (at 460 items/min)
2023-10-26 08:05:08 [scrapy.extensions.logstats] INFO: Crawled 6276 pages (at 206 pages/min), scraped 29629 items (at 517 items/min)
2023-10-26 08:06:38 [scrapy.extensions.logstats] INFO: Crawled 6368 pages (at 92 pages/min), scraped 29721 items (at 92 items/min)
2023-10-26 08:06:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87243654&maxPrice=87246705. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-26 08:07:08 [scrapy.extensions.logstats] INFO: Crawled 6492 pages (at 124 pages/min), scraped 30076 items (at 355 items/min)
2023-10-26 08:08:08 [scrapy.extensions.logstats] INFO: Crawled 6833 pages (at 341 pages/min), scraped 31069 items (at 993 items/min)
2023-10-28 05:45:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:45:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:45:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:45:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:45:58 [scrapy.extensions.telnet] INFO: Telnet Password: c4c4cbabb34ba4f5
2023-10-28 05:45:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:45:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:45:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:45:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:45:58 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:45:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:45:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:48:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:48:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:48:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:48:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:48:10 [scrapy.extensions.telnet] INFO: Telnet Password: e7c8cbbd8208627f
2023-10-28 05:48:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:48:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:48:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:48:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:48:11 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:48:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:48:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 05:49:11 [scrapy.extensions.logstats] INFO: Crawled 472 pages (at 472 pages/min), scraped 1172 items (at 1172 items/min)
2023-10-28 05:50:11 [scrapy.extensions.logstats] INFO: Crawled 1164 pages (at 692 pages/min), scraped 2099 items (at 927 items/min)
2023-10-28 05:50:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80725099&maxPrice=80737305>: HTTP status code is not handled or not allowed
2023-10-28 05:50:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80712892&maxPrice=80725098>: HTTP status code is not handled or not allowed
2023-10-28 05:50:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99801638&maxPrice=99804688>: HTTP status code is not handled or not allowed
2023-10-28 05:50:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99798586&maxPrice=99801637>: HTTP status code is not handled or not allowed
2023-10-28 05:50:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81201173&maxPrice=81213379>: HTTP status code is not handled or not allowed
2023-10-28 05:50:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80737306&maxPrice=80749512>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99813845&maxPrice=99816895>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99810793&maxPrice=99813844>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81213380&maxPrice=81225586>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81237794&maxPrice=81250000>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81225587&maxPrice=81237793>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80749513&maxPrice=80761719>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80551150&maxPrice=80554200>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80548098&maxPrice=80551149>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80648806&maxPrice=80651856>: HTTP status code is not handled or not allowed
2023-10-28 05:50:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80645754&maxPrice=80648805>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80514529&maxPrice=80517579>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80511477&maxPrice=80514528>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80541994&maxPrice=80548097>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80560305&maxPrice=80566407>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80554201&maxPrice=80560304>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80508425&maxPrice=80511476>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80505373&maxPrice=80508424>: HTTP status code is not handled or not allowed
2023-10-28 05:50:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80639650&maxPrice=80645753>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80657961&maxPrice=80664063>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80651857&maxPrice=80657960>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99911501&maxPrice=99914551>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99908449&maxPrice=99911500>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99868776&maxPrice=99871827>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99865724&maxPrice=99868775>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99874880&maxPrice=99877930>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99871828&maxPrice=99874879>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81176759&maxPrice=81201172>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81152345&maxPrice=81176758>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99792482&maxPrice=99798585>: HTTP status code is not handled or not allowed
2023-10-28 05:50:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99780275&maxPrice=99786378>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80480959&maxPrice=80493165>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80468751&maxPrice=80480958>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80493166&maxPrice=80505372>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99804689&maxPrice=99810792>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99816896&maxPrice=99822999>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99743654&maxPrice=99749757>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99725344&maxPrice=99731446>: HTTP status code is not handled or not allowed
2023-10-28 05:50:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99719240&maxPrice=99725343>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99945070&maxPrice=99951172>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99938966&maxPrice=99945069>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99920656&maxPrice=99926758>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99914552&maxPrice=99920655>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99853517&maxPrice=99859620>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80664064&maxPrice=80712891>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80810548&maxPrice=80859375>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80761720&maxPrice=80810547>: HTTP status code is not handled or not allowed
2023-10-28 05:50:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99768068&maxPrice=99780274>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99755861&maxPrice=99768067>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80517580&maxPrice=80541993>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99841310&maxPrice=99853516>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99829103&maxPrice=99841309>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80566408&maxPrice=80590821>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80590822&maxPrice=80615235>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80615236&maxPrice=80639649>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99731447&maxPrice=99743653>: HTTP status code is not handled or not allowed
2023-10-28 05:50:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99707033&maxPrice=99719239>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99926759&maxPrice=99938965>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99890138&maxPrice=99902344>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81054689&maxPrice=81152344>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85058595&maxPrice=85156250>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99877931&maxPrice=99890137>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=84960939&maxPrice=85058594>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94238283&maxPrice=94335938>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94140626&maxPrice=94238282>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99511720&maxPrice=99609375>: HTTP status code is not handled or not allowed
2023-10-28 05:51:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99414064&maxPrice=99511719>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94433595&maxPrice=94531250>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94335939&maxPrice=94433594>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=57812501&maxPrice=59375000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=56250001&maxPrice=57812500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=84570314&maxPrice=84765625>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=84375001&maxPrice=84570313>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=59375001&maxPrice=62500000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80078126&maxPrice=80273438>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80859376&maxPrice=81054688>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=84765626&maxPrice=84960938>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95507814&maxPrice=95703125>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95312501&maxPrice=95507813>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95898439&maxPrice=96093750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95703126&maxPrice=95898438>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99023439&maxPrice=99218750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98828126&maxPrice=99023438>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96679689&maxPrice=96875000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96484376&maxPrice=96679688>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93164064&maxPrice=93359375>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92968751&maxPrice=93164063>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17&minPrice=0&maxPrice=50000000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93554689&maxPrice=93750000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93359376&maxPrice=93554688>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67968751&maxPrice=68750000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67187501&maxPrice=67968750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63281251&maxPrice=64062500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62500001&maxPrice=63281250>: HTTP status code is not handled or not allowed
2023-10-28 05:51:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64843751&maxPrice=65625000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64062501&maxPrice=64843750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85156251&maxPrice=85546875>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82421876&maxPrice=82812500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82031251&maxPrice=82421875>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96093751&maxPrice=96484375>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98437501&maxPrice=98828125>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93750001&maxPrice=94140625>: HTTP status code is not handled or not allowed
2023-10-28 05:51:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94921876&maxPrice=95312500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94531251&maxPrice=94921875>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91796876&maxPrice=92187500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90625001&maxPrice=91015625>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91406251&maxPrice=91796875>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65625001&maxPrice=67187500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=50000001&maxPrice=56250000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85937501&maxPrice=86718750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=86718751&maxPrice=87500000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81250001&maxPrice=82031250>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=71875001&maxPrice=75000000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68750001&maxPrice=71875000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=83593751&maxPrice=84375000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82812501&maxPrice=83593750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=97656251&maxPrice=98437500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96875001&maxPrice=97656250>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88281251&maxPrice=89062500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87500001&maxPrice=88281250>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89843751&maxPrice=90625000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89062501&maxPrice=89843750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92187501&maxPrice=92968750>: HTTP status code is not handled or not allowed
2023-10-28 05:51:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78125001&maxPrice=79687500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76562501&maxPrice=78125000>: HTTP status code is not handled or not allowed
2023-10-28 05:51:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75000001&maxPrice=76562500>: HTTP status code is not handled or not allowed
2023-10-28 05:51:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 05:51:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1123439,
 'downloader/request_count': 1824,
 'downloader/request_method_count/GET': 1824,
 'downloader/response_bytes': 107175454,
 'downloader/response_count': 1824,
 'downloader/response_status_count/200': 1692,
 'downloader/response_status_count/403': 131,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 176.323863,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 10, 51, 7, 326701),
 'httpcompression/response_bytes': 369423150,
 'httpcompression/response_count': 1693,
 'httperror/response_ignored_count': 132,
 'httperror/response_ignored_status_count/403': 131,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 2769,
 'log_count/INFO': 144,
 'log_count/WARNING': 1,
 'request_depth_max': 53,
 'response_received_count': 1824,
 'scheduler/dequeued': 1824,
 'scheduler/dequeued/memory': 1824,
 'scheduler/enqueued': 1824,
 'scheduler/enqueued/memory': 1824,
 'start_time': datetime.datetime(2023, 10, 28, 10, 48, 11, 2838)}
2023-10-28 05:51:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 05:55:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:55:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:55:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:55:59 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:55:59 [scrapy.extensions.telnet] INFO: Telnet Password: 2cb91091e66b3833
2023-10-28 05:55:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:55:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:55:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:55:59 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:55:59 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:55:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:55:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:56:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 05:56:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 05:56:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1756,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.541252,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 10, 56, 1, 334015),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 28, 10, 55, 59, 792763)}
2023-10-28 05:56:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 05:56:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:56:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:56:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:56:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:56:13 [scrapy.extensions.telnet] INFO: Telnet Password: f7f944830fa0e150
2023-10-28 05:56:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:56:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:56:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:56:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:56:13 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:56:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:56:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:56:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 05:56:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 05:56:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1755,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.956949,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 10, 56, 14, 814403),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 28, 10, 56, 13, 857454)}
2023-10-28 05:56:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 05:56:17 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:56:17 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:56:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:56:17 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:56:17 [scrapy.extensions.telnet] INFO: Telnet Password: bd1bac5a2d0a6716
2023-10-28 05:56:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:56:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:56:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:56:17 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:56:17 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:56:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:56:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:56:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 05:56:18 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 05:56:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1756,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.96878,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 10, 56, 18, 682595),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 28, 10, 56, 17, 713815)}
2023-10-28 05:56:18 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 05:56:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:56:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:56:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:56:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:56:35 [scrapy.extensions.telnet] INFO: Telnet Password: 2853973711cb26f7
2023-10-28 05:56:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:56:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:56:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:56:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:56:35 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:56:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:56:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:56:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 05:56:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 05:56:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1755,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.362832,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 10, 56, 36, 844534),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 28, 10, 56, 35, 481702)}
2023-10-28 05:56:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 05:58:21 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:58:21 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:58:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:58:21 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:58:21 [scrapy.extensions.telnet] INFO: Telnet Password: c64bf75a63897ada
2023-10-28 05:58:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:58:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:58:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:58:21 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:58:21 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:58:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:58:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:59:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:59:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:59:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:59:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:59:22 [scrapy.extensions.telnet] INFO: Telnet Password: 57aa8933dcd50b35
2023-10-28 05:59:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:59:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:59:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:59:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:59:22 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:59:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:59:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 05:59:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 05:59:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 05:59:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 05:59:57 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 05:59:57 [scrapy.extensions.telnet] INFO: Telnet Password: 2a7d92fb97e8fcd8
2023-10-28 05:59:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 05:59:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 05:59:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 05:59:57 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 05:59:57 [scrapy.core.engine] INFO: Spider opened
2023-10-28 05:59:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 05:59:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 06:00:38 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 06:00:38 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 06:00:38 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 06:00:38 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 06:00:38 [scrapy.extensions.telnet] INFO: Telnet Password: 943b39554bd7320c
2023-10-28 06:00:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 06:00:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 06:00:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 06:00:38 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 06:00:38 [scrapy.core.engine] INFO: Spider opened
2023-10-28 06:00:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 06:00:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 06:01:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 06:01:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 176429,
 'downloader/request_count': 258,
 'downloader/request_method_count/GET': 258,
 'downloader/response_bytes': 16024168,
 'downloader/response_count': 258,
 'downloader/response_status_count/200': 258,
 'elapsed_time_seconds': 34.630379,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 11, 1, 12, 798806),
 'httpcompression/response_bytes': 55314189,
 'httpcompression/response_count': 258,
 'item_scraped_count': 321,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 7,
 'response_received_count': 258,
 'scheduler/dequeued': 258,
 'scheduler/dequeued/memory': 258,
 'scheduler/enqueued': 258,
 'scheduler/enqueued/memory': 258,
 'start_time': datetime.datetime(2023, 10, 28, 11, 0, 38, 168427)}
2023-10-28 06:01:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 06:01:27 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 06:01:27 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 06:01:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 06:01:27 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 06:01:27 [scrapy.extensions.telnet] INFO: Telnet Password: e13ddebcadb7b5ea
2023-10-28 06:01:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 06:01:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 06:01:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 06:01:27 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 06:01:27 [scrapy.core.engine] INFO: Spider opened
2023-10-28 06:01:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 06:01:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 06:01:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 06:01:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 06:01:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 06:01:59 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 06:01:59 [scrapy.extensions.telnet] INFO: Telnet Password: d7f291769216eb99
2023-10-28 06:01:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 06:01:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 06:01:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 06:01:59 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 06:01:59 [scrapy.core.engine] INFO: Spider opened
2023-10-28 06:01:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 06:01:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 06:02:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 06:02:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 06:02:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 06:02:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 06:02:14 [scrapy.extensions.telnet] INFO: Telnet Password: 626ab852d1961e95
2023-10-28 06:02:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 06:02:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 06:02:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 06:02:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 06:02:14 [scrapy.core.engine] INFO: Spider opened
2023-10-28 06:02:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 06:02:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 07:09:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 07:09:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 07:09:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 07:09:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 07:09:58 [scrapy.extensions.telnet] INFO: Telnet Password: b65d386f99170c7d
2023-10-28 07:09:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 07:09:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 07:09:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 07:09:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 07:09:58 [scrapy.core.engine] INFO: Spider opened
2023-10-28 07:09:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 07:09:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 07:10:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 07:10:00 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 07:10:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 07:10:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 861,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 2.709003,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 12, 10, 0, 925231),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 28, 12, 9, 58, 216228)}
2023-10-28 07:10:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 07:10:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 07:10:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 07:10:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 07:10:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 07:10:06 [scrapy.extensions.telnet] INFO: Telnet Password: b31eb7dd1ce612b5
2023-10-28 07:10:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 07:10:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 07:10:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 07:10:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 07:10:06 [scrapy.core.engine] INFO: Spider opened
2023-10-28 07:10:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 07:10:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 07:10:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 07:10:08 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 07:10:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 07:10:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 861,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 2.274322,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 12, 10, 8, 358611),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 28, 12, 10, 6, 84289)}
2023-10-28 07:10:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 07:10:11 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 07:10:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 07:10:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 07:10:11 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 07:10:11 [scrapy.extensions.telnet] INFO: Telnet Password: ca0d6ba0242e4217
2023-10-28 07:10:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 07:10:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 07:10:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 07:10:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 07:10:11 [scrapy.core.engine] INFO: Spider opened
2023-10-28 07:10:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 07:10:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 07:10:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 07:10:13 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 07:10:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 07:10:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 861,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 2.210865,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 12, 10, 14, 18725),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 28, 12, 10, 11, 807860)}
2023-10-28 07:10:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 07:12:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 07:12:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 07:12:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 07:12:18 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 07:12:18 [scrapy.extensions.telnet] INFO: Telnet Password: c2cc76de00f123ef
2023-10-28 07:12:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 07:12:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 07:12:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 07:12:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 07:12:18 [scrapy.core.engine] INFO: Spider opened
2023-10-28 07:12:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 07:12:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 07:12:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-whole-36-aurora-avenue-queanbeyan-nsw-2620-504446336> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-shops-7-8-53-mosaic-drive-lalor-vic-3075-504425272> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-31-clapham-road-regents-park-nsw-2143-504430460> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-595-smollett-street-albury-nsw-2640-504441056> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-monte-ore-house-49-rathdowne-street-carlton-vic-3053-504435436> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-molendinar-qld-4214-504393080> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-swanbank-qld-4306-504354932> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-4-17-orange-street-williamstown-north-vic-3016-504408808> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-12-murray-street-tamworth-nsw-2340-504420052> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 07:12:31 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1fe947d5090>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 165, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 07:12:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 6285,
 'downloader/request_count': 13,
 'downloader/request_method_count/GET': 13,
 'downloader/response_bytes': 897847,
 'downloader/response_count': 13,
 'downloader/response_status_count/200': 13,
 'elapsed_time_seconds': 12.649826,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 28, 12, 12, 31, 24860),
 'httpcompression/response_bytes': 3258852,
 'httpcompression/response_count': 13,
 'item_scraped_count': 12,
 'log_count/ERROR': 11,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 13,
 'scheduler/dequeued': 13,
 'scheduler/dequeued/memory': 13,
 'scheduler/enqueued': 13,
 'scheduler/enqueued/memory': 13,
 'spider_exceptions/BrokenPipeError': 10,
 'start_time': datetime.datetime(2023, 10, 28, 12, 12, 18, 375034)}
2023-10-28 07:12:31 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 07:12:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 07:12:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 07:12:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 07:12:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 07:12:45 [scrapy.extensions.telnet] INFO: Telnet Password: 2707935a7965dcd7
2023-10-28 07:12:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 07:12:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 07:12:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 07:12:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 07:12:45 [scrapy.core.engine] INFO: Spider opened
2023-10-28 07:12:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 07:12:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 07:13:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 07:13:45 [scrapy.extensions.logstats] INFO: Crawled 172 pages (at 172 pages/min), scraped 314 items (at 314 items/min)
2023-10-28 07:14:45 [scrapy.extensions.logstats] INFO: Crawled 372 pages (at 200 pages/min), scraped 713 items (at 399 items/min)
2023-10-28 07:15:45 [scrapy.extensions.logstats] INFO: Crawled 602 pages (at 230 pages/min), scraped 1480 items (at 767 items/min)
2023-10-28 07:16:45 [scrapy.extensions.logstats] INFO: Crawled 851 pages (at 249 pages/min), scraped 2935 items (at 1455 items/min)
2023-10-28 07:17:45 [scrapy.extensions.logstats] INFO: Crawled 1101 pages (at 250 pages/min), scraped 4417 items (at 1482 items/min)
2023-10-28 07:18:45 [scrapy.extensions.logstats] INFO: Crawled 1357 pages (at 256 pages/min), scraped 5785 items (at 1368 items/min)
2023-10-28 07:19:45 [scrapy.extensions.logstats] INFO: Crawled 1607 pages (at 250 pages/min), scraped 7172 items (at 1387 items/min)
2023-10-28 07:20:45 [scrapy.extensions.logstats] INFO: Crawled 1826 pages (at 219 pages/min), scraped 8102 items (at 930 items/min)
2023-10-28 07:21:45 [scrapy.extensions.logstats] INFO: Crawled 2074 pages (at 248 pages/min), scraped 9102 items (at 1000 items/min)
2023-10-28 07:22:45 [scrapy.extensions.logstats] INFO: Crawled 2292 pages (at 218 pages/min), scraped 10085 items (at 983 items/min)
2023-10-28 07:23:45 [scrapy.extensions.logstats] INFO: Crawled 2512 pages (at 220 pages/min), scraped 11232 items (at 1147 items/min)
2023-10-28 07:24:45 [scrapy.extensions.logstats] INFO: Crawled 2722 pages (at 210 pages/min), scraped 12371 items (at 1139 items/min)
2023-10-28 07:25:45 [scrapy.extensions.logstats] INFO: Crawled 2969 pages (at 247 pages/min), scraped 13667 items (at 1296 items/min)
2023-10-28 07:26:45 [scrapy.extensions.logstats] INFO: Crawled 3219 pages (at 250 pages/min), scraped 14927 items (at 1260 items/min)
2023-10-28 07:27:45 [scrapy.extensions.logstats] INFO: Crawled 3479 pages (at 260 pages/min), scraped 15864 items (at 937 items/min)
2023-10-28 07:28:45 [scrapy.extensions.logstats] INFO: Crawled 3741 pages (at 262 pages/min), scraped 17014 items (at 1150 items/min)
2023-10-28 07:29:45 [scrapy.extensions.logstats] INFO: Crawled 4015 pages (at 274 pages/min), scraped 18120 items (at 1106 items/min)
2023-10-28 07:30:45 [scrapy.extensions.logstats] INFO: Crawled 4257 pages (at 242 pages/min), scraped 19106 items (at 986 items/min)
2023-10-28 07:31:45 [scrapy.extensions.logstats] INFO: Crawled 4491 pages (at 234 pages/min), scraped 20404 items (at 1298 items/min)
2023-10-28 07:32:45 [scrapy.extensions.logstats] INFO: Crawled 4747 pages (at 256 pages/min), scraped 21546 items (at 1142 items/min)
2023-10-28 07:33:45 [scrapy.extensions.logstats] INFO: Crawled 5000 pages (at 253 pages/min), scraped 22447 items (at 901 items/min)
2023-10-28 07:34:45 [scrapy.extensions.logstats] INFO: Crawled 5246 pages (at 246 pages/min), scraped 23225 items (at 778 items/min)
2023-10-28 07:35:45 [scrapy.extensions.logstats] INFO: Crawled 5459 pages (at 213 pages/min), scraped 23629 items (at 404 items/min)
2023-10-28 07:36:45 [scrapy.extensions.logstats] INFO: Crawled 5667 pages (at 208 pages/min), scraped 24036 items (at 407 items/min)
2023-10-28 07:37:45 [scrapy.extensions.logstats] INFO: Crawled 5895 pages (at 228 pages/min), scraped 24363 items (at 327 items/min)
2023-10-28 07:38:45 [scrapy.extensions.logstats] INFO: Crawled 6144 pages (at 249 pages/min), scraped 24790 items (at 427 items/min)
2023-10-28 07:39:45 [scrapy.extensions.logstats] INFO: Crawled 6391 pages (at 247 pages/min), scraped 25267 items (at 477 items/min)
2023-10-28 07:40:45 [scrapy.extensions.logstats] INFO: Crawled 6636 pages (at 245 pages/min), scraped 25785 items (at 518 items/min)
2023-10-28 07:41:45 [scrapy.extensions.logstats] INFO: Crawled 6889 pages (at 253 pages/min), scraped 26388 items (at 603 items/min)
2023-10-28 07:42:45 [scrapy.extensions.logstats] INFO: Crawled 7146 pages (at 257 pages/min), scraped 26990 items (at 602 items/min)
2023-10-28 07:43:45 [scrapy.extensions.logstats] INFO: Crawled 7392 pages (at 246 pages/min), scraped 28254 items (at 1264 items/min)
2023-10-28 07:44:45 [scrapy.extensions.logstats] INFO: Crawled 7619 pages (at 227 pages/min), scraped 29926 items (at 1672 items/min)
2023-10-28 07:45:45 [scrapy.extensions.logstats] INFO: Crawled 7868 pages (at 249 pages/min), scraped 31642 items (at 1716 items/min)
2023-10-28 07:46:45 [scrapy.extensions.logstats] INFO: Crawled 8113 pages (at 245 pages/min), scraped 33648 items (at 2006 items/min)
2023-10-28 07:47:45 [scrapy.extensions.logstats] INFO: Crawled 8365 pages (at 252 pages/min), scraped 35601 items (at 1953 items/min)
2023-10-28 07:48:45 [scrapy.extensions.logstats] INFO: Crawled 8605 pages (at 240 pages/min), scraped 37560 items (at 1959 items/min)
2023-10-28 07:49:45 [scrapy.extensions.logstats] INFO: Crawled 8854 pages (at 249 pages/min), scraped 39597 items (at 2037 items/min)
2023-10-28 07:50:45 [scrapy.extensions.logstats] INFO: Crawled 9098 pages (at 244 pages/min), scraped 41652 items (at 2055 items/min)
2023-10-28 07:51:45 [scrapy.extensions.logstats] INFO: Crawled 9319 pages (at 221 pages/min), scraped 43280 items (at 1628 items/min)
2023-10-28 07:52:45 [scrapy.extensions.logstats] INFO: Crawled 9572 pages (at 253 pages/min), scraped 45428 items (at 2148 items/min)
2023-10-28 07:53:45 [scrapy.extensions.logstats] INFO: Crawled 9816 pages (at 244 pages/min), scraped 47366 items (at 1938 items/min)
2023-10-28 07:54:45 [scrapy.extensions.logstats] INFO: Crawled 10068 pages (at 252 pages/min), scraped 49206 items (at 1840 items/min)
2023-10-28 07:55:45 [scrapy.extensions.logstats] INFO: Crawled 10331 pages (at 263 pages/min), scraped 51290 items (at 2084 items/min)
2023-10-28 19:47:17 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 19:47:17 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 19:47:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 19:47:17 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 19:47:17 [scrapy.extensions.telnet] INFO: Telnet Password: 55f67a5c380b8fc3
2023-10-28 19:47:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 19:47:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 19:47:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 19:47:17 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 19:47:17 [scrapy.core.engine] INFO: Spider opened
2023-10-28 19:47:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 19:47:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 19:47:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 19:48:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75878908&maxPrice=75976563>: HTTP status code is not handled or not allowed
2023-10-28 19:48:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75781251&maxPrice=75878907>: HTTP status code is not handled or not allowed
2023-10-28 19:48:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=52343751&maxPrice=53125000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=51562501&maxPrice=52343750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=54687501&maxPrice=56250000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63867189&maxPrice=63964844>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65332033&maxPrice=65429688>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65234376&maxPrice=65332032>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=53125001&maxPrice=54687500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63964845&maxPrice=64062500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96386720&maxPrice=96484375>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63574220&maxPrice=63671875>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96289064&maxPrice=96386719>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63476564&maxPrice=63574219>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77685548&maxPrice=77734375>: HTTP status code is not handled or not allowed
2023-10-28 19:48:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77392580&maxPrice=77441407>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77343751&maxPrice=77392579>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77490236&maxPrice=77539063>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77441408&maxPrice=77490235>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82763673&maxPrice=82812500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82714845&maxPrice=82763672>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82617189&maxPrice=82666016>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82666017&maxPrice=82714844>: HTTP status code is not handled or not allowed
2023-10-28 19:48:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78076173&maxPrice=78125000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78027345&maxPrice=78076172>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77636720&maxPrice=77685547>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.extensions.logstats] INFO: Crawled 549 pages (at 549 pages/min), scraped 1345 items (at 1345 items/min)
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65527345&maxPrice=65625000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65429689&maxPrice=65527344>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=50781251&maxPrice=51562500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=50000001&maxPrice=50781250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75097658&maxPrice=75195313>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75000001&maxPrice=75097657>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75292970&maxPrice=75390625>: HTTP status code is not handled or not allowed
2023-10-28 19:48:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75195314&maxPrice=75292969>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75976564&maxPrice=76074219>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76074220&maxPrice=76171875>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=57031251&maxPrice=57812500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=56250001&maxPrice=57031250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=58593751&maxPrice=59375000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=57812501&maxPrice=58593750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77929689&maxPrice=78027344>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77539064&maxPrice=77636719>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96093751&maxPrice=96289063>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96484376&maxPrice=96679688>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63281251&maxPrice=63476563>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96679689&maxPrice=96875000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63085939&maxPrice=63281250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62890626&maxPrice=63085938>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63671876&maxPrice=63867188>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75585939&maxPrice=75781250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75390626&maxPrice=75585938>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82226564&maxPrice=82421875>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82031251&maxPrice=82226563>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76367189&maxPrice=76562500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76171876&maxPrice=76367188>: HTTP status code is not handled or not allowed
2023-10-28 19:48:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82421876&maxPrice=82617188>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=59375001&maxPrice=62500000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77734376&maxPrice=77929688>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64453126&maxPrice=64843750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64843751&maxPrice=65234375>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95703126&maxPrice=96093750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64062501&maxPrice=64453125>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95312501&maxPrice=95703125>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62500001&maxPrice=62890625>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87890626&maxPrice=88281250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87500001&maxPrice=87890625>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88281251&maxPrice=88671875>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88671876&maxPrice=89062500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91015626&maxPrice=91406250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90625001&maxPrice=91015625>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92187501&maxPrice=92578125>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92578126&maxPrice=92968750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91796876&maxPrice=92187500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=91406251&maxPrice=91796875>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93359376&maxPrice=93750000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92968751&maxPrice=93359375>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85546876&maxPrice=85937500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85156251&maxPrice=85546875>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=86328126&maxPrice=86718750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85937501&maxPrice=86328125>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=86718751&maxPrice=87109375>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87109376&maxPrice=87500000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67187501&maxPrice=67968750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67968751&maxPrice=68750000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89843751&maxPrice=90625000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89062501&maxPrice=89843750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99218751&maxPrice=100000000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98437501&maxPrice=99218750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93750001&maxPrice=94531250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94531251&maxPrice=95312500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80468751&maxPrice=81250000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79687501&maxPrice=80468750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78906251&maxPrice=79687500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78125001&maxPrice=78906250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76562501&maxPrice=77343750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=84375001&maxPrice=85156250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81250001&maxPrice=82031250>: HTTP status code is not handled or not allowed
2023-10-28 19:48:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65625001&maxPrice=67187500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=71875001&maxPrice=75000000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68750001&maxPrice=71875000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=83593751&maxPrice=84375000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82812501&maxPrice=83593750>: HTTP status code is not handled or not allowed
2023-10-28 19:48:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96875001&maxPrice=98437500>: HTTP status code is not handled or not allowed
2023-10-28 19:48:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14&minPrice=0&maxPrice=50000000>: HTTP status code is not handled or not allowed
2023-10-28 19:48:24 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 19:48:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 370950,
 'downloader/request_count': 627,
 'downloader/request_method_count/GET': 627,
 'downloader/response_bytes': 35938808,
 'downloader/response_count': 627,
 'downloader/response_status_count/200': 522,
 'downloader/response_status_count/403': 104,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 67.073328,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 0, 48, 24, 214259),
 'httpcompression/response_bytes': 129632982,
 'httpcompression/response_count': 523,
 'httperror/response_ignored_count': 105,
 'httperror/response_ignored_status_count/403': 104,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 1345,
 'log_count/INFO': 116,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 627,
 'scheduler/dequeued': 627,
 'scheduler/dequeued/memory': 627,
 'scheduler/enqueued': 627,
 'scheduler/enqueued/memory': 627,
 'start_time': datetime.datetime(2023, 10, 29, 0, 47, 17, 140931)}
2023-10-28 19:48:24 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 19:48:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 19:48:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 19:48:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 19:48:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 19:48:47 [scrapy.extensions.telnet] INFO: Telnet Password: 0f9503fbdf5238ae
2023-10-28 19:48:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 19:48:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 19:48:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 19:48:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 19:48:47 [scrapy.core.engine] INFO: Spider opened
2023-10-28 19:48:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 19:48:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 19:48:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 19:48:48 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 19:48:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1655,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.184347,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 0, 48, 48, 301356),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 0, 48, 47, 117009)}
2023-10-28 19:48:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 19:48:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 19:48:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 19:48:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 19:48:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 19:48:58 [scrapy.extensions.telnet] INFO: Telnet Password: 6ae81a50554069e7
2023-10-28 19:48:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 19:48:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 19:48:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 19:48:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 19:48:58 [scrapy.core.engine] INFO: Spider opened
2023-10-28 19:48:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 19:48:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 19:48:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 19:48:59 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 19:48:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1655,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.011804,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 0, 48, 59, 672409),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 0, 48, 58, 660605)}
2023-10-28 19:48:59 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 19:49:05 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 19:49:05 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 19:49:05 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 19:49:05 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 19:49:05 [scrapy.extensions.telnet] INFO: Telnet Password: 51d9c31effb39e07
2023-10-28 19:49:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 19:49:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 19:49:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 19:49:05 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 19:49:05 [scrapy.core.engine] INFO: Spider opened
2023-10-28 19:49:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 19:49:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 19:49:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 19:49:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 19:49:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1655,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.863591,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 0, 49, 6, 854961),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 0, 49, 5, 991370)}
2023-10-28 19:49:06 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 19:49:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 19:49:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 19:49:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 19:49:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 19:49:47 [scrapy.extensions.telnet] INFO: Telnet Password: 33ba8d2afbe98e22
2023-10-28 19:49:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 19:49:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 19:49:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 19:49:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 19:49:47 [scrapy.core.engine] INFO: Spider opened
2023-10-28 19:49:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 19:49:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 19:49:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:01:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:01:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:01:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:01:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:01:48 [scrapy.extensions.telnet] INFO: Telnet Password: 2fa5c3ee37351bc2
2023-10-28 20:01:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:01:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:01:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:01:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:01:48 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:01:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:01:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:01:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:02:48 [scrapy.extensions.logstats] INFO: Crawled 523 pages (at 523 pages/min), scraped 1396 items (at 1396 items/min)
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99975587&maxPrice=99987793>: HTTP status code is not handled or not allowed
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89978029&maxPrice=89990235>: HTTP status code is not handled or not allowed
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89868166&maxPrice=89880372>: HTTP status code is not handled or not allowed
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79669191&maxPrice=79675293>: HTTP status code is not handled or not allowed
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78118898&maxPrice=78125000>: HTTP status code is not handled or not allowed
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78112794&maxPrice=78118897>: HTTP status code is not handled or not allowed
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99389650&maxPrice=99395753>: HTTP status code is not handled or not allowed
2023-10-28 20:02:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90032961&maxPrice=90039063>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99395754&maxPrice=99401856>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99377443&maxPrice=99383546>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99383547&maxPrice=99389649>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99407961&maxPrice=99414063>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79522707&maxPrice=79528809>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79516603&maxPrice=79522706>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89910891&maxPrice=89916993>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90008547&maxPrice=90014649>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90002443&maxPrice=90008546>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89996340&maxPrice=90002442>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89886477&maxPrice=89892579>: HTTP status code is not handled or not allowed
2023-10-28 20:02:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89880373&maxPrice=89886476>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89990236&maxPrice=89996339>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89904787&maxPrice=89910890>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90020754&maxPrice=90026856>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99993898&maxPrice=100000000>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90014650&maxPrice=90020753>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99987794&maxPrice=99993897>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79534914&maxPrice=79541016>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79528810&maxPrice=79534913>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90026857&maxPrice=90032960>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99401857&maxPrice=99407960>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79559328&maxPrice=79565430>: HTTP status code is not handled or not allowed
2023-10-28 20:02:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79553224&maxPrice=79559327>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78106691&maxPrice=78112793>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79663087&maxPrice=79669190>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78100587&maxPrice=78106690>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79681398&maxPrice=79687500>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89965822&maxPrice=89978028>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79675294&maxPrice=79681397>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62011720&maxPrice=62109375>: HTTP status code is not handled or not allowed
2023-10-28 20:02:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=61914064&maxPrice=62011719>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89892580&maxPrice=89904786>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62304689&maxPrice=62402344>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62402345&maxPrice=62500000>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89929201&maxPrice=89941407>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90209962&maxPrice=90234375>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99365236&maxPrice=99377442>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89916994&maxPrice=89929200>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90185548&maxPrice=90209961>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79650880&maxPrice=79663086>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79541017&maxPrice=79553223>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79638673&maxPrice=79650879>: HTTP status code is not handled or not allowed
2023-10-28 20:02:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79577638&maxPrice=79589844>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79565431&maxPrice=79577637>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99951173&maxPrice=99975586>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=60546876&maxPrice=60937500>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=60156251&maxPrice=60546875>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89843751&maxPrice=89868165>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89941408&maxPrice=89965821>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=61718751&maxPrice=61914063>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62109376&maxPrice=62304688>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90307619&maxPrice=90332032>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90283205&maxPrice=90307618>: HTTP status code is not handled or not allowed
2023-10-28 20:02:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78051759&maxPrice=78076172>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78027345&maxPrice=78051758>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79614259&maxPrice=79638672>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79492189&maxPrice=79516602>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79589845&maxPrice=79614258>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99340822&maxPrice=99365235>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99316408&maxPrice=99340821>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90136720&maxPrice=90185547>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22&minPrice=0&maxPrice=50000000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=61328126&maxPrice=61718750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78076173&maxPrice=78100586>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=60937501&maxPrice=61328125>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90039064&maxPrice=90087891>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90087892&maxPrice=90136719>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=59375001&maxPrice=60156250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99902345&maxPrice=99951172>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68701173&maxPrice=68750000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68652345&maxPrice=68701172>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90380861&maxPrice=90429688>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90332033&maxPrice=90380860>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90234376&maxPrice=90283204>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99267580&maxPrice=99316407>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99560548&maxPrice=99609375>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99511720&maxPrice=99560547>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99218751&maxPrice=99267579>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65527345&maxPrice=65625000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65429689&maxPrice=65527344>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65332033&maxPrice=65429688>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=57421876&maxPrice=57812500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65234376&maxPrice=65332032>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68554689&maxPrice=68652344>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=57031251&maxPrice=57421875>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68261720&maxPrice=68359375>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68164064&maxPrice=68261719>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99804689&maxPrice=99902344>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99707033&maxPrice=99804688>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90527345&maxPrice=90625000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99609376&maxPrice=99707032>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90429689&maxPrice=90527344>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=99414064&maxPrice=99511719>: HTTP status code is not handled or not allowed
2023-10-28 20:03:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76464845&maxPrice=76562500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76367189&maxPrice=76464844>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65039064&maxPrice=65234375>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77929689&maxPrice=78027344>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77832033&maxPrice=77929688>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64843751&maxPrice=65039063>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77734376&maxPrice=77832032>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=56250001&maxPrice=57031250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67968751&maxPrice=68164063>: HTTP status code is not handled or not allowed
2023-10-28 20:03:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88867189&maxPrice=89062500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88671876&maxPrice=88867188>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68359376&maxPrice=68554688>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88476564&maxPrice=88671875>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=88281251&maxPrice=88476563>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=58593751&maxPrice=59375000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=57812501&maxPrice=58593750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79101564&maxPrice=79296875>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78906251&maxPrice=79101563>: HTTP status code is not handled or not allowed
2023-10-28 20:03:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80664064&maxPrice=80859375>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80468751&maxPrice=80664063>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79296876&maxPrice=79492188>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81054689&maxPrice=81250000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=80859376&maxPrice=81054688>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76171876&maxPrice=76367188>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=74218751&maxPrice=75000000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=73437501&maxPrice=74218750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=86914064&maxPrice=87109375>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=72656251&maxPrice=73437500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=86718751&maxPrice=86914063>: HTTP status code is not handled or not allowed
2023-10-28 20:03:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=71875001&maxPrice=72656250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87109376&maxPrice=87304688>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87304689&maxPrice=87500000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89453126&maxPrice=89843750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=89062501&maxPrice=89453125>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98828126&maxPrice=99218750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98437501&maxPrice=98828125>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=98046876&maxPrice=98437500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=97656251&maxPrice=98046875>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75781251&maxPrice=76171875>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=77343751&maxPrice=77734375>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75390626&maxPrice=75781250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=75000001&maxPrice=75390625>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=63281251&maxPrice=64062500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=62500001&maxPrice=63281250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=83984376&maxPrice=84375000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=83593751&maxPrice=83984375>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82812501&maxPrice=83203125>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=83203126&maxPrice=83593750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=64062501&maxPrice=64843750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=67187501&maxPrice=67968750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=87500001&maxPrice=88281250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92968751&maxPrice=93750000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=96875001&maxPrice=97656250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=92187501&maxPrice=92968750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=78125001&maxPrice=78906250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79687501&maxPrice=80468750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=76562501&maxPrice=77343750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85156251&maxPrice=85937500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=84375001&maxPrice=85156250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=81250001&maxPrice=82031250>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=82031251&maxPrice=82812500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=85937501&maxPrice=86718750>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=68750001&maxPrice=71875000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=65625001&maxPrice=67187500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2&minPrice=50000001&maxPrice=56250000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=90625001&maxPrice=92187500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=95312501&maxPrice=96875000>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=93750001&maxPrice=95312500>: HTTP status code is not handled or not allowed
2023-10-28 20:03:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:03:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 458873,
 'downloader/request_count': 768,
 'downloader/request_method_count/GET': 768,
 'downloader/response_bytes': 40953417,
 'downloader/response_count': 768,
 'downloader/response_status_count/200': 597,
 'downloader/response_status_count/403': 170,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 82.172179,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 3, 10, 938008),
 'httpcompression/response_bytes': 147756672,
 'httpcompression/response_count': 598,
 'httperror/response_ignored_count': 171,
 'httperror/response_ignored_status_count/403': 170,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 1618,
 'log_count/INFO': 182,
 'log_count/WARNING': 1,
 'request_depth_max': 52,
 'response_received_count': 768,
 'scheduler/dequeued': 768,
 'scheduler/dequeued/memory': 768,
 'scheduler/enqueued': 768,
 'scheduler/enqueued/memory': 768,
 'start_time': datetime.datetime(2023, 10, 29, 1, 1, 48, 765829)}
2023-10-28 20:03:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:11:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:11:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:11:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:11:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:11:28 [scrapy.extensions.telnet] INFO: Telnet Password: 6aa18bc5529ada4a
2023-10-28 20:11:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:11:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:11:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:11:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:11:28 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:11:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:11:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:11:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 20:11:29 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:11:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1174,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.251524,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 11, 29, 285192),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 11, 28, 33668)}
2023-10-28 20:11:29 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:11:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:11:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:11:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:11:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:11:34 [scrapy.extensions.telnet] INFO: Telnet Password: 8638dbf8d8e44eb0
2023-10-28 20:11:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:11:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:11:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:11:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:11:34 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:11:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:11:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:11:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 20:11:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:11:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1655,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.854997,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 11, 35, 117904),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 11, 34, 262907)}
2023-10-28 20:11:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:11:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:11:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:11:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:11:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:11:37 [scrapy.extensions.telnet] INFO: Telnet Password: 2b0ff13560793842
2023-10-28 20:11:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:11:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:11:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:11:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:11:37 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:11:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:11:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:11:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding>: HTTP status code is not handled or not allowed
2023-10-28 20:11:37 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:11:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1655,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.870018,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 11, 37, 927485),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 11, 37, 57467)}
2023-10-28 20:11:37 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:12:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:12:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:12:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:12:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:12:00 [scrapy.extensions.telnet] INFO: Telnet Password: 2d029e680f4d8fc1
2023-10-28 20:12:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:12:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:12:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:12:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:12:00 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:12:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:12:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:12:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:12:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:12:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:12:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:12:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:12:36 [scrapy.extensions.telnet] INFO: Telnet Password: ef0be8e42c4eb135
2023-10-28 20:12:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:12:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:12:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:12:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:12:36 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:12:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:12:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:12:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:13:36 [scrapy.extensions.logstats] INFO: Crawled 424 pages (at 424 pages/min), scraped 975 items (at 975 items/min)
2023-10-28 20:14:13 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-28 20:14:13 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-28 20:14:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 504723,
 'downloader/request_count': 841,
 'downloader/request_method_count/GET': 841,
 'downloader/response_bytes': 56857367,
 'downloader/response_count': 841,
 'downloader/response_status_count/200': 840,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 97.692618,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 14, 14, 680184),
 'httpcompression/response_bytes': 207133542,
 'httpcompression/response_count': 841,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 2750,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 56,
 'response_received_count': 841,
 'scheduler/dequeued': 841,
 'scheduler/dequeued/memory': 841,
 'scheduler/enqueued': 1003,
 'scheduler/enqueued/memory': 1003,
 'start_time': datetime.datetime(2023, 10, 29, 1, 12, 36, 987566)}
2023-10-28 20:14:14 [scrapy.core.engine] INFO: Spider closed (shutdown)
2023-10-28 20:14:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:14:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:14:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:14:18 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:14:18 [scrapy.extensions.telnet] INFO: Telnet Password: 7325a35b3179c02f
2023-10-28 20:14:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:14:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:14:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:14:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:14:18 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:14:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:14:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:14:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:14:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:14:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:14:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:14:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:14:45 [scrapy.extensions.telnet] INFO: Telnet Password: f127472ba9c1c8a0
2023-10-28 20:14:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:14:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:14:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:14:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:14:45 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:14:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:14:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:23:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:23:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:23:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:23:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:23:28 [scrapy.extensions.telnet] INFO: Telnet Password: 0644201a226f69a1
2023-10-28 20:23:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:23:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:23:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:23:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:23:28 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:23:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:23:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:23:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-mckay-gardens-professional-centre-unit-10-5-mckay-gardens-turner-act-2612-504403660> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-175-fitzmaurice-street-wagga-wagga-nsw-2650-504435284> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-58-bolton-street-newcastle-nsw-2300-504451764> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-137-marrickville-road-marrickville-nsw-2204-504414268> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-shops-1-2-princes-highway-st-peters-nsw-2044-504451768> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-30-ross-street-forest-lodge-nsw-2037-504408592> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-2-418-princes-highway-narre-warren-vic-3805-504414256> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-23-belmore-road-lorn-nsw-2320-504398400> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-20-93-97-newton-road-wetherill-park-nsw-2164-504424972> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-52-54-johnston-street-fitzroy-vic-3065-504414276> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-140-abernethy-road-belmont-wa-6104-504430312> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-164-port-road-alberton-sa-5014-504414248> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-1-31-33-leighton-place-hornsby-nsw-2077-504451740> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:49 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:23:49 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x16269b14e10>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 165, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:23:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 67417,
 'downloader/request_count': 122,
 'downloader/request_method_count/GET': 122,
 'downloader/response_bytes': 8357739,
 'downloader/response_count': 122,
 'downloader/response_status_count/200': 121,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 20.927003,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 23, 49, 402199),
 'httpcompression/response_bytes': 29794512,
 'httpcompression/response_count': 122,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 206,
 'log_count/ERROR': 15,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 12,
 'response_received_count': 122,
 'scheduler/dequeued': 122,
 'scheduler/dequeued/memory': 122,
 'scheduler/enqueued': 122,
 'scheduler/enqueued/memory': 122,
 'spider_exceptions/BrokenPipeError': 14,
 'start_time': datetime.datetime(2023, 10, 29, 1, 23, 28, 475196)}
2023-10-28 20:23:49 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:24:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:24:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:24:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:24:55 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:24:55 [scrapy.extensions.telnet] INFO: Telnet Password: 43ce1afd9d181c17
2023-10-28 20:24:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:24:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:24:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:24:55 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:24:55 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:24:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:24:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:25:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:25:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-warehouses-459-the-boulevarde-kirrawee-nsw-2232-504403712> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-28 20:25:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-penthouse-level-14-15-50-market-street-melbourne-vic-3000-504440964> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-54-terry-road-rouse-hill-nsw-2155-504435320> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-475-485-495-cobbitty-road-cobbitty-nsw-2570-504334148> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-146-gilba-road-girraween-nsw-2145-504392956> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-2-nepean-highway-mentone-vic-3194-504387232> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-jones-business-park-8-483-newman-road-geebung-qld-4034-504430384> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-part-level-11-488-bourke-street-melbourne-vic-3000-504387228> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-41-cookson-street-cnr-thorn-street-camberwell-vic-3124-504425112> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-the-hideaway-hotel-5-walter-street-tiaro-qld-4650-504398496> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 158, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:25:10 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x212f7245cd0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 165, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 20:25:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 41942,
 'downloader/request_count': 78,
 'downloader/request_method_count/GET': 78,
 'downloader/response_bytes': 5343654,
 'downloader/response_count': 78,
 'downloader/response_status_count/200': 77,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 14.728345,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 25, 10, 191165),
 'httpcompression/response_bytes': 19105221,
 'httpcompression/response_count': 78,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 129,
 'log_count/ERROR': 12,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 8,
 'response_received_count': 78,
 'scheduler/dequeued': 78,
 'scheduler/dequeued/memory': 78,
 'scheduler/enqueued': 78,
 'scheduler/enqueued/memory': 78,
 'spider_exceptions/BrokenPipeError': 11,
 'start_time': datetime.datetime(2023, 10, 29, 1, 24, 55, 462820)}
2023-10-28 20:25:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:29:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:29:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:29:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:29:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:29:45 [scrapy.extensions.telnet] INFO: Telnet Password: f23a4cd89d1938b2
2023-10-28 20:29:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:29:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:29:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:29:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:29:45 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:29:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:29:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:29:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:30:45 [scrapy.extensions.logstats] INFO: Crawled 455 pages (at 455 pages/min), scraped 1112 items (at 1112 items/min)
2023-10-28 20:37:43 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:37:43 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:37:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:37:43 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:37:43 [scrapy.extensions.telnet] INFO: Telnet Password: e6351ebe4541eba7
2023-10-28 20:37:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:37:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:37:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:37:43 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:37:43 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:37:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:37:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 20:37:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-28 20:38:43 [scrapy.extensions.logstats] INFO: Crawled 472 pages (at 472 pages/min), scraped 1153 items (at 1153 items/min)
2023-10-28 20:39:43 [scrapy.extensions.logstats] INFO: Crawled 1159 pages (at 687 pages/min), scraped 2067 items (at 914 items/min)
2023-10-28 20:40:43 [scrapy.extensions.logstats] INFO: Crawled 1839 pages (at 680 pages/min), scraped 3086 items (at 1019 items/min)
2023-10-28 20:41:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:41:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:41:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:41:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:41:13 [scrapy.extensions.telnet] INFO: Telnet Password: c83d47c834443dd0
2023-10-28 20:41:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:41:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:41:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:41:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:41:13 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:41:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:41:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:41:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:41:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:41:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:41:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:41:16 [scrapy.extensions.telnet] INFO: Telnet Password: aa3bf8a06fb28f6d
2023-10-28 20:41:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:41:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:41:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:41:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:41:16 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:41:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:41:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:41:18 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:41:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 316,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 60296,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 2.172472,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 41, 18, 702349),
 'httpcompression/response_bytes': 198064,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 41, 16, 529877)}
2023-10-28 20:41:18 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:41:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=79797365&maxPrice=79809571. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-28 20:41:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1&minPrice=94946291&maxPrice=94949342. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-28 20:41:43 [scrapy.extensions.logstats] INFO: Crawled 2380 pages (at 541 pages/min), scraped 4047 items (at 961 items/min)
2023-10-28 20:41:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:41:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:41:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:41:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:41:45 [scrapy.extensions.telnet] INFO: Telnet Password: 2fa61050e0dbec72
2023-10-28 20:41:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:41:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:41:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:41:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:41:45 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:41:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:41:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:41:49 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:41:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 316,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 59778,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.582783,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 41, 49, 24160),
 'httpcompression/response_bytes': 198023,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 41, 45, 441377)}
2023-10-28 20:41:49 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:41:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:41:52 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:41:52 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:41:52 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:41:52 [scrapy.extensions.telnet] INFO: Telnet Password: 644ddf63443a70e1
2023-10-28 20:41:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:41:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:41:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:41:52 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:41:52 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:41:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:41:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:41:55 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:41:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 351,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 60491,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.068465,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 41, 55, 81362),
 'httpcompression/response_bytes': 200116,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 41, 52, 12897)}
2023-10-28 20:41:55 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:42:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:42:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:42:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:42:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:42:14 [scrapy.extensions.telnet] INFO: Telnet Password: 783b43e4055461ed
2023-10-28 20:42:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:42:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:42:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:42:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:42:14 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:42:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:42:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:42:43 [scrapy.extensions.logstats] INFO: Crawled 2913 pages (at 533 pages/min), scraped 5096 items (at 1049 items/min)
2023-10-28 20:42:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=northern-region-sa&propertyTypes=industrial-warehouse> (failed 3 times): DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-28 20:42:51 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=northern-region-sa&propertyTypes=industrial-warehouse>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-28 20:42:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:42:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1053,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 36.320666,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 42, 51, 159391),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 29, 1, 42, 14, 838725)}
2023-10-28 20:42:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:43:43 [scrapy.extensions.logstats] INFO: Crawled 3482 pages (at 569 pages/min), scraped 6195 items (at 1099 items/min)
2023-10-28 20:43:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:43:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:43:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:43:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:43:45 [scrapy.extensions.telnet] INFO: Telnet Password: a75444ab4f77b918
2023-10-28 20:43:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:43:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:43:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:43:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:43:45 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:43:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:43:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:43:47 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:43:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 351,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 60495,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 1.827733,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 43, 47, 440857),
 'httpcompression/response_bytes': 200113,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 43, 45, 613124)}
2023-10-28 20:43:47 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:43:52 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:43:52 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:43:52 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:43:52 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:43:52 [scrapy.extensions.telnet] INFO: Telnet Password: 6f9eabfd698bae31
2023-10-28 20:43:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:43:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:43:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:43:52 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:43:52 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:43:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:43:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:43:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 20:43:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 351,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 59987,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 2.023199,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 1, 43, 54, 131298),
 'httpcompression/response_bytes': 200071,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 1, 43, 52, 108099)}
2023-10-28 20:43:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 20:44:43 [scrapy.extensions.logstats] INFO: Crawled 4055 pages (at 573 pages/min), scraped 8434 items (at 2239 items/min)
2023-10-28 20:45:43 [scrapy.extensions.logstats] INFO: Crawled 4612 pages (at 557 pages/min), scraped 13112 items (at 4678 items/min)
2023-10-28 20:46:05 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 20:46:05 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 20:46:05 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 20:46:05 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 20:46:05 [scrapy.extensions.telnet] INFO: Telnet Password: 0c8b388c876f7b3b
2023-10-28 20:46:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 20:46:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 20:46:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 20:46:05 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 20:46:05 [scrapy.core.engine] INFO: Spider opened
2023-10-28 20:46:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 20:46:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-28 20:46:43 [scrapy.extensions.logstats] INFO: Crawled 5175 pages (at 563 pages/min), scraped 18008 items (at 4896 items/min)
2023-10-28 20:47:05 [scrapy.extensions.logstats] INFO: Crawled 528 pages (at 528 pages/min), scraped 1910 items (at 1910 items/min)
2023-10-28 20:47:43 [scrapy.extensions.logstats] INFO: Crawled 5718 pages (at 543 pages/min), scraped 22409 items (at 4401 items/min)
2023-10-28 20:48:05 [scrapy.extensions.logstats] INFO: Crawled 1132 pages (at 604 pages/min), scraped 4974 items (at 3064 items/min)
2023-10-28 20:48:43 [scrapy.extensions.logstats] INFO: Crawled 6285 pages (at 567 pages/min), scraped 27001 items (at 4592 items/min)
2023-10-28 20:49:05 [scrapy.extensions.logstats] INFO: Crawled 1664 pages (at 532 pages/min), scraped 8293 items (at 3319 items/min)
2023-10-28 20:49:43 [scrapy.extensions.logstats] INFO: Crawled 6837 pages (at 552 pages/min), scraped 31529 items (at 4528 items/min)
2023-10-28 20:50:05 [scrapy.extensions.logstats] INFO: Crawled 2198 pages (at 534 pages/min), scraped 12261 items (at 3968 items/min)
2023-10-28 20:50:43 [scrapy.extensions.logstats] INFO: Crawled 7394 pages (at 557 pages/min), scraped 36169 items (at 4640 items/min)
2023-10-28 20:51:43 [scrapy.extensions.logstats] INFO: Crawled 7958 pages (at 564 pages/min), scraped 41024 items (at 4855 items/min)
2023-10-28 20:52:43 [scrapy.extensions.logstats] INFO: Crawled 8523 pages (at 565 pages/min), scraped 46004 items (at 4980 items/min)
2023-10-28 20:53:43 [scrapy.extensions.logstats] INFO: Crawled 9085 pages (at 562 pages/min), scraped 50828 items (at 4824 items/min)
2023-10-28 20:54:43 [scrapy.extensions.logstats] INFO: Crawled 9657 pages (at 572 pages/min), scraped 55718 items (at 4890 items/min)
2023-10-28 20:55:43 [scrapy.extensions.logstats] INFO: Crawled 10226 pages (at 569 pages/min), scraped 60268 items (at 4550 items/min)
2023-10-28 20:56:44 [scrapy.extensions.logstats] INFO: Crawled 10785 pages (at 559 pages/min), scraped 64802 items (at 4534 items/min)
2023-10-28 20:57:43 [scrapy.extensions.logstats] INFO: Crawled 11351 pages (at 566 pages/min), scraped 69365 items (at 4563 items/min)
2023-10-28 20:58:43 [scrapy.extensions.logstats] INFO: Crawled 11912 pages (at 561 pages/min), scraped 73958 items (at 4593 items/min)
2023-10-28 20:59:43 [scrapy.extensions.logstats] INFO: Crawled 12482 pages (at 570 pages/min), scraped 78683 items (at 4725 items/min)
2023-10-28 21:00:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-28 21:00:04 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-28 21:00:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 2,
 'downloader/request_bytes': 7955304,
 'downloader/request_count': 12691,
 'downloader/request_method_count/GET': 12691,
 'downloader/response_bytes': 894475494,
 'downloader/response_count': 12689,
 'downloader/response_status_count/200': 12688,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1342.047906,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 0, 6, 30656),
 'httpcompression/response_bytes': 3533235290,
 'httpcompression/response_count': 12689,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 80523,
 'log_count/INFO': 34,
 'log_count/WARNING': 3,
 'request_depth_max': 133,
 'response_received_count': 12689,
 'retry/count': 2,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 2,
 'scheduler/dequeued': 12691,
 'scheduler/dequeued/memory': 12691,
 'scheduler/enqueued': 12877,
 'scheduler/enqueued/memory': 12877,
 'start_time': datetime.datetime(2023, 10, 29, 1, 37, 43, 982750)}
2023-10-28 21:00:06 [scrapy.core.engine] INFO: Spider closed (shutdown)
2023-10-28 21:00:52 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:00:52 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:00:52 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:00:52 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:00:52 [scrapy.extensions.telnet] INFO: Telnet Password: 55f81b6c45d44894
2023-10-28 21:00:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:00:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:00:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:00:52 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:00:52 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:00:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:00:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:01:52 [scrapy.extensions.logstats] INFO: Crawled 445 pages (at 445 pages/min), scraped 1485 items (at 1485 items/min)
2023-10-28 21:02:52 [scrapy.extensions.logstats] INFO: Crawled 977 pages (at 532 pages/min), scraped 4401 items (at 2916 items/min)
2023-10-28 21:03:52 [scrapy.extensions.logstats] INFO: Crawled 1512 pages (at 535 pages/min), scraped 7282 items (at 2881 items/min)
2023-10-28 21:04:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-28 21:04:04 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-28 21:04:06 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-28 21:04:06 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1976564&maxPrice=1977540. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-28 21:04:06 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978517&maxPrice=1979493. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-28 21:20:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:20:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:20:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:20:24 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:20:24 [scrapy.extensions.telnet] INFO: Telnet Password: 6fd208bb52abff89
2023-10-28 21:20:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:20:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:20:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:20:24 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:20:24 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:20:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:20:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:20:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:20:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 158178,
 'downloader/request_count': 273,
 'downloader/request_method_count/GET': 273,
 'downloader/response_bytes': 18619717,
 'downloader/response_count': 273,
 'downloader/response_status_count/200': 273,
 'elapsed_time_seconds': 28.794605,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 20, 53, 417115),
 'httpcompression/response_bytes': 65826707,
 'httpcompression/response_count': 273,
 'item_scraped_count': 496,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 25,
 'response_received_count': 273,
 'scheduler/dequeued': 273,
 'scheduler/dequeued/memory': 273,
 'scheduler/enqueued': 273,
 'scheduler/enqueued/memory': 273,
 'start_time': datetime.datetime(2023, 10, 29, 2, 20, 24, 622510)}
2023-10-28 21:20:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:24:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:24:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:24:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:24:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:24:32 [scrapy.extensions.telnet] INFO: Telnet Password: 2da2f5023769aa05
2023-10-28 21:24:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:24:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:24:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:24:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:24:32 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:24:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:24:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:25:32 [scrapy.extensions.logstats] INFO: Crawled 298 pages (at 298 pages/min), scraped 740 items (at 740 items/min)
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1570314&maxPrice=1578125>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1757814&maxPrice=1765625>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1957033&maxPrice=1960938>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1960939&maxPrice=1964844>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1750001&maxPrice=1757813>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1218751&maxPrice=1234375>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1902345&maxPrice=1906250>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1964845&maxPrice=1968750>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1773439&maxPrice=1777344>: HTTP status code is not handled or not allowed
2023-10-28 21:25:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1953126&maxPrice=1957032>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1234376&maxPrice=1250000>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1777345&maxPrice=1781250>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1140626&maxPrice=1156250>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1769533&maxPrice=1773438>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1765626&maxPrice=1769532>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1125001&maxPrice=1140625>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1589845&maxPrice=1593750>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1582033&maxPrice=1585938>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1917970&maxPrice=1921875>: HTTP status code is not handled or not allowed
2023-10-28 21:25:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1914064&maxPrice=1917969>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1578126&maxPrice=1582032>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1613283&maxPrice=1617188>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1609376&maxPrice=1613282>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1621095&maxPrice=1625000>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1617189&maxPrice=1621094>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1585939&maxPrice=1589844>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1171876&maxPrice=1187500>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1156251&maxPrice=1171875>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1894533&maxPrice=1898438>: HTTP status code is not handled or not allowed
2023-10-28 21:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1890626&maxPrice=1894532>: HTTP status code is not handled or not allowed
2023-10-28 21:26:32 [scrapy.extensions.logstats] INFO: Crawled 912 pages (at 614 pages/min), scraped 3665 items (at 2925 items/min)
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1640626&maxPrice=1642579>: HTTP status code is not handled or not allowed
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1252932&maxPrice=1253907>: HTTP status code is not handled or not allowed
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1652345&maxPrice=1654297>: HTTP status code is not handled or not allowed
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1293947&maxPrice=1294922>: HTTP status code is not handled or not allowed
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1253908&maxPrice=1254884>: HTTP status code is not handled or not allowed
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1269533&maxPrice=1270509>: HTTP status code is not handled or not allowed
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1272463&maxPrice=1273438>: HTTP status code is not handled or not allowed
2023-10-28 21:26:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1278322&maxPrice=1279297>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1295900&maxPrice=1296875>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1254885&maxPrice=1255860>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1294923&maxPrice=1295899>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1270510&maxPrice=1271485>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1809572&maxPrice=1810547>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1808595&maxPrice=1809571>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1811525&maxPrice=1812500>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1810548&maxPrice=1811524>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1262697&maxPrice=1263672>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1277345&maxPrice=1278321>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1261720&maxPrice=1262696>: HTTP status code is not handled or not allowed
2023-10-28 21:26:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1271486&maxPrice=1272462>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1649416&maxPrice=1650391>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1648439&maxPrice=1649415>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1642580&maxPrice=1643556>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1643557&maxPrice=1644532>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1805666&maxPrice=1806641>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1804689&maxPrice=1805665>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1806642&maxPrice=1807618>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1807619&maxPrice=1808594>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1255861&maxPrice=1256837>: HTTP status code is not handled or not allowed
2023-10-28 21:26:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1256838&maxPrice=1257813>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1292970&maxPrice=1293946>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1250978&maxPrice=1251954>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1250001&maxPrice=1250977>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1281251&maxPrice=1285157>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1285158&maxPrice=1289063>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1251955&maxPrice=1252931>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1542970&maxPrice=1544922>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1544923&maxPrice=1546875>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1207033&maxPrice=1210938>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1203126&maxPrice=1207032>: HTTP status code is not handled or not allowed
2023-10-28 21:26:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1257814&maxPrice=1261719>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1265626&maxPrice=1269532>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1273439&maxPrice=1277344>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1210939&maxPrice=1214844>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1800783&maxPrice=1804688>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1671876&maxPrice=1675782>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1796876&maxPrice=1800782>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1628908&maxPrice=1632813>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1625001&maxPrice=1628907>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1636720&maxPrice=1640625>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1632814&maxPrice=1636719>: HTTP status code is not handled or not allowed
2023-10-28 21:26:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1656251&maxPrice=1660157>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1683595&maxPrice=1687500>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1679689&maxPrice=1683594>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1667970&maxPrice=1671875>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1664064&maxPrice=1667969>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1535158&maxPrice=1539063>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1531251&maxPrice=1535157>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1539064&maxPrice=1542969>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1546876&maxPrice=1550782>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1558595&maxPrice=1562500>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1554689&maxPrice=1558594>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1304689&maxPrice=1312500>: HTTP status code is not handled or not allowed
2023-10-28 21:26:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1296876&maxPrice=1304688>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1789064&maxPrice=1796875>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1781251&maxPrice=1789063>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1820314&maxPrice=1828125>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1812501&maxPrice=1820313>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1835939&maxPrice=1843750>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1828126&maxPrice=1835938>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1078126&maxPrice=1093750>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1062501&maxPrice=1078125>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1187501&maxPrice=1203125>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1734376&maxPrice=1750000>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1437501&maxPrice=1500000>: HTTP status code is not handled or not allowed
2023-10-28 21:26:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1718751&maxPrice=1734375>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1375001&maxPrice=1437500>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=7&minPrice=0&maxPrice=1000000>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1984376&maxPrice=2000000>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1968751&maxPrice=1984375>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1000001&maxPrice=1062500>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1500001&maxPrice=1531250>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1687501&maxPrice=1718750>: HTTP status code is not handled or not allowed
2023-10-28 21:26:41 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:26:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 674673,
 'downloader/request_count': 1020,
 'downloader/request_method_count/GET': 1020,
 'downloader/response_bytes': 61983551,
 'downloader/response_count': 1020,
 'downloader/response_status_count/200': 907,
 'downloader/response_status_count/403': 113,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 129.049367,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 26, 41, 668918),
 'httpcompression/response_bytes': 236960341,
 'httpcompression/response_count': 907,
 'httperror/response_ignored_count': 113,
 'httperror/response_ignored_status_count/403': 113,
 'item_scraped_count': 3801,
 'log_count/INFO': 125,
 'log_count/WARNING': 1,
 'request_depth_max': 62,
 'response_received_count': 1020,
 'scheduler/dequeued': 1020,
 'scheduler/dequeued/memory': 1020,
 'scheduler/enqueued': 1020,
 'scheduler/enqueued/memory': 1020,
 'start_time': datetime.datetime(2023, 10, 29, 2, 24, 32, 619551)}
2023-10-28 21:26:41 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:31:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:31:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:31:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:31:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:31:32 [scrapy.extensions.telnet] INFO: Telnet Password: deb313f874b83798
2023-10-28 21:31:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:31:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:31:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:31:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:31:32 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:31:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:31:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:31:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 21:31:33 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 21:31:33 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:31:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 1047,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 1.73537,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 31, 33, 862209),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 29, 2, 31, 32, 126839)}
2023-10-28 21:31:33 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:31:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:31:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:31:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:31:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:31:51 [scrapy.extensions.telnet] INFO: Telnet Password: edb31ce12eb5f52b
2023-10-28 21:31:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:31:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:31:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:31:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:31:51 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:31:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:31:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:31:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 21:31:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-10-28 21:31:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:31:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 1047,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 1.497598,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 31, 53, 286196),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 29, 2, 31, 51, 788598)}
2023-10-28 21:31:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:33:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:33:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:33:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:33:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:33:31 [scrapy.extensions.telnet] INFO: Telnet Password: 309ea984cc1ca55b
2023-10-28 21:33:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:33:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:33:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:33:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:33:31 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:33:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:33:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:34:31 [scrapy.extensions.logstats] INFO: Crawled 532 pages (at 532 pages/min), scraped 1001 items (at 1001 items/min)
2023-10-28 21:35:31 [scrapy.extensions.logstats] INFO: Crawled 1200 pages (at 668 pages/min), scraped 1721 items (at 720 items/min)
2023-10-28 21:36:31 [scrapy.extensions.logstats] INFO: Crawled 1869 pages (at 669 pages/min), scraped 2432 items (at 711 items/min)
2023-10-28 21:37:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15907242&maxPrice=15908218>: HTTP status code is not handled or not allowed
2023-10-28 21:37:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15906265&maxPrice=15907241>: HTTP status code is not handled or not allowed
2023-10-28 21:37:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15914078&maxPrice=15915054>: HTTP status code is not handled or not allowed
2023-10-28 21:37:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15909196&maxPrice=15910171>: HTTP status code is not handled or not allowed
2023-10-28 21:37:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15908219&maxPrice=15909195>: HTTP status code is not handled or not allowed
2023-10-28 21:37:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11414074&maxPrice=11415050>: HTTP status code is not handled or not allowed
2023-10-28 21:37:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15917008&maxPrice=15917983>: HTTP status code is not handled or not allowed
2023-10-28 21:37:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15921890&maxPrice=15922866>: HTTP status code is not handled or not allowed
2023-10-28 21:37:31 [scrapy.extensions.logstats] INFO: Crawled 2542 pages (at 673 pages/min), scraped 3146 items (at 714 items/min)
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5375982&maxPrice=5376958>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5375005&maxPrice=5375981>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5379889&maxPrice=5380864>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5378912&maxPrice=5379888>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14927749&maxPrice=14929701>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11166027&maxPrice=11167979>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15905289&maxPrice=15906264>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15904312&maxPrice=15905288>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15877946&maxPrice=15878921>: HTTP status code is not handled or not allowed
2023-10-28 21:38:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15876969&maxPrice=15877945>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14925796&maxPrice=14927748>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11164074&maxPrice=11166026>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11158215&maxPrice=11160167>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11156261&maxPrice=11158214>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15903336&maxPrice=15904311>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15902359&maxPrice=15903335>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11162121&maxPrice=11164073>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11160168&maxPrice=11162120>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11169933&maxPrice=11171885>: HTTP status code is not handled or not allowed
2023-10-28 21:38:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11167980&maxPrice=11169932>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14935561&maxPrice=14937513>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14933608&maxPrice=14935560>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14931655&maxPrice=14933607>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14929702&maxPrice=14931654>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14923843&maxPrice=14925795>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14921889&maxPrice=14923842>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15875992&maxPrice=15876968>: HTTP status code is not handled or not allowed
2023-10-28 21:38:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15875015&maxPrice=15875991>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14906264&maxPrice=14907240>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11214855&maxPrice=11215831>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11188488&maxPrice=11189464>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11187511&maxPrice=11188487>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.extensions.logstats] INFO: Crawled 3207 pages (at 665 pages/min), scraped 3834 items (at 688 items/min)
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11190442&maxPrice=11191417>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11189465&maxPrice=11190441>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11203136&maxPrice=11210948>: HTTP status code is not handled or not allowed
2023-10-28 21:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15898453&maxPrice=15902358>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9753916&maxPrice=9757821>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9750009&maxPrice=9753915>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9761728&maxPrice=9765633>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9757822&maxPrice=9761727>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=11&minPrice=0&maxPrice=2000000>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5386724&maxPrice=5390629>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9769541&maxPrice=9773446>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5382818&maxPrice=5386723>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14890639&maxPrice=14906263>: HTTP status code is not handled or not allowed
2023-10-28 21:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14875014&maxPrice=14890638>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15890640&maxPrice=15898452>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9765634&maxPrice=9769540>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9816416&maxPrice=9820321>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9812509&maxPrice=9816415>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9820322&maxPrice=9824227>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11140636&maxPrice=11156260>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11125011&maxPrice=11140635>: HTTP status code is not handled or not allowed
2023-10-28 21:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12750012&maxPrice=12875011>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9625009&maxPrice=9750008>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9500009&maxPrice=9625008>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14750014&maxPrice=14875013>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11000010&maxPrice=11125010>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9824228&maxPrice=9828133>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11875011&maxPrice=12000010>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2765627&maxPrice=2781251>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11750011&maxPrice=11875010>: HTTP status code is not handled or not allowed
2023-10-28 21:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2750002&maxPrice=2765626>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2984377&maxPrice=3000001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2968752&maxPrice=2984376>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2875002&maxPrice=2890626>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2906252&maxPrice=2921876>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5000004&maxPrice=5062504>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4937504&maxPrice=5000003>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4875004&maxPrice=4937503>: HTTP status code is not handled or not allowed
2023-10-28 21:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5187505&maxPrice=5250004>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5125005&maxPrice=5187504>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12500012&maxPrice=12750011>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15500015&maxPrice=15750014>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=10750010&maxPrice=11000009>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=10500010&maxPrice=10750009>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=512000511&maxPrice=0>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=256000255&maxPrice=512000510>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=10250010&maxPrice=10500009>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=10000009&maxPrice=10250009>: HTTP status code is not handled or not allowed
2023-10-28 21:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11500011&maxPrice=11750010>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2843752&maxPrice=2875001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2812502&maxPrice=2843751>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2937502&maxPrice=2968751>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4625004&maxPrice=4750003>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4500004&maxPrice=4625003>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4750004&maxPrice=4875003>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9000008&maxPrice=9500008>: HTTP status code is not handled or not allowed
2023-10-28 21:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5250005&maxPrice=5375004>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12000011&maxPrice=12500011>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14000013&maxPrice=14500013>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13500013&maxPrice=14000012>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13000012&maxPrice=13500012>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15000014&maxPrice=15500014>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=128000127&maxPrice=256000254>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2437502&maxPrice=2500001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2375002&maxPrice=2437501>: HTTP status code is not handled or not allowed
2023-10-28 21:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2562502&maxPrice=2625001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2500002&maxPrice=2562501>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2687502&maxPrice=2750001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2625002&maxPrice=2687501>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3937503&maxPrice=4000002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3875003&maxPrice=3937502>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3812503&maxPrice=3875002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3750003&maxPrice=3812502>: HTTP status code is not handled or not allowed
2023-10-28 21:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8000007&maxPrice=9000007>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=64000063&maxPrice=128000126>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2250002&maxPrice=2375001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2125002&maxPrice=2250001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2000001&maxPrice=2125001>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3125003&maxPrice=3250002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3000002&maxPrice=3125002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3625003&maxPrice=3750002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3500003&maxPrice=3625002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3375003&maxPrice=3500002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3250003&maxPrice=3375002>: HTTP status code is not handled or not allowed
2023-10-28 21:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4000003&maxPrice=4500003>: HTTP status code is not handled or not allowed
2023-10-28 21:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=16000015&maxPrice=24000022>: HTTP status code is not handled or not allowed
2023-10-28 21:38:41 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:38:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2256361,
 'downloader/request_count': 3293,
 'downloader/request_method_count/GET': 3293,
 'downloader/response_bytes': 197873487,
 'downloader/response_count': 3293,
 'downloader/response_status_count/200': 3167,
 'downloader/response_status_count/403': 126,
 'elapsed_time_seconds': 309.988898,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 38, 41, 313375),
 'httpcompression/response_bytes': 686389325,
 'httpcompression/response_count': 3167,
 'httperror/response_ignored_count': 126,
 'httperror/response_ignored_status_count/403': 126,
 'item_scraped_count': 3834,
 'log_count/INFO': 141,
 'log_count/WARNING': 1,
 'request_depth_max': 41,
 'response_received_count': 3293,
 'scheduler/dequeued': 3293,
 'scheduler/dequeued/memory': 3293,
 'scheduler/enqueued': 3293,
 'scheduler/enqueued/memory': 3293,
 'start_time': datetime.datetime(2023, 10, 29, 2, 33, 31, 324477)}
2023-10-28 21:38:41 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:41:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:41:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:41:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:41:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:41:50 [scrapy.extensions.telnet] INFO: Telnet Password: 228f3832eb9012b1
2023-10-28 21:41:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:41:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:41:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:41:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:41:50 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:41:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:41:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>: HTTP status code is not handled or not allowed
2023-10-28 21:41:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:41:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 349,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1173,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.468016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 41, 51, 585112),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 2, 41, 50, 117096)}
2023-10-28 21:41:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:43:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:43:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:43:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:43:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:43:12 [scrapy.extensions.telnet] INFO: Telnet Password: 18f49aea8fa101c1
2023-10-28 21:43:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:43:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:43:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:43:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:43:12 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:43:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:43:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:43:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15968765&maxPrice=16000014>: HTTP status code is not handled or not allowed
2023-10-28 21:43:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15937515&maxPrice=15968764>: HTTP status code is not handled or not allowed
2023-10-28 21:43:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8781258&maxPrice=8812507>: HTTP status code is not handled or not allowed
2023-10-28 21:43:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8750008&maxPrice=8781257>: HTTP status code is not handled or not allowed
2023-10-28 21:43:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8843758&maxPrice=8875007>: HTTP status code is not handled or not allowed
2023-10-28 21:43:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9968759&maxPrice=10000008>: HTTP status code is not handled or not allowed
2023-10-28 21:43:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2052737&maxPrice=2054689>: HTTP status code is not handled or not allowed
2023-10-28 21:43:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2041018&maxPrice=2042970>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9930674&maxPrice=9931649>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9929697&maxPrice=9930673>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9932627&maxPrice=9933602>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9931650&maxPrice=9932626>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8977548&maxPrice=8978523>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8973642&maxPrice=8974617>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8975595&maxPrice=8976570>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8974618&maxPrice=8975594>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8976571&maxPrice=8977547>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8972665&maxPrice=8973641>: HTTP status code is not handled or not allowed
2023-10-28 21:44:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8968758&maxPrice=8969734>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8982430&maxPrice=8983406>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8995126&maxPrice=8996101>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8994149&maxPrice=8995125>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8989267&maxPrice=8990242>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8988290&maxPrice=8989266>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8984383&maxPrice=8985359>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8987314&maxPrice=8988289>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8986337&maxPrice=8987313>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8998055&maxPrice=8999031>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9935556&maxPrice=9937508>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9933603&maxPrice=9935555>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9923838&maxPrice=9925790>: HTTP status code is not handled or not allowed
2023-10-28 21:44:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9921884&maxPrice=9923837>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9927744&maxPrice=9929696>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9925791&maxPrice=9927743>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9906259&maxPrice=9921883>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2058596&maxPrice=2060548>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3488285&maxPrice=3490237>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9937509&maxPrice=9968758>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9875009&maxPrice=9906258>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13343763&maxPrice=13375012>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13312513&maxPrice=13343762>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8937508&maxPrice=8968757>: HTTP status code is not handled or not allowed
2023-10-28 21:44:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8812508&maxPrice=8843757>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13468763&maxPrice=13500012>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2027346&maxPrice=2031251>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13437513&maxPrice=13468762>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2023440&maxPrice=2027345>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2164065&maxPrice=2167970>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2175784&maxPrice=2179689>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2179690&maxPrice=2183595>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2171877&maxPrice=2175783>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3929691&maxPrice=3933596>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2054690&maxPrice=2058595>: HTTP status code is not handled or not allowed
2023-10-28 21:44:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2046877&maxPrice=2050783>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3917972&maxPrice=3921877>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3914066&maxPrice=3917971>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3484378&maxPrice=3488284>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3496097&maxPrice=3500002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3492191&maxPrice=3496096>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13250013&maxPrice=13312512>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2007814&maxPrice=2015626>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8875008&maxPrice=8937507>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2000001&maxPrice=2007813>: HTTP status code is not handled or not allowed
2023-10-28 21:44:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13375013&maxPrice=13437512>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15875015&maxPrice=15937514>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15812515&maxPrice=15875014>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15750015&maxPrice=15812514>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2156252&maxPrice=2164064>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2031252&maxPrice=2039064>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3921878&maxPrice=3929690>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3882816&maxPrice=3890627>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3875003&maxPrice=3882815>: HTTP status code is not handled or not allowed
2023-10-28 21:44:12 [scrapy.extensions.logstats] INFO: Crawled 597 pages (at 597 pages/min), scraped 1118 items (at 1118 items/min)
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3890628&maxPrice=3898440>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3476566&maxPrice=3484377>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3906253&maxPrice=3914065>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3898441&maxPrice=3906252>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13125013&maxPrice=13250012>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3468753&maxPrice=3476565>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9750009&maxPrice=9875008>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13000012&maxPrice=13125012>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15625015&maxPrice=15750014>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15500015&maxPrice=15625014>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2109377&maxPrice=2125001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2093752&maxPrice=2109376>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15250015&maxPrice=15375014>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15375015&maxPrice=15500014>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2234377&maxPrice=2250001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2218752&maxPrice=2234376>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3421878&maxPrice=3437502>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3406253&maxPrice=3421877>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5375005&maxPrice=5437504>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9250009&maxPrice=9500008>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9000008&maxPrice=9250008>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9500009&maxPrice=9750008>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12750012&maxPrice=13000011>: HTTP status code is not handled or not allowed
2023-10-28 21:44:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12500012&maxPrice=12750011>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13750013&maxPrice=14000012>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13500013&maxPrice=13750012>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15000014&maxPrice=15250014>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2125002&maxPrice=2156251>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2062502&maxPrice=2093751>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2187502&maxPrice=2218751>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3937503&maxPrice=3968752>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3968753&maxPrice=4000002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3437503&maxPrice=3468752>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3375003&maxPrice=3406252>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4625004&maxPrice=4750003>: HTTP status code is not handled or not allowed
2023-10-28 21:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4500004&maxPrice=4625003>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4875004&maxPrice=5000003>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4750004&maxPrice=4875003>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12000011&maxPrice=12500011>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5250005&maxPrice=5375004>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3562503&maxPrice=3625002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3500003&maxPrice=3562502>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3625003&maxPrice=3687502>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3687503&maxPrice=3750002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14000013&maxPrice=14500013>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14500014&maxPrice=15000013>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=128000127&maxPrice=0>: HTTP status code is not handled or not allowed
2023-10-28 21:44:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=7&minPrice=0&maxPrice=2000000>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=64000063&maxPrice=128000126>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3187503&maxPrice=3250002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3125003&maxPrice=3187502>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3312503&maxPrice=3375002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11000010&maxPrice=12000010>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3250003&maxPrice=3312502>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=10000009&maxPrice=11000009>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4250004&maxPrice=4500003>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4000003&maxPrice=4250003>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5000004&maxPrice=5250004>: HTTP status code is not handled or not allowed
2023-10-28 21:44:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2625002&maxPrice=2750001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2500002&maxPrice=2625001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2875002&maxPrice=3000001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2750002&maxPrice=2875001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3750003&maxPrice=3875002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=24000023&maxPrice=32000030>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=16000015&maxPrice=24000022>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3000002&maxPrice=3125002>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2375002&maxPrice=2500001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2250002&maxPrice=2375001>: HTTP status code is not handled or not allowed
2023-10-28 21:44:18 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:44:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 429439,
 'downloader/request_count': 665,
 'downloader/request_method_count/GET': 665,
 'downloader/response_bytes': 34921728,
 'downloader/response_count': 665,
 'downloader/response_status_count/200': 525,
 'downloader/response_status_count/403': 140,
 'elapsed_time_seconds': 65.972098,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 44, 18, 664389),
 'httpcompression/response_bytes': 124514528,
 'httpcompression/response_count': 525,
 'httperror/response_ignored_count': 140,
 'httperror/response_ignored_status_count/403': 140,
 'item_scraped_count': 1118,
 'log_count/INFO': 151,
 'log_count/WARNING': 1,
 'request_depth_max': 41,
 'response_received_count': 665,
 'scheduler/dequeued': 665,
 'scheduler/dequeued/memory': 665,
 'scheduler/enqueued': 665,
 'scheduler/enqueued/memory': 665,
 'start_time': datetime.datetime(2023, 10, 29, 2, 43, 12, 692291)}
2023-10-28 21:44:18 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:51:42 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:51:42 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:51:42 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:51:42 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:51:42 [scrapy.extensions.telnet] INFO: Telnet Password: e0508993cec233d5
2023-10-28 21:51:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:51:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:51:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:51:42 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:51:42 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:51:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:51:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:52:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:52:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25&minPrice=0&maxPrice=2000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:52:42 [scrapy.extensions.logstats] INFO: Crawled 315 pages (at 315 pages/min), scraped 908 items (at 908 items/min)
2023-10-28 21:52:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=1000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=1000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=500000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=22&minPrice=0&maxPrice=500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=21&minPrice=0&maxPrice=250000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=20&minPrice=0&maxPrice=250000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=16&minPrice=0&maxPrice=125000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=15&minPrice=0&maxPrice=125000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=10&minPrice=0&maxPrice=62500> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=9&minPrice=0&maxPrice=62500)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=5&minPrice=0&maxPrice=31250> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=4&minPrice=0&maxPrice=31250)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:42 [scrapy.extensions.logstats] INFO: Crawled 398 pages (at 83 pages/min), scraped 1712 items (at 804 items/min)
2023-10-28 21:53:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=1953> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:53:44 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:53:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 246471,
 'downloader/request_count': 403,
 'downloader/request_method_count/GET': 403,
 'downloader/response_bytes': 28125940,
 'downloader/response_count': 403,
 'downloader/response_status_count/200': 403,
 'elapsed_time_seconds': 122.816985,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 53, 44, 961503),
 'httpcompression/response_bytes': 107828980,
 'httpcompression/response_count': 403,
 'item_scraped_count': 1735,
 'log_count/ERROR': 12,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 154,
 'response_received_count': 403,
 'scheduler/dequeued': 403,
 'scheduler/dequeued/memory': 403,
 'scheduler/enqueued': 403,
 'scheduler/enqueued/memory': 403,
 'spider_exceptions/AttributeError': 12,
 'start_time': datetime.datetime(2023, 10, 29, 2, 51, 42, 144518)}
2023-10-28 21:53:44 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 21:54:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 21:54:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 21:54:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 21:54:26 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 21:54:26 [scrapy.extensions.telnet] INFO: Telnet Password: 26e00a2f7a696282
2023-10-28 21:54:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 21:54:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 21:54:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 21:54:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 21:54:26 [scrapy.core.engine] INFO: Spider opened
2023-10-28 21:54:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 21:54:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 21:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:55:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25&minPrice=0&maxPrice=2000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:55:26 [scrapy.extensions.logstats] INFO: Crawled 317 pages (at 317 pages/min), scraped 918 items (at 918 items/min)
2023-10-28 21:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=1000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=1000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:55:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=500000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=22&minPrice=0&maxPrice=500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=21&minPrice=0&maxPrice=250000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=20&minPrice=0&maxPrice=250000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=16&minPrice=0&maxPrice=125000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=15&minPrice=0&maxPrice=125000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=10&minPrice=0&maxPrice=62500> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=9&minPrice=0&maxPrice=62500)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=5&minPrice=0&maxPrice=31250> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=4&minPrice=0&maxPrice=31250)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:26 [scrapy.extensions.logstats] INFO: Crawled 400 pages (at 83 pages/min), scraped 1724 items (at 806 items/min)
2023-10-28 21:56:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=1953> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 138, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 21:56:28 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 21:56:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 246471,
 'downloader/request_count': 403,
 'downloader/request_method_count/GET': 403,
 'downloader/response_bytes': 28127263,
 'downloader/response_count': 403,
 'downloader/response_status_count/200': 403,
 'elapsed_time_seconds': 121.860675,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 2, 56, 28, 88316),
 'httpcompression/response_bytes': 107837547,
 'httpcompression/response_count': 403,
 'item_scraped_count': 1735,
 'log_count/ERROR': 12,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 154,
 'response_received_count': 403,
 'scheduler/dequeued': 403,
 'scheduler/dequeued/memory': 403,
 'scheduler/enqueued': 403,
 'scheduler/enqueued/memory': 403,
 'spider_exceptions/AttributeError': 12,
 'start_time': datetime.datetime(2023, 10, 29, 2, 54, 26, 227641)}
2023-10-28 21:56:28 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:03:05 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:03:05 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:03:05 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:03:05 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:03:05 [scrapy.extensions.telnet] INFO: Telnet Password: f7b4da797c059117
2023-10-28 22:03:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:03:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:03:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:03:05 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:03:05 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:03:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:03:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:03:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:03:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25&minPrice=0&maxPrice=2000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:04:05 [scrapy.extensions.logstats] INFO: Crawled 310 pages (at 310 pages/min), scraped 858 items (at 858 items/min)
2023-10-28 22:04:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=1000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=1000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:04:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=500000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=22&minPrice=0&maxPrice=500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=21&minPrice=0&maxPrice=250000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=20&minPrice=0&maxPrice=250000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:05:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=16&minPrice=0&maxPrice=125000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=15&minPrice=0&maxPrice=125000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:05:05 [scrapy.extensions.logstats] INFO: Crawled 383 pages (at 73 pages/min), scraped 1576 items (at 718 items/min)
2023-10-28 22:05:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=10&minPrice=0&maxPrice=62500> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=9&minPrice=0&maxPrice=62500)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:05:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=5&minPrice=0&maxPrice=31250> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=4&minPrice=0&maxPrice=31250)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:05:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:05:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:05:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:05:22 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 22:05:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 245798,
 'downloader/request_count': 402,
 'downloader/request_method_count/GET': 402,
 'downloader/response_bytes': 28064214,
 'downloader/response_count': 402,
 'downloader/response_status_count/200': 402,
 'elapsed_time_seconds': 136.646578,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 3, 5, 22, 67733),
 'httpcompression/response_bytes': 107626777,
 'httpcompression/response_count': 402,
 'item_scraped_count': 1733,
 'log_count/ERROR': 11,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 153,
 'response_received_count': 402,
 'scheduler/dequeued': 402,
 'scheduler/dequeued/memory': 402,
 'scheduler/enqueued': 402,
 'scheduler/enqueued/memory': 402,
 'spider_exceptions/AttributeError': 11,
 'start_time': datetime.datetime(2023, 10, 29, 3, 3, 5, 421155)}
2023-10-28 22:05:22 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:11:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:11:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:11:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:11:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:11:31 [scrapy.extensions.telnet] INFO: Telnet Password: b92760a5c3b342d7
2023-10-28 22:11:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:11:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:11:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:11:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:11:31 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:11:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:11:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:12:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:12:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25&minPrice=0&maxPrice=2000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:12:31 [scrapy.extensions.logstats] INFO: Crawled 306 pages (at 306 pages/min), scraped 818 items (at 818 items/min)
2023-10-28 22:12:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=1000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=1000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=500000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=22&minPrice=0&maxPrice=500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=21&minPrice=0&maxPrice=250000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=20&minPrice=0&maxPrice=250000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=16&minPrice=0&maxPrice=125000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=15&minPrice=0&maxPrice=125000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:31 [scrapy.extensions.logstats] INFO: Crawled 382 pages (at 76 pages/min), scraped 1566 items (at 748 items/min)
2023-10-28 22:13:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=10&minPrice=0&maxPrice=62500> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=9&minPrice=0&maxPrice=62500)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=5&minPrice=0&maxPrice=31250> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=4&minPrice=0&maxPrice=31250)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 145, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:13:46 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 22:13:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 245798,
 'downloader/request_count': 402,
 'downloader/request_method_count/GET': 402,
 'downloader/response_bytes': 28061110,
 'downloader/response_count': 402,
 'downloader/response_status_count/200': 402,
 'elapsed_time_seconds': 134.721756,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 3, 13, 46, 2618),
 'httpcompression/response_bytes': 107618160,
 'httpcompression/response_count': 402,
 'item_scraped_count': 1733,
 'log_count/ERROR': 11,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 153,
 'response_received_count': 402,
 'scheduler/dequeued': 402,
 'scheduler/dequeued/memory': 402,
 'scheduler/enqueued': 402,
 'scheduler/enqueued/memory': 402,
 'spider_exceptions/AttributeError': 11,
 'start_time': datetime.datetime(2023, 10, 29, 3, 11, 31, 280862)}
2023-10-28 22:13:46 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:20:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:20:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:20:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:20:25 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:20:25 [scrapy.extensions.telnet] INFO: Telnet Password: 4648c841f544d79a
2023-10-28 22:20:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:20:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:20:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:20:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:20:25 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:20:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:20:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:20:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:21:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25&minPrice=0&maxPrice=2000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:21:25 [scrapy.extensions.logstats] INFO: Crawled 317 pages (at 317 pages/min), scraped 928 items (at 928 items/min)
2023-10-28 22:21:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=1000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=1000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:21:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=500000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=22&minPrice=0&maxPrice=500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:22:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=21&minPrice=0&maxPrice=250000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=20&minPrice=0&maxPrice=250000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:22:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=16&minPrice=0&maxPrice=125000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=15&minPrice=0&maxPrice=125000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:22:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=10&minPrice=0&maxPrice=62500> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=9&minPrice=0&maxPrice=62500)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:22:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=5&minPrice=0&maxPrice=31250> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=4&minPrice=0&maxPrice=31250)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:22:25 [scrapy.extensions.logstats] INFO: Crawled 397 pages (at 80 pages/min), scraped 1702 items (at 774 items/min)
2023-10-28 22:22:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:22:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:22:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:33:00 [scrapy.extensions.logstats] INFO: Crawled 402 pages (at 5 pages/min), scraped 1733 items (at 31 items/min)
2023-10-28 22:33:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 22:33:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 245798,
 'downloader/request_count': 402,
 'downloader/request_method_count/GET': 402,
 'downloader/response_bytes': 28044125,
 'downloader/response_count': 402,
 'downloader/response_status_count/200': 402,
 'elapsed_time_seconds': 755.400248,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 3, 33, 0, 542018),
 'httpcompression/response_bytes': 107625432,
 'httpcompression/response_count': 402,
 'item_scraped_count': 1733,
 'log_count/ERROR': 11,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 153,
 'response_received_count': 402,
 'scheduler/dequeued': 402,
 'scheduler/dequeued/memory': 402,
 'scheduler/enqueued': 402,
 'scheduler/enqueued/memory': 402,
 'spider_exceptions/AttributeError': 11,
 'start_time': datetime.datetime(2023, 10, 29, 3, 20, 25, 141770)}
2023-10-28 22:33:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:34:05 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:34:05 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:34:05 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:34:05 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:34:05 [scrapy.extensions.telnet] INFO: Telnet Password: b59373f0fb665e11
2023-10-28 22:34:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:34:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:34:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:34:05 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:34:05 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:34:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:34:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:34:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:34:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25&minPrice=0&maxPrice=2000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:35:05 [scrapy.extensions.logstats] INFO: Crawled 318 pages (at 318 pages/min), scraped 938 items (at 938 items/min)
2023-10-28 22:35:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24&minPrice=0&maxPrice=1000000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=1000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:35:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=23&minPrice=0&maxPrice=500000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=22&minPrice=0&maxPrice=500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:36:05 [scrapy.extensions.logstats] INFO: Crawled 353 pages (at 35 pages/min), scraped 1282 items (at 344 items/min)
2023-10-28 22:36:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=21&minPrice=0&maxPrice=250000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=20&minPrice=0&maxPrice=250000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:36:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=16&minPrice=0&maxPrice=125000> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=15&minPrice=0&maxPrice=125000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:37:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=10&minPrice=0&maxPrice=62500> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=9&minPrice=0&maxPrice=62500)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:37:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=5&minPrice=0&maxPrice=31250> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=4&minPrice=0&maxPrice=31250)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:37:06 [scrapy.extensions.logstats] INFO: Crawled 397 pages (at 44 pages/min), scraped 1702 items (at 420 items/min)
2023-10-28 22:37:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:37:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=0&maxPrice=15625)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:37:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=3906> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=0&maxPrice=7812)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 146, in parse
    self.msleep(20)
    ^^^^^^^^^^^
AttributeError: 'scrapspider' object has no attribute 'msleep'
2023-10-28 22:37:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 22:37:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 245908,
 'downloader/request_count': 402,
 'downloader/request_method_count/GET': 402,
 'downloader/response_bytes': 28040395,
 'downloader/response_count': 402,
 'downloader/response_status_count/200': 402,
 'elapsed_time_seconds': 187.473476,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 3, 37, 13, 180971),
 'httpcompression/response_bytes': 107566499,
 'httpcompression/response_count': 402,
 'item_scraped_count': 1733,
 'log_count/ERROR': 11,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 153,
 'response_received_count': 402,
 'scheduler/dequeued': 402,
 'scheduler/dequeued/memory': 402,
 'scheduler/enqueued': 402,
 'scheduler/enqueued/memory': 402,
 'spider_exceptions/AttributeError': 11,
 'start_time': datetime.datetime(2023, 10, 29, 3, 34, 5, 707495)}
2023-10-28 22:37:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:37:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:37:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:37:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:37:53 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:37:53 [scrapy.extensions.telnet] INFO: Telnet Password: 071775792a3b6266
2023-10-28 22:37:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:37:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:37:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:37:53 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:37:53 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:37:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:37:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:38:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2203127&maxPrice=2210939>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3507816&maxPrice=3515627>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3500003&maxPrice=3507815>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3554691&maxPrice=3562502>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3546878&maxPrice=3554690>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3929691&maxPrice=3937502>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3289066&maxPrice=3296877>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3296878&maxPrice=3304690>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3921878&maxPrice=3929690>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5015630&maxPrice=5031254>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3281253&maxPrice=3289065>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5000004&maxPrice=5015629>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5031255&maxPrice=5046879>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5046880&maxPrice=5062504>: HTTP status code is not handled or not allowed
2023-10-28 22:38:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3304691&maxPrice=3312502>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3273441&maxPrice=3281252>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5265630&maxPrice=5281254>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5343755&maxPrice=5359379>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5250005&maxPrice=5265629>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5359380&maxPrice=5375004>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5296880&maxPrice=5312504>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5281255&maxPrice=5296879>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9187509&maxPrice=9250008>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2214846&maxPrice=2218751>: HTTP status code is not handled or not allowed
2023-10-28 22:38:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2210940&maxPrice=2214845>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9125009&maxPrice=9187508>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5328130&maxPrice=5343754>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5312505&maxPrice=5328129>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9000008&maxPrice=9062508>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9062509&maxPrice=9125008>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3265628&maxPrice=3273440>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3523441&maxPrice=3531252>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3515628&maxPrice=3523440>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3539066&maxPrice=3546877>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3531253&maxPrice=3539065>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3617191&maxPrice=3625002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2164065&maxPrice=2171876>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2156252&maxPrice=2164064>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3609378&maxPrice=3617190>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2179690&maxPrice=2187501>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2171877&maxPrice=2179689>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5375005&maxPrice=5406254>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3687503&maxPrice=3703127>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5093755&maxPrice=5125004>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5062505&maxPrice=5093754>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3703128&maxPrice=3718752>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5406255&maxPrice=5437504>: HTTP status code is not handled or not allowed
2023-10-28 22:38:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9375009&maxPrice=9500008>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9250009&maxPrice=9375008>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3734378&maxPrice=3750002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3718753&maxPrice=3734377>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3250003&maxPrice=3265627>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3593753&maxPrice=3609377>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2140627&maxPrice=2156251>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2125002&maxPrice=2140626>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3906253&maxPrice=3921877>: HTTP status code is not handled or not allowed
2023-10-28 22:38:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2187502&maxPrice=2203126>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2234377&maxPrice=2250001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2218752&maxPrice=2234376>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8750008&maxPrice=9000007>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8500008&maxPrice=8750007>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4250004&maxPrice=4312503>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4312504&maxPrice=4375003>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4437504&maxPrice=4500003>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4375004&maxPrice=4437503>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5125005&maxPrice=5187504>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5187505&maxPrice=5250004>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9750009&maxPrice=10000008>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9500009&maxPrice=9750008>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3656253&maxPrice=3687502>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3625003&maxPrice=3656252>: HTTP status code is not handled or not allowed
2023-10-28 22:38:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3343753&maxPrice=3375002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3312503&maxPrice=3343752>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3562503&maxPrice=3593752>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3875003&maxPrice=3906252>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12500012&maxPrice=13000011>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15000014&maxPrice=15500014>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15500015&maxPrice=16000014>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12000011&maxPrice=12500011>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13500013&maxPrice=14000012>: HTTP status code is not handled or not allowed
2023-10-28 22:38:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13000012&maxPrice=13500012>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14500014&maxPrice=15000013>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14000013&maxPrice=14500013>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3968753&maxPrice=4000002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3937503&maxPrice=3968752>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=16000015&maxPrice=18000016>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=22000021&maxPrice=24000022>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=18000017&maxPrice=20000018>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=20000019&maxPrice=22000020>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=128000127&maxPrice=256000254>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=256000255&maxPrice=0>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3062503&maxPrice=3125002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3000002&maxPrice=3062502>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3187503&maxPrice=3250002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3125003&maxPrice=3187502>: HTTP status code is not handled or not allowed
2023-10-28 22:38:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3437503&maxPrice=3500002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3375003&maxPrice=3437502>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=7&minPrice=0&maxPrice=2000000>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2812502&maxPrice=2875001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2750002&maxPrice=2812501>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4750004&maxPrice=5000003>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4500004&maxPrice=4750003>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4000003&maxPrice=4250003>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2937502&maxPrice=3000001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2875002&maxPrice=2937501>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11000010&maxPrice=12000010>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=10000009&maxPrice=11000009>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3750003&maxPrice=3875002>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2000001&maxPrice=2125001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2375002&maxPrice=2500001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2250002&maxPrice=2375001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2625002&maxPrice=2750001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2500002&maxPrice=2625001>: HTTP status code is not handled or not allowed
2023-10-28 22:38:41 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 22:38:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 326894,
 'downloader/request_count': 517,
 'downloader/request_method_count/GET': 517,
 'downloader/response_bytes': 27185794,
 'downloader/response_count': 517,
 'downloader/response_status_count/200': 404,
 'downloader/response_status_count/403': 113,
 'elapsed_time_seconds': 47.850806,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 3, 38, 41, 647760),
 'httpcompression/response_bytes': 96480769,
 'httpcompression/response_count': 404,
 'httperror/response_ignored_count': 113,
 'httperror/response_ignored_status_count/403': 113,
 'item_scraped_count': 821,
 'log_count/INFO': 123,
 'log_count/WARNING': 1,
 'request_depth_max': 35,
 'response_received_count': 517,
 'scheduler/dequeued': 517,
 'scheduler/dequeued/memory': 517,
 'scheduler/enqueued': 517,
 'scheduler/enqueued/memory': 517,
 'start_time': datetime.datetime(2023, 10, 29, 3, 37, 53, 796954)}
2023-10-28 22:38:41 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:39:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:39:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:39:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:39:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:39:56 [scrapy.extensions.telnet] INFO: Telnet Password: cc9ddec67f7ba511
2023-10-28 22:39:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:39:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:39:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:39:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:39:56 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:39:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:39:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:39:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>: HTTP status code is not handled or not allowed
2023-10-28 22:39:57 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 22:39:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 349,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1659,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.105476,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 3, 39, 57, 395261),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 3, 39, 56, 289785)}
2023-10-28 22:39:57 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:40:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:40:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:40:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:40:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:40:02 [scrapy.extensions.telnet] INFO: Telnet Password: fa107f893557ecf6
2023-10-28 22:40:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:40:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:40:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:40:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:40:02 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:40:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:40:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:40:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>: HTTP status code is not handled or not allowed
2023-10-28 22:40:03 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 22:40:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 349,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1659,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.878587,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 3, 40, 3, 340402),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 3, 40, 2, 461815)}
2023-10-28 22:40:03 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 22:40:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:40:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:40:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:40:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:40:34 [scrapy.extensions.telnet] INFO: Telnet Password: 575cf88da4ca6790
2023-10-28 22:40:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:40:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:40:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:40:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:40:34 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:40:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:40:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:41:57 [scrapy.extensions.logstats] INFO: Crawled 458 pages (at 458 pages/min), scraped 918 items (at 918 items/min)
2023-10-28 22:45:08 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:45:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:45:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:45:08 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:45:08 [scrapy.extensions.telnet] INFO: Telnet Password: 4c89224e1ac62b83
2023-10-28 22:45:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:45:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:45:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:45:08 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:45:08 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:45:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:45:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:46:08 [scrapy.extensions.logstats] INFO: Crawled 530 pages (at 530 pages/min), scraped 1815 items (at 1815 items/min)
2023-10-28 22:47:08 [scrapy.extensions.logstats] INFO: Crawled 1180 pages (at 650 pages/min), scraped 4876 items (at 3061 items/min)
2023-10-28 22:48:08 [scrapy.extensions.logstats] INFO: Crawled 1810 pages (at 630 pages/min), scraped 8234 items (at 3358 items/min)
2023-10-28 22:49:08 [scrapy.extensions.logstats] INFO: Crawled 2457 pages (at 647 pages/min), scraped 12486 items (at 4252 items/min)
2023-10-28 22:51:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 22:51:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 22:51:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 22:51:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 22:51:31 [scrapy.extensions.telnet] INFO: Telnet Password: 118f22b36fa64184
2023-10-28 22:51:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 22:51:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 22:51:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 22:51:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 22:51:31 [scrapy.core.engine] INFO: Spider opened
2023-10-28 22:51:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 22:51:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 22:52:31 [scrapy.extensions.logstats] INFO: Crawled 498 pages (at 498 pages/min), scraped 1861 items (at 1861 items/min)
2023-10-28 22:53:31 [scrapy.extensions.logstats] INFO: Crawled 1148 pages (at 650 pages/min), scraped 6866 items (at 5005 items/min)
2023-10-28 22:55:08 [scrapy.extensions.logstats] INFO: Crawled 1450 pages (at 302 pages/min), scraped 9016 items (at 2150 items/min)
2023-10-28 22:55:36 [scrapy.extensions.logstats] INFO: Crawled 1458 pages (at 8 pages/min), scraped 9071 items (at 55 items/min)
2023-10-28 22:56:31 [scrapy.extensions.logstats] INFO: Crawled 2057 pages (at 599 pages/min), scraped 13726 items (at 4655 items/min)
2023-10-28 22:57:31 [scrapy.extensions.logstats] INFO: Crawled 2714 pages (at 657 pages/min), scraped 19351 items (at 5625 items/min)
2023-10-28 22:58:31 [scrapy.extensions.logstats] INFO: Crawled 3373 pages (at 659 pages/min), scraped 24919 items (at 5568 items/min)
2023-10-28 23:00:10 [scrapy.extensions.logstats] INFO: Crawled 3443 pages (at 70 pages/min), scraped 25466 items (at 547 items/min)
2023-10-28 23:00:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 23:00:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 23:00:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 23:00:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 23:00:20 [scrapy.extensions.telnet] INFO: Telnet Password: d4bf377a65c9ba6f
2023-10-28 23:00:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 23:00:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 23:00:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 23:00:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 23:00:20 [scrapy.core.engine] INFO: Spider opened
2023-10-28 23:00:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 23:00:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-218-main-north-road-prospect-sa-5082-504314800> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=13)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 837, in _callmethod
    raise convert_to_error(kind, result)
multiprocessing.managers.RemoteError: 
---------------------------------------------------------------------------
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 260, in serve_client
    self.id_to_local_proxy_obj[ident]
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
KeyError: '190462ab410'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 262, in serve_client
    raise ke
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 256, in serve_client
    obj, exposed, gettypeid = id_to_obj[ident]
                              ~~~~~~~~~^^^^^^^
KeyError: '190462ab410'
---------------------------------------------------------------------------
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-unit-4-302-cormack-road-wingfield-sa-5013-504443608> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=13)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-level-1-268-flinders-street-adelaide-sa-5000-504395572> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=15> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 62, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-159-hermann-thumm-drive-lyndoch-sa-5351-504438196> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=13)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-45-grenfell-street-adelaide-sa-5000-501503355> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-60-john-street-salisbury-sa-5108-504368644> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-unit-1-653-magill-road-magill-sa-5072-504411200> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-4-8-angas-street-kent-town-sa-5067-504365276> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-26-flinders-street-adelaide-sa-5000-502948210> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-238-unley-road-unley-sa-5061-504437936> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-101-grenfell-street-adelaide-sa-5000-502552830> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-1326-1378-south-road-clovelly-park-sa-5042-504325804> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-level-2-11-81-flinders-street-adelaide-sa-5000-504019535> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=14)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-20-cawthorne-street-thebarton-sa-5031-504437836> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 180, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 23:00:42 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x235e5c255d0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 187, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-28 23:00:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 84087,
 'downloader/request_count': 156,
 'downloader/request_method_count/GET': 156,
 'downloader/response_bytes': 10677901,
 'downloader/response_count': 156,
 'downloader/response_status_count/200': 156,
 'elapsed_time_seconds': 22.123581,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 4, 0, 42, 236171),
 'httpcompression/response_bytes': 38355168,
 'httpcompression/response_count': 156,
 'item_scraped_count': 267,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 15,
 'response_received_count': 156,
 'scheduler/dequeued': 156,
 'scheduler/dequeued/memory': 156,
 'scheduler/enqueued': 156,
 'scheduler/enqueued/memory': 156,
 'spider_exceptions/BrokenPipeError': 14,
 'spider_exceptions/RemoteError': 1,
 'start_time': datetime.datetime(2023, 10, 29, 4, 0, 20, 112590)}
2023-10-28 23:00:42 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 23:01:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 23:01:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 23:01:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 23:01:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 23:01:13 [scrapy.extensions.telnet] INFO: Telnet Password: 3c613bdfe697f88f
2023-10-28 23:01:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 23:01:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 23:01:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 23:01:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 23:01:13 [scrapy.core.engine] INFO: Spider opened
2023-10-28 23:01:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 23:01:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 23:02:13 [scrapy.extensions.logstats] INFO: Crawled 221 pages (at 221 pages/min), scraped 730 items (at 730 items/min)
2023-10-28 23:03:13 [scrapy.extensions.logstats] INFO: Crawled 748 pages (at 527 pages/min), scraped 2594 items (at 1864 items/min)
2023-10-28 23:03:17 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-28 23:03:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 522849,
 'downloader/request_count': 785,
 'downloader/request_method_count/GET': 785,
 'downloader/response_bytes': 52122393,
 'downloader/response_count': 785,
 'downloader/response_status_count/200': 785,
 'elapsed_time_seconds': 124.089097,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 4, 3, 17, 190890),
 'httpcompression/response_bytes': 195887427,
 'httpcompression/response_count': 785,
 'item_scraped_count': 2815,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 108,
 'response_received_count': 785,
 'scheduler/dequeued': 785,
 'scheduler/dequeued/memory': 785,
 'scheduler/enqueued': 785,
 'scheduler/enqueued/memory': 785,
 'start_time': datetime.datetime(2023, 10, 29, 4, 1, 13, 101793)}
2023-10-28 23:03:17 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-28 23:08:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 23:08:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 23:08:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 23:08:18 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 23:08:18 [scrapy.extensions.telnet] INFO: Telnet Password: 4536f523f560a89a
2023-10-28 23:08:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 23:08:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 23:08:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 23:08:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 23:08:18 [scrapy.core.engine] INFO: Spider opened
2023-10-28 23:08:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 23:08:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 23:09:18 [scrapy.extensions.logstats] INFO: Crawled 502 pages (at 502 pages/min), scraped 1897 items (at 1897 items/min)
2023-10-28 23:10:18 [scrapy.extensions.logstats] INFO: Crawled 1138 pages (at 636 pages/min), scraped 5600 items (at 3703 items/min)
2023-10-28 23:12:21 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-28 23:12:21 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-28 23:12:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-28 23:12:21 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-28 23:12:21 [scrapy.extensions.telnet] INFO: Telnet Password: 7c0cb0480a4a5d30
2023-10-28 23:12:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-28 23:12:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-28 23:12:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-28 23:12:21 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-28 23:12:21 [scrapy.core.engine] INFO: Spider opened
2023-10-28 23:12:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-28 23:12:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-28 23:13:21 [scrapy.extensions.logstats] INFO: Crawled 443 pages (at 443 pages/min), scraped 1396 items (at 1396 items/min)
2023-10-28 23:14:21 [scrapy.extensions.logstats] INFO: Crawled 1060 pages (at 617 pages/min), scraped 4403 items (at 3007 items/min)
2023-10-28 23:15:21 [scrapy.extensions.logstats] INFO: Crawled 1681 pages (at 621 pages/min), scraped 4960 items (at 557 items/min)
2023-10-28 23:15:31 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-28 23:15:31 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-28 23:15:32 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 00:09:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:09:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:09:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:09:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:09:22 [scrapy.extensions.telnet] INFO: Telnet Password: ba5f60cddeb2ad48
2023-10-29 00:09:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:09:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:09:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:09:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:09:22 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:09:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:09:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:10:22 [scrapy.extensions.logstats] INFO: Crawled 306 pages (at 306 pages/min), scraped 769 items (at 769 items/min)
2023-10-29 00:11:22 [scrapy.extensions.logstats] INFO: Crawled 623 pages (at 317 pages/min), scraped 1842 items (at 1073 items/min)
2023-10-29 00:11:44 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 00:11:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 494422,
 'downloader/request_count': 764,
 'downloader/request_method_count/GET': 764,
 'downloader/response_bytes': 53260166,
 'downloader/response_count': 764,
 'downloader/response_status_count/200': 764,
 'elapsed_time_seconds': 142.059885,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 11, 44, 381660),
 'httpcompression/response_bytes': 211897743,
 'httpcompression/response_count': 764,
 'item_scraped_count': 2367,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 135,
 'response_received_count': 764,
 'scheduler/dequeued': 764,
 'scheduler/dequeued/memory': 764,
 'scheduler/enqueued': 764,
 'scheduler/enqueued/memory': 764,
 'start_time': datetime.datetime(2023, 10, 29, 5, 9, 22, 321775)}
2023-10-29 00:11:44 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 00:18:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:18:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:18:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:18:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:18:06 [scrapy.extensions.telnet] INFO: Telnet Password: b0411ff59677eace
2023-10-29 00:18:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:18:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:18:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:18:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:18:06 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:18:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:18:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:19:06 [scrapy.extensions.logstats] INFO: Crawled 504 pages (at 504 pages/min), scraped 608 items (at 608 items/min)
2023-10-29 00:20:06 [scrapy.extensions.logstats] INFO: Crawled 1146 pages (at 642 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:21:06 [scrapy.extensions.logstats] INFO: Crawled 1769 pages (at 623 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:22:06 [scrapy.extensions.logstats] INFO: Crawled 2400 pages (at 631 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:22:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:22:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:22:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:22:18 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:22:18 [scrapy.extensions.telnet] INFO: Telnet Password: 38f10df6e47da7f0
2023-10-29 00:22:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:22:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:22:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:22:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:22:18 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:22:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:22:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-29 00:23:06 [scrapy.extensions.logstats] INFO: Crawled 3026 pages (at 626 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:23:18 [scrapy.extensions.logstats] INFO: Crawled 531 pages (at 531 pages/min), scraped 617 items (at 617 items/min)
2023-10-29 00:24:06 [scrapy.extensions.logstats] INFO: Crawled 3648 pages (at 622 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:25:01 [scrapy.extensions.logstats] INFO: Crawled 1004 pages (at 473 pages/min), scraped 617 items (at 0 items/min)
2023-10-29 00:25:06 [scrapy.extensions.logstats] INFO: Crawled 4278 pages (at 630 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:25:27 [scrapy.extensions.logstats] INFO: Crawled 1012 pages (at 8 pages/min), scraped 617 items (at 0 items/min)
2023-10-29 00:26:06 [scrapy.extensions.logstats] INFO: Crawled 4778 pages (at 500 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:27:06 [scrapy.extensions.logstats] INFO: Crawled 5286 pages (at 508 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:28:06 [scrapy.extensions.logstats] INFO: Crawled 5828 pages (at 542 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:28:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:28:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:28:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:28:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:28:47 [scrapy.extensions.telnet] INFO: Telnet Password: fb7f58fd4ab56196
2023-10-29 00:28:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:28:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:28:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:28:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:28:47 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:28:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:28:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-29 00:29:06 [scrapy.extensions.logstats] INFO: Crawled 6458 pages (at 630 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:29:47 [scrapy.extensions.logstats] INFO: Crawled 446 pages (at 446 pages/min), scraped 1157 items (at 1157 items/min)
2023-10-29 00:30:06 [scrapy.extensions.logstats] INFO: Crawled 7095 pages (at 637 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:30:27 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 00:30:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 408137,
 'downloader/request_count': 634,
 'downloader/request_method_count/GET': 634,
 'downloader/response_bytes': 44101663,
 'downloader/response_count': 634,
 'downloader/response_status_count/200': 634,
 'elapsed_time_seconds': 100.692452,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 30, 27, 940359),
 'httpcompression/response_bytes': 173438109,
 'httpcompression/response_count': 634,
 'item_scraped_count': 1571,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 96,
 'response_received_count': 634,
 'scheduler/dequeued': 634,
 'scheduler/dequeued/memory': 634,
 'scheduler/enqueued': 634,
 'scheduler/enqueued/memory': 634,
 'start_time': datetime.datetime(2023, 10, 29, 5, 28, 47, 247907)}
2023-10-29 00:30:27 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 00:31:06 [scrapy.extensions.logstats] INFO: Crawled 7747 pages (at 652 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:32:06 [scrapy.extensions.logstats] INFO: Crawled 8403 pages (at 656 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:33:06 [scrapy.extensions.logstats] INFO: Crawled 9067 pages (at 664 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:34:06 [scrapy.extensions.logstats] INFO: Crawled 9712 pages (at 645 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:34:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:34:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:34:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:34:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:34:35 [scrapy.extensions.telnet] INFO: Telnet Password: 2ae1acce140b0ccf
2023-10-29 00:34:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:34:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:34:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:34:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:34:35 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:34:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:34:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-29 00:34:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799570159&maxPrice=1799572111>: HTTP status code is not handled or not allowed
2023-10-29 00:34:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799568206&maxPrice=1799570158>: HTTP status code is not handled or not allowed
2023-10-29 00:34:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799579924&maxPrice=1799581877>: HTTP status code is not handled or not allowed
2023-10-29 00:34:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799591643&maxPrice=1799593595>: HTTP status code is not handled or not allowed
2023-10-29 00:34:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799574065&maxPrice=1799575041>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799583831&maxPrice=1799584807>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799576018&maxPrice=1799576994>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-93-ferries-mcdonald-road-monarto-south-sa-5254-504425000>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-14-13-17-sturton-road-edinburgh-sa-5111-504430260>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799594573&maxPrice=1799595548>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-cavan-connect-logistics-park-23-45-stock-road-cavan-sa-5094-504343976>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-4-15-penner-avenue-burton-sa-5110-504386284>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-sale/property-50-moss-avenue-marleston-sa-5033-504451072?listingType=rent>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-1-2-42-roxburgh-avenue-lonsdale-sa-5160-504440100>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=3>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-511-grand-junction-wingfield-sa-5013-504438896>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799575042&maxPrice=1799576017>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662324905&maxPrice=1662325880>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799584808&maxPrice=1799585783>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799586761&maxPrice=1799587736>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799576995&maxPrice=1799577970>: HTTP status code is not handled or not allowed
2023-10-29 00:34:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799593596&maxPrice=1799594572>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799582855&maxPrice=1799583830>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-4a-108-tolley-road-st-agnes-sa-5097-504407016>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-2-portsmouth-court-gillman-sa-5013-504450084>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799581878&maxPrice=1799582854>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-20-22-kapara-road-gillman-sa-5013-504434100>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799577971&maxPrice=1799578947>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799578948&maxPrice=1799579923>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-25-31-hewittson-road-edinburgh-north-sa-5113-504327364>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au//for-lease/property-336-346-richmond-road-netley-sa-5037-504402600>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 00:34:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 13049,
 'downloader/request_count': 23,
 'downloader/request_method_count/GET': 23,
 'downloader/response_bytes': 706142,
 'downloader/response_count': 23,
 'downloader/response_status_count/200': 10,
 'downloader/response_status_count/403': 13,
 'elapsed_time_seconds': 5.054289,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 34, 40, 275643),
 'httpcompression/response_bytes': 2536224,
 'httpcompression/response_count': 10,
 'httperror/response_ignored_count': 13,
 'httperror/response_ignored_status_count/403': 13,
 'item_scraped_count': 28,
 'log_count/INFO': 23,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 23,
 'scheduler/dequeued': 23,
 'scheduler/dequeued/memory': 23,
 'scheduler/enqueued': 23,
 'scheduler/enqueued/memory': 23,
 'start_time': datetime.datetime(2023, 10, 29, 5, 34, 35, 221354)}
2023-10-29 00:34:40 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799573089&maxPrice=1799574064>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799572112&maxPrice=1799573088>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799585784&maxPrice=1799586760>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662323928&maxPrice=1662324904>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803652194&maxPrice=1803653170>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803658053&maxPrice=1803660006>: HTTP status code is not handled or not allowed
2023-10-29 00:34:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803661960&maxPrice=1803663912>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799564299&maxPrice=1799568205>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799587737&maxPrice=1799591642>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662321975&maxPrice=1662323927>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662327834&maxPrice=1662329786>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662325881&maxPrice=1662327833>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662316116&maxPrice=1662318068>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662314162&maxPrice=1662316115>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978848659&maxPrice=1978849634>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978847682&maxPrice=1978848658>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660470410&maxPrice=1660478222>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660493848&maxPrice=1660501659>: HTTP status code is not handled or not allowed
2023-10-29 00:34:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660486035&maxPrice=1660493847>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978822291&maxPrice=1978830102>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978814478&maxPrice=1978822290>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803767428&maxPrice=1803775240>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803759616&maxPrice=1803767427>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803751803&maxPrice=1803759615>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978869166&maxPrice=1978871118>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978851588&maxPrice=1978853540>: HTTP status code is not handled or not allowed
2023-10-29 00:34:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978849635&maxPrice=1978851587>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803863132&maxPrice=1803864108>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803862155&maxPrice=1803863131>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803861178&maxPrice=1803862154>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978634791&maxPrice=1978635767>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978636744&maxPrice=1978637720>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978633815&maxPrice=1978634790>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978632838&maxPrice=1978633814>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660345410&maxPrice=1660376659>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660314160&maxPrice=1660345409>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978701197&maxPrice=1978705102>: HTTP status code is not handled or not allowed
2023-10-29 00:34:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978697291&maxPrice=1978701196>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978640650&maxPrice=1978642602>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978638697&maxPrice=1978640649>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978628932&maxPrice=1978630884>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978626978&maxPrice=1978628931>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662439162&maxPrice=1662501661>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662376662&maxPrice=1662439161>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978689478&maxPrice=1978697290>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660251660&maxPrice=1660314159>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978705103&maxPrice=1978712915>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660376660&maxPrice=1660439159>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978643580&maxPrice=1978644556>: HTTP status code is not handled or not allowed
2023-10-29 00:34:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978642603&maxPrice=1978643579>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978644557&maxPrice=1978645533>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978751978&maxPrice=1978814477>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1653064153&maxPrice=1653126652>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1653001652&maxPrice=1653064152>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1802501802&maxPrice=1802751801>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1971751971&maxPrice=1972001970>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1971501971&maxPrice=1971751970>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1855251855&maxPrice=1855501854>: HTTP status code is not handled or not allowed
2023-10-29 00:34:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1855001854&maxPrice=1855251854>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1853501853&maxPrice=1853751852>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1855751855&maxPrice=1856001854>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1855501855&maxPrice=1855751854>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1979501979&maxPrice=1979751978>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1653751653&maxPrice=1654001652>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1653501653&maxPrice=1653751652>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1660501660&maxPrice=1660751659>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662751662&maxPrice=1663001661>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662501662&maxPrice=1662751661>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1662001661&maxPrice=1662251661>: HTTP status code is not handled or not allowed
2023-10-29 00:34:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1797501797&maxPrice=1798001796>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1799001798&maxPrice=1799501798>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1797001796&maxPrice=1797501796>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1803001802&maxPrice=1803501802>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1851501851&maxPrice=1852001850>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1851001850&maxPrice=1851501850>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1983501983&maxPrice=1984001982>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1983001982&maxPrice=1983501982>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1854501854&maxPrice=1855001853>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1854001853&maxPrice=1854501853>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1978001977&maxPrice=1978501977>: HTTP status code is not handled or not allowed
2023-10-29 00:34:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1979001978&maxPrice=1979501978>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1657501657&maxPrice=1658001656>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1657001656&maxPrice=1657501656>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1661501661&maxPrice=1662001660>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1661001660&maxPrice=1661501660>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1663501663&maxPrice=1664001662>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1796001795&maxPrice=1797001795>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1663001662&maxPrice=1663501662>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1798001797&maxPrice=1799001797>: HTTP status code is not handled or not allowed
2023-10-29 00:34:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1849001848&maxPrice=1850001848>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1848001847&maxPrice=1849001847>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1970001969&maxPrice=1971001969>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1850001849&maxPrice=1851001849>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1982001981&maxPrice=1983001981>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1977001976&maxPrice=1978001976>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1976001975&maxPrice=1977001975>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1852001851&maxPrice=1853001851>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1652001651&maxPrice=1653001651>: HTTP status code is not handled or not allowed
2023-10-29 00:34:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1655001654&maxPrice=1656001654>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1654001653&maxPrice=1655001653>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1656001655&maxPrice=1657001655>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1659001658&maxPrice=1660001658>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1658001657&maxPrice=1659001657>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1822001821&maxPrice=1824001822>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1820001819&maxPrice=1822001820>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1800001799&maxPrice=1802001800>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1806001805&maxPrice=1808001806>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1804001803&maxPrice=1806001804>: HTTP status code is not handled or not allowed
2023-10-29 00:34:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1842001841&maxPrice=1844001842>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1840001839&maxPrice=1842001840>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1844001843&maxPrice=1846001844>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1846001845&maxPrice=1848001846>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1968001967&maxPrice=1970001968>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1974001973&maxPrice=1976001974>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1972001971&maxPrice=1974001972>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1980001979&maxPrice=1982001980>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1634001633&maxPrice=1636001634>: HTTP status code is not handled or not allowed
2023-10-29 00:34:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1632001631&maxPrice=1634001632>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1638001637&maxPrice=1640001638>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1648001647&maxPrice=1650001648>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1650001649&maxPrice=1652001650>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1636001635&maxPrice=1638001636>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1816001815&maxPrice=1820001818>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1792001791&maxPrice=1796001794>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1396001395&maxPrice=1400001398>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1392001391&maxPrice=1396001394>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1896001895&maxPrice=1900001898>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1836001835&maxPrice=1840001838>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1832001831&maxPrice=1836001834>: HTTP status code is not handled or not allowed
2023-10-29 00:34:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1900001899&maxPrice=1904001902>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1404001403&maxPrice=1408001406>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1400001399&maxPrice=1404001402>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1352001351&maxPrice=1360001358>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1344001343&maxPrice=1352001350>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1368001367&maxPrice=1376001374>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1384001383&maxPrice=1392001390>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1376001375&maxPrice=1384001382>: HTTP status code is not handled or not allowed
2023-10-29 00:34:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1360001359&maxPrice=1368001366>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1644001643&maxPrice=1648001646>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1640001639&maxPrice=1644001642>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1808001807&maxPrice=1816001814>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1824001823&maxPrice=1832001830>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1888001887&maxPrice=1896001894>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1904001903&maxPrice=1912001910>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1912001911&maxPrice=1920001918>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1952001951&maxPrice=1968001966>: HTTP status code is not handled or not allowed
2023-10-29 00:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1872001871&maxPrice=1888001886>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1856001855&maxPrice=1872001870>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14220718&maxPrice=14221694>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1616001615&maxPrice=1632001630>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1600001599&maxPrice=1616001614>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1584001583&maxPrice=1600001598>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1568001567&maxPrice=1584001582>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14224624&maxPrice=14225600>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1712001711&maxPrice=1728001726>: HTTP status code is not handled or not allowed
2023-10-29 00:34:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1696001695&maxPrice=1712001710>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1680001679&maxPrice=1696001694>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1664001663&maxPrice=1680001678>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1920001919&maxPrice=1952001950>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2016002015&maxPrice=2048002046>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1984001983&maxPrice=2016002014>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1536001535&maxPrice=1568001566>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1760001759&maxPrice=1792001790>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1728001727&maxPrice=1760001758>: HTTP status code is not handled or not allowed
2023-10-29 00:34:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14242202&maxPrice=14244154>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14248061&maxPrice=14250013>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14246108&maxPrice=14248060>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14240249&maxPrice=14242201>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14238296&maxPrice=14240248>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14203139&maxPrice=14207045>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1472001471&maxPrice=1536001534>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1408001407&maxPrice=1472001470>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1280001279&maxPrice=1344001342>: HTTP status code is not handled or not allowed
2023-10-29 00:34:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4903325&maxPrice=4904300>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4902348&maxPrice=4903324>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14359389&maxPrice=14363295>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14363296&maxPrice=14367201>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14371108&maxPrice=14375013>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14367202&maxPrice=14371107>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4905278&maxPrice=4906253>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4904301&maxPrice=4905277>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14214858&maxPrice=14218763>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14210952&maxPrice=14214857>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14230483&maxPrice=14234388>: HTTP status code is not handled or not allowed
2023-10-29 00:34:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14226577&maxPrice=14230482>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14234389&maxPrice=14238295>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14195327&maxPrice=14203138>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14187514&maxPrice=14195326>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2554690&maxPrice=2555666>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2561526&maxPrice=2562501>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2560549&maxPrice=2561525>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2557620&maxPrice=2558595>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2556643&maxPrice=2557619>: HTTP status code is not handled or not allowed
2023-10-29 00:34:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2578127&maxPrice=2579103>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2583011&maxPrice=2583986>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2582034&maxPrice=2583010>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14343764&maxPrice=14359388>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14390639&maxPrice=14406263>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14375014&maxPrice=14390638>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14328139&maxPrice=14343763>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14312514&maxPrice=14328138>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14421889&maxPrice=14437513>: HTTP status code is not handled or not allowed
2023-10-29 00:35:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14406264&maxPrice=14421888>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1024001023&maxPrice=1280001278>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3240238&maxPrice=3242190>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3246097&maxPrice=3248049>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3238285&maxPrice=3240237>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3236332&maxPrice=3238284>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3234378&maxPrice=3236331>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2558596&maxPrice=2560548>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3716800&maxPrice=3718752>: HTTP status code is not handled or not allowed
2023-10-29 00:35:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3714847&maxPrice=3716799>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4890629&maxPrice=4898441>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4929692&maxPrice=4937503>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4921879&maxPrice=4929691>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14281264&maxPrice=14312513>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14468764&maxPrice=14500013>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14250014&maxPrice=14281263>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14437514&maxPrice=14468763>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4096004095&maxPrice=0>: HTTP status code is not handled or not allowed
2023-10-29 00:35:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2048002047&maxPrice=4096004094>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2121096&maxPrice=2125001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2105471&maxPrice=2109376>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2101565&maxPrice=2105470>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3242191&maxPrice=3246096>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3710941&maxPrice=3714846>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3707035&maxPrice=3710940>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3703128&maxPrice=3707034>: HTTP status code is not handled or not allowed
2023-10-29 00:35:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2589846&maxPrice=2593751>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4609379&maxPrice=4625003>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2585940&maxPrice=2589845>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4593754&maxPrice=4609378>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2117190&maxPrice=2121095>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4875004&maxPrice=4890628>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4906254&maxPrice=4921878>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2054690&maxPrice=2062501>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2046877&maxPrice=2054689>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14125014&maxPrice=14187513>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2109377&maxPrice=2117189>: HTTP status code is not handled or not allowed
2023-10-29 00:35:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2093752&maxPrice=2101564>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3601566&maxPrice=3609377>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3617191&maxPrice=3625002>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3593753&maxPrice=3601565>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3609378&maxPrice=3617190>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2546877&maxPrice=2554689>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3695316&maxPrice=3703127>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3687503&maxPrice=3695315>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4562504&maxPrice=4593753>: HTTP status code is not handled or not allowed
2023-10-29 00:35:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4968754&maxPrice=5000003>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4937504&maxPrice=4968753>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4718754&maxPrice=4750003>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4687504&maxPrice=4718753>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14000013&maxPrice=14125013>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.extensions.logstats] INFO: Crawled 10344 pages (at 632 pages/min), scraped 608 items (at 0 items/min)
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3203128&maxPrice=3218752>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3187503&maxPrice=3203127>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2031252&maxPrice=2046876>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2015627&maxPrice=2031251>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2000001&maxPrice=2015626>: HTTP status code is not handled or not allowed
2023-10-29 00:35:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3218753&maxPrice=3234377>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3734378&maxPrice=3750002>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3718753&maxPrice=3734377>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2531252&maxPrice=2546876>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2562502&maxPrice=2578126>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2609377&maxPrice=2625001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2593752&maxPrice=2609376>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4500004&maxPrice=4562503>: HTTP status code is not handled or not allowed
2023-10-29 00:35:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4625004&maxPrice=4687503>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15750015&maxPrice=16000014>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15500015&maxPrice=15750014>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14750014&maxPrice=15000013>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=14500014&maxPrice=14750013>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2062502&maxPrice=2093751>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3968753&maxPrice=4000002>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3562503&maxPrice=3593752>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3937503&maxPrice=3968752>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2968752&maxPrice=3000001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2937502&maxPrice=2968751>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2906252&maxPrice=2937501>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2875002&maxPrice=2906251>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2500002&maxPrice=2531251>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4750004&maxPrice=4875003>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=18000017&maxPrice=20000018>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=16000015&maxPrice=18000016>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=22000021&maxPrice=24000022>: HTTP status code is not handled or not allowed
2023-10-29 00:35:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=20000019&maxPrice=22000020>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=15000014&maxPrice=15500014>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2187502&maxPrice=2250001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2125002&maxPrice=2187501>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3062503&maxPrice=3125002>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3125003&maxPrice=3187502>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3000002&maxPrice=3062502>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3500003&maxPrice=3562502>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3875003&maxPrice=3937502>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3625003&maxPrice=3687502>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2312502&maxPrice=2375001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2250002&maxPrice=2312501>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2437502&maxPrice=2500001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2687502&maxPrice=2750001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2375002&maxPrice=2437501>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=13000012&maxPrice=14000012>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2625002&maxPrice=2687501>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=12000011&maxPrice=13000011>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=8000007&maxPrice=9000007>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=9000008&maxPrice=10000008>: HTTP status code is not handled or not allowed
2023-10-29 00:35:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=11000010&maxPrice=12000010>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=10000009&maxPrice=11000009>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=6&minPrice=0&maxPrice=2000000>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3375003&maxPrice=3500002>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3750003&maxPrice=3875002>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=3250003&maxPrice=3375002>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5500005&maxPrice=6000004>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2750002&maxPrice=2875001>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=5000004&maxPrice=5500004>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4000003&maxPrice=4500003>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=32000031&maxPrice=64000062>: HTTP status code is not handled or not allowed
2023-10-29 00:35:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 00:35:12 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x237e2c958d0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 199, in close
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-29 00:35:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7286354,
 'downloader/request_count': 10404,
 'downloader/request_method_count/GET': 10404,
 'downloader/response_bytes': 620703027,
 'downloader/response_count': 10404,
 'downloader/response_status_count/200': 10071,
 'downloader/response_status_count/403': 333,
 'elapsed_time_seconds': 1026.412834,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 35, 12, 909867),
 'httpcompression/response_bytes': 2139815821,
 'httpcompression/response_count': 10071,
 'httperror/response_ignored_count': 333,
 'httperror/response_ignored_status_count/403': 333,
 'item_scraped_count': 608,
 'log_count/ERROR': 1,
 'log_count/INFO': 360,
 'log_count/WARNING': 1,
 'request_depth_max': 55,
 'response_received_count': 10404,
 'scheduler/dequeued': 10404,
 'scheduler/dequeued/memory': 10404,
 'scheduler/enqueued': 10404,
 'scheduler/enqueued/memory': 10404,
 'start_time': datetime.datetime(2023, 10, 29, 5, 18, 6, 497033)}
2023-10-29 00:35:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 00:35:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:35:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:35:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:35:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:35:51 [scrapy.extensions.telnet] INFO: Telnet Password: 7d60632e1769dc01
2023-10-29 00:35:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:35:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:35:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:35:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:35:51 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:35:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:35:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:35:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>: HTTP status code is not handled or not allowed
2023-10-29 00:35:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 00:35:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 349,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1174,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 1.287701,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 35, 52, 652398),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 5, 35, 51, 364697)}
2023-10-29 00:35:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 00:36:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:36:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:36:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:36:25 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:36:25 [scrapy.extensions.telnet] INFO: Telnet Password: 3c0ca607109d7341
2023-10-29 00:36:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:36:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:36:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:36:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:36:25 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:36:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:36:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:36:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>: HTTP status code is not handled or not allowed
2023-10-29 00:36:29 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 00:36:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 349,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1174,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 3.590234,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 36, 29, 132978),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 29, 5, 36, 25, 542744)}
2023-10-29 00:36:29 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 00:36:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:36:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:36:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:36:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:36:46 [scrapy.extensions.telnet] INFO: Telnet Password: febaadaf39901367
2023-10-29 00:36:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:36:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:36:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:36:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:36:46 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:36:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:36:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:37:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse> (failed 3 times): DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-29 00:37:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\downloader\middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.realcommercial.com.au.
2023-10-29 00:37:22 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 00:37:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1047,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 36.348962,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 37, 22, 462434),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2023, 10, 29, 5, 36, 46, 113472)}
2023-10-29 00:37:22 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 00:38:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:38:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:38:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:38:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:38:44 [scrapy.extensions.telnet] INFO: Telnet Password: 58c96be075d7c7bc
2023-10-29 00:38:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:38:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:38:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:38:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:38:44 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:38:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:38:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:39:44 [scrapy.extensions.logstats] INFO: Crawled 378 pages (at 378 pages/min), scraped 664 items (at 664 items/min)
2023-10-29 00:40:56 [scrapy.extensions.logstats] INFO: Crawled 529 pages (at 151 pages/min), scraped 1440 items (at 776 items/min)
2023-10-29 00:41:27 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:41:27 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:41:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:41:27 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:41:27 [scrapy.extensions.telnet] INFO: Telnet Password: b0c48d8bab513b9a
2023-10-29 00:41:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:41:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:41:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:41:27 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:41:27 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:41:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:41:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:43:18 [scrapy.extensions.logstats] INFO: Crawled 291 pages (at 291 pages/min), scraped 575 items (at 575 items/min)
2023-10-29 00:44:35 [scrapy.extensions.logstats] INFO: Crawled 300 pages (at 9 pages/min), scraped 586 items (at 11 items/min)
2023-10-29 00:45:27 [scrapy.extensions.logstats] INFO: Crawled 431 pages (at 131 pages/min), scraped 772 items (at 186 items/min)
2023-10-29 00:47:17 [scrapy.extensions.logstats] INFO: Crawled 662 pages (at 231 pages/min), scraped 1609 items (at 837 items/min)
2023-10-29 00:47:54 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:47:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:47:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:47:54 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:47:54 [scrapy.extensions.telnet] INFO: Telnet Password: 9e63cea0914d42e5
2023-10-29 00:47:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:47:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:47:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:47:54 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:47:54 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:47:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:47:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:48:54 [scrapy.extensions.logstats] INFO: Crawled 366 pages (at 366 pages/min), scraped 751 items (at 751 items/min)
2023-10-29 00:49:54 [scrapy.extensions.logstats] INFO: Crawled 801 pages (at 435 pages/min), scraped 1342 items (at 591 items/min)
2023-10-29 00:50:54 [scrapy.extensions.logstats] INFO: Crawled 1242 pages (at 441 pages/min), scraped 2007 items (at 665 items/min)
2023-10-29 00:51:54 [scrapy.extensions.logstats] INFO: Crawled 1699 pages (at 457 pages/min), scraped 2465 items (at 458 items/min)
2023-10-29 00:52:32 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-29 00:52:32 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-29 00:52:32 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 00:52:32 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1818083850&maxPrice=1818087755. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 00:53:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:53:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:53:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:53:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:53:36 [scrapy.extensions.telnet] INFO: Telnet Password: 2801db86b676bb1d
2023-10-29 00:53:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:53:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:53:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:53:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:53:36 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:53:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:53:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:54:36 [scrapy.extensions.logstats] INFO: Crawled 398 pages (at 398 pages/min), scraped 811 items (at 811 items/min)
2023-10-29 00:55:29 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-29 00:55:29 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-29 00:55:30 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 00:55:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=8&minPrice=0&maxPrice=2000000. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 00:55:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4312504&maxPrice=4343753. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 00:55:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=2156252&maxPrice=2187501. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 00:55:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1024001023&maxPrice=1536001534. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 00:55:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1536001535&maxPrice=2048002046. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 00:55:30 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=4343754&maxPrice=4375003. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 00:55:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 00:55:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 00:55:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 00:55:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 00:55:58 [scrapy.extensions.telnet] INFO: Telnet Password: b1a8e1ab6944f3a9
2023-10-29 00:55:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 00:55:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 00:55:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 00:55:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 00:55:58 [scrapy.core.engine] INFO: Spider opened
2023-10-29 00:55:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 00:55:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 00:56:58 [scrapy.extensions.logstats] INFO: Crawled 370 pages (at 370 pages/min), scraped 803 items (at 803 items/min)
2023-10-29 00:57:58 [scrapy.extensions.logstats] INFO: Crawled 760 pages (at 390 pages/min), scraped 1113 items (at 310 items/min)
2023-10-29 00:58:09 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-29 00:58:09 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-29 00:58:11 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x25672ba5d10>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\twisted\internet\task.py", line 526, in _oneWorkUnit
    result = next(self._iterator)
             ^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 197, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-29 00:58:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 558968,
 'downloader/request_count': 848,
 'downloader/request_method_count/GET': 848,
 'downloader/response_bytes': 59863443,
 'downloader/response_count': 848,
 'downloader/response_status_count/200': 848,
 'elapsed_time_seconds': 132.265027,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2023, 10, 29, 5, 58, 11, 199163),
 'httpcompression/response_bytes': 245268972,
 'httpcompression/response_count': 848,
 'item_scraped_count': 1113,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 99,
 'response_received_count': 848,
 'scheduler/dequeued': 848,
 'scheduler/dequeued/memory': 848,
 'scheduler/enqueued': 858,
 'scheduler/enqueued/memory': 858,
 'start_time': datetime.datetime(2023, 10, 29, 5, 55, 58, 934136)}
2023-10-29 00:58:11 [scrapy.core.engine] INFO: Spider closed (shutdown)
2023-10-29 00:58:11 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 01:00:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:00:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:00:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:00:03 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:00:03 [scrapy.extensions.telnet] INFO: Telnet Password: 8215032ad18c94c0
2023-10-29 01:00:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:00:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:00:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:00:03 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:00:03 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:00:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:00:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:00:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=25> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=24)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 143, in parse
    params['maxPrice'] = middle
                         ^^^^^^
UnboundLocalError: cannot access local variable 'middle' where it is not associated with a value
2023-10-29 01:00:47 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 01:00:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 158288,
 'downloader/request_count': 273,
 'downloader/request_method_count/GET': 273,
 'downloader/response_bytes': 18622014,
 'downloader/response_count': 273,
 'downloader/response_status_count/200': 273,
 'elapsed_time_seconds': 43.495674,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 6, 0, 47, 304486),
 'httpcompression/response_bytes': 65823511,
 'httpcompression/response_count': 273,
 'item_scraped_count': 496,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 25,
 'response_received_count': 273,
 'scheduler/dequeued': 273,
 'scheduler/dequeued/memory': 273,
 'scheduler/enqueued': 273,
 'scheduler/enqueued/memory': 273,
 'spider_exceptions/UnboundLocalError': 1,
 'start_time': datetime.datetime(2023, 10, 29, 6, 0, 3, 808812)}
2023-10-29 01:00:47 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 01:03:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:03:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:03:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:03:07 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:03:07 [scrapy.extensions.telnet] INFO: Telnet Password: fb40fd1739b25d20
2023-10-29 01:03:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:03:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:03:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:03:07 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:03:07 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:03:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:03:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:03:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:03:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:03:23 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:03:23 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:03:23 [scrapy.extensions.telnet] INFO: Telnet Password: 24850c76d8652fda
2023-10-29 01:03:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:03:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:03:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:03:23 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:03:23 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:03:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:03:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:04:23 [scrapy.extensions.logstats] INFO: Crawled 307 pages (at 307 pages/min), scraped 775 items (at 775 items/min)
2023-10-29 01:05:23 [scrapy.extensions.logstats] INFO: Crawled 437 pages (at 130 pages/min), scraped 1882 items (at 1107 items/min)
2023-10-29 01:06:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 01:06:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 437743,
 'downloader/request_count': 682,
 'downloader/request_method_count/GET': 682,
 'downloader/response_bytes': 47203032,
 'downloader/response_count': 682,
 'downloader/response_status_count/200': 682,
 'elapsed_time_seconds': 170.949832,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 6, 6, 14, 700183),
 'httpcompression/response_bytes': 185367189,
 'httpcompression/response_count': 682,
 'item_scraped_count': 3372,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 136,
 'response_received_count': 682,
 'scheduler/dequeued': 682,
 'scheduler/dequeued/memory': 682,
 'scheduler/enqueued': 682,
 'scheduler/enqueued/memory': 682,
 'start_time': datetime.datetime(2023, 10, 29, 6, 3, 23, 750351)}
2023-10-29 01:06:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 01:08:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:08:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:08:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:08:07 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:08:07 [scrapy.extensions.telnet] INFO: Telnet Password: cda4e8263a8f93b6
2023-10-29 01:08:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:08:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:08:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:08:07 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:08:07 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:08:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:08:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:08:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 01:08:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2146,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 278176,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'elapsed_time_seconds': 5.455094,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 6, 8, 12, 902028),
 'httpcompression/response_bytes': 1121785,
 'httpcompression/response_count': 4,
 'item_scraped_count': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 10, 29, 6, 8, 7, 446934)}
2023-10-29 01:08:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 01:08:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:08:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:08:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:08:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:08:51 [scrapy.extensions.telnet] INFO: Telnet Password: 884c59222dabff00
2023-10-29 01:08:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:08:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:08:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:08:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:08:51 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:08:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:08:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:10:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2023-10-29 01:10:47 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 01:10:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2146,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 278168,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'elapsed_time_seconds': 116.617154,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 6, 10, 47, 755408),
 'httpcompression/response_bytes': 1121784,
 'httpcompression/response_count': 4,
 'item_scraped_count': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 10, 29, 6, 8, 51, 138254)}
2023-10-29 01:10:47 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 01:11:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:11:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:11:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:11:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:11:47 [scrapy.extensions.telnet] INFO: Telnet Password: 6dd3d3ac37fe4827
2023-10-29 01:11:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:11:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:11:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:11:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:11:47 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:11:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:11:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:11:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-1-pelican-point-road-outer-harbor-sa-5018-504236544> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=5018&propertyTypes=industrial-warehouse)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 185, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-29 01:11:55 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 01:11:55 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1daf2a61490>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 192, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-29 01:11:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2655,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 346014,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 7.989176,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 6, 11, 55, 407011),
 'httpcompression/response_bytes': 1359736,
 'httpcompression/response_count': 5,
 'item_scraped_count': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 29, 6, 11, 47, 417835)}
2023-10-29 01:11:55 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 01:12:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:12:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:12:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:12:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:12:50 [scrapy.extensions.telnet] INFO: Telnet Password: f3de09c3150c7a55
2023-10-29 01:12:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:12:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:12:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:12:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:12:50 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:12:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:12:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:13:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:13:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:13:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:13:26 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:13:26 [scrapy.extensions.telnet] INFO: Telnet Password: 453a02a25e00cf12
2023-10-29 01:13:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:13:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:13:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:13:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:13:26 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:13:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:13:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:14:28 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 4 pages/min), scraped 3 items (at 3 items/min)
2023-10-29 01:14:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 01:14:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2695,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 346090,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 85.733072,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 6, 14, 52, 142306),
 'httpcompression/response_bytes': 1359830,
 'httpcompression/response_count': 5,
 'item_scraped_count': 4,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'start_time': datetime.datetime(2023, 10, 29, 6, 13, 26, 409234)}
2023-10-29 01:14:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 01:15:08 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:15:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:15:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:15:08 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:15:08 [scrapy.extensions.telnet] INFO: Telnet Password: ff05cdbdf1e07905
2023-10-29 01:15:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:15:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:15:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:15:08 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:15:08 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:15:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:15:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:15:28 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 01:15:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2146,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 277767,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'elapsed_time_seconds': 19.30391,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 6, 15, 28, 53246),
 'httpcompression/response_bytes': 1121749,
 'httpcompression/response_count': 4,
 'item_scraped_count': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 10, 29, 6, 15, 8, 749336)}
2023-10-29 01:15:28 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 01:52:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:52:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:52:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:52:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:52:22 [scrapy.extensions.telnet] INFO: Telnet Password: 4bfb095c14c6cd4a
2023-10-29 01:52:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:52:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:52:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:52:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:52:22 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:52:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:52:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:53:22 [scrapy.extensions.logstats] INFO: Crawled 189 pages (at 189 pages/min), scraped 349 items (at 349 items/min)
2023-10-29 01:54:16 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-29 01:54:16 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-29 01:54:18 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 01:54:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:54:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:54:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:54:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:54:40 [scrapy.extensions.telnet] INFO: Telnet Password: e247db97f659e4d7
2023-10-29 01:54:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:54:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:54:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:54:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:54:40 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:54:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:54:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 01:55:40 [scrapy.extensions.logstats] INFO: Crawled 277 pages (at 277 pages/min), scraped 536 items (at 536 items/min)
2023-10-29 01:56:38 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-29 01:56:38 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-29 01:56:39 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 01:56:39 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1386715&maxPrice=1387690. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 01:56:39 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1492185&maxPrice=1493159. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 01:59:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 01:59:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 01:59:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 01:59:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 01:59:35 [scrapy.extensions.telnet] INFO: Telnet Password: 954242626cc13800
2023-10-29 01:59:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 01:59:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 01:59:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 01:59:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 01:59:35 [scrapy.core.engine] INFO: Spider opened
2023-10-29 01:59:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 01:59:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 02:00:35 [scrapy.extensions.logstats] INFO: Crawled 285 pages (at 285 pages/min), scraped 616 items (at 616 items/min)
2023-10-29 02:01:35 [scrapy.extensions.logstats] INFO: Crawled 598 pages (at 313 pages/min), scraped 2349 items (at 1733 items/min)
2023-10-29 02:02:35 [scrapy.extensions.logstats] INFO: Crawled 1042 pages (at 444 pages/min), scraped 5031 items (at 2682 items/min)
2023-10-29 02:03:35 [scrapy.extensions.logstats] INFO: Crawled 1470 pages (at 428 pages/min), scraped 8686 items (at 3655 items/min)
2023-10-29 02:04:15 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-29 02:04:15 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-29 02:04:15 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 02:04:15 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=562498&maxPrice=563472. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 02:04:15 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=2&minPrice=437498&maxPrice=499998. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 02:04:15 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=excludesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=724605&maxPrice=726558. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 02:16:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 02:16:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 02:16:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 02:16:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 02:16:50 [scrapy.extensions.telnet] INFO: Telnet Password: f2e48eba91607f0b
2023-10-29 02:16:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 02:16:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 02:16:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 02:16:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 02:16:50 [scrapy.core.engine] INFO: Spider opened
2023-10-29 02:16:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 02:16:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 02:17:50 [scrapy.extensions.logstats] INFO: Crawled 284 pages (at 284 pages/min), scraped 614 items (at 614 items/min)
2023-10-29 02:18:50 [scrapy.extensions.logstats] INFO: Crawled 521 pages (at 237 pages/min), scraped 2104 items (at 1490 items/min)
2023-10-29 02:19:50 [scrapy.extensions.logstats] INFO: Crawled 899 pages (at 378 pages/min), scraped 4986 items (at 2882 items/min)
2023-10-29 02:20:50 [scrapy.extensions.logstats] INFO: Crawled 1290 pages (at 391 pages/min), scraped 7535 items (at 2549 items/min)
2023-10-29 02:21:50 [scrapy.extensions.logstats] INFO: Crawled 1688 pages (at 398 pages/min), scraped 10606 items (at 3071 items/min)
2023-10-29 02:22:50 [scrapy.extensions.logstats] INFO: Crawled 2133 pages (at 445 pages/min), scraped 12502 items (at 1896 items/min)
2023-10-29 02:23:37 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 02:23:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1653836,
 'downloader/request_count': 2453,
 'downloader/request_method_count/GET': 2453,
 'downloader/response_bytes': 168312795,
 'downloader/response_count': 2453,
 'downloader/response_status_count/200': 2453,
 'elapsed_time_seconds': 406.933558,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 7, 23, 37, 502477),
 'httpcompression/response_bytes': 675355417,
 'httpcompression/response_count': 2453,
 'item_scraped_count': 13488,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'request_depth_max': 164,
 'response_received_count': 2453,
 'scheduler/dequeued': 2453,
 'scheduler/dequeued/memory': 2453,
 'scheduler/enqueued': 2453,
 'scheduler/enqueued/memory': 2453,
 'start_time': datetime.datetime(2023, 10, 29, 7, 16, 50, 568919)}
2023-10-29 02:23:37 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 02:28:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 02:28:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 02:28:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 02:28:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 02:28:02 [scrapy.extensions.telnet] INFO: Telnet Password: 57a4dbe5b72c1527
2023-10-29 02:28:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 02:28:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 02:28:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 02:28:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 02:28:02 [scrapy.core.engine] INFO: Spider opened
2023-10-29 02:28:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 02:28:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 02:29:02 [scrapy.extensions.logstats] INFO: Crawled 283 pages (at 283 pages/min), scraped 604 items (at 604 items/min)
2023-10-29 02:29:49 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-29 02:29:49 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-29 02:29:49 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-29 02:29:49 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=1171871&maxPrice=1187496. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-29 02:30:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 02:30:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 02:30:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 02:30:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 02:30:22 [scrapy.extensions.telnet] INFO: Telnet Password: 88178d3fc70d2fa8
2023-10-29 02:30:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 02:30:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 02:30:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 02:30:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 02:30:22 [scrapy.core.engine] INFO: Spider opened
2023-10-29 02:30:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 02:30:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 02:31:22 [scrapy.extensions.logstats] INFO: Crawled 285 pages (at 285 pages/min), scraped 624 items (at 624 items/min)
2023-10-29 02:32:22 [scrapy.extensions.logstats] INFO: Crawled 525 pages (at 240 pages/min), scraped 2140 items (at 1516 items/min)
2023-10-29 02:33:22 [scrapy.extensions.logstats] INFO: Crawled 908 pages (at 383 pages/min), scraped 4644 items (at 2504 items/min)
2023-10-29 02:34:22 [scrapy.extensions.logstats] INFO: Crawled 1324 pages (at 416 pages/min), scraped 6930 items (at 2286 items/min)
2023-10-29 02:35:22 [scrapy.extensions.logstats] INFO: Crawled 1694 pages (at 370 pages/min), scraped 9087 items (at 2157 items/min)
2023-10-29 02:35:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 02:35:52 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x24a928a6310>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 197, in close
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-29 02:35:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1280350,
 'downloader/request_count': 1906,
 'downloader/request_method_count/GET': 1906,
 'downloader/response_bytes': 133316934,
 'downloader/response_count': 1906,
 'downloader/response_status_count/200': 1906,
 'elapsed_time_seconds': 329.860594,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 7, 35, 52, 673159),
 'httpcompression/response_bytes': 547959197,
 'httpcompression/response_count': 1906,
 'item_scraped_count': 9771,
 'log_count/ERROR': 1,
 'log_count/INFO': 15,
 'log_count/WARNING': 1,
 'request_depth_max': 160,
 'response_received_count': 1906,
 'scheduler/dequeued': 1906,
 'scheduler/dequeued/memory': 1906,
 'scheduler/enqueued': 1906,
 'scheduler/enqueued/memory': 1906,
 'start_time': datetime.datetime(2023, 10, 29, 7, 30, 22, 812565)}
2023-10-29 02:35:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 12:41:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 12:41:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 12:41:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 12:41:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 12:41:31 [scrapy.extensions.telnet] INFO: Telnet Password: 273886f9632f4785
2023-10-29 12:41:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 12:41:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 12:41:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 12:41:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 12:41:31 [scrapy.core.engine] INFO: Spider opened
2023-10-29 12:41:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 12:41:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 12:42:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 12:42:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 12:42:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 12:42:03 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 12:42:03 [scrapy.extensions.telnet] INFO: Telnet Password: 4a853d3cc6d3cf22
2023-10-29 12:42:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 12:42:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 12:42:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 12:42:03 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 12:42:03 [scrapy.core.engine] INFO: Spider opened
2023-10-29 12:42:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 12:42:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 12:43:03 [scrapy.extensions.logstats] INFO: Crawled 323 pages (at 323 pages/min), scraped 917 items (at 917 items/min)
2023-10-29 12:44:03 [scrapy.extensions.logstats] INFO: Crawled 820 pages (at 497 pages/min), scraped 4422 items (at 3505 items/min)
2023-10-29 12:45:03 [scrapy.extensions.logstats] INFO: Crawled 1346 pages (at 526 pages/min), scraped 7997 items (at 3575 items/min)
2023-10-29 12:46:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 12:46:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1280350,
 'downloader/request_count': 1906,
 'downloader/request_method_count/GET': 1906,
 'downloader/response_bytes': 132847768,
 'downloader/response_count': 1906,
 'downloader/response_status_count/200': 1906,
 'elapsed_time_seconds': 238.598223,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 17, 46, 1, 750263),
 'httpcompression/response_bytes': 547964134,
 'httpcompression/response_count': 1906,
 'item_scraped_count': 11870,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 160,
 'response_received_count': 1906,
 'scheduler/dequeued': 1906,
 'scheduler/dequeued/memory': 1906,
 'scheduler/enqueued': 1906,
 'scheduler/enqueued/memory': 1906,
 'start_time': datetime.datetime(2023, 10, 29, 17, 42, 3, 152040)}
2023-10-29 12:46:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-29 12:52:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-29 12:52:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-29 12:52:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-29 12:52:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-29 12:52:16 [scrapy.extensions.telnet] INFO: Telnet Password: b255fcb9e78c96a2
2023-10-29 12:52:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-29 12:52:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-29 12:52:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-29 12:52:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-29 12:52:16 [scrapy.core.engine] INFO: Spider opened
2023-10-29 12:52:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-29 12:52:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-29 12:53:16 [scrapy.extensions.logstats] INFO: Crawled 333 pages (at 333 pages/min), scraped 972 items (at 972 items/min)
2023-10-29 12:54:16 [scrapy.extensions.logstats] INFO: Crawled 827 pages (at 494 pages/min), scraped 4429 items (at 3457 items/min)
2023-10-29 12:55:16 [scrapy.extensions.logstats] INFO: Crawled 1305 pages (at 478 pages/min), scraped 7648 items (at 3219 items/min)
2023-10-29 12:56:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-29 12:56:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1280350,
 'downloader/request_count': 1906,
 'downloader/request_method_count/GET': 1906,
 'downloader/response_bytes': 132839317,
 'downloader/response_count': 1906,
 'downloader/response_status_count/200': 1906,
 'elapsed_time_seconds': 237.756709,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 29, 17, 56, 14, 473868),
 'httpcompression/response_bytes': 547961170,
 'httpcompression/response_count': 1906,
 'item_scraped_count': 11870,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 160,
 'response_received_count': 1906,
 'scheduler/dequeued': 1906,
 'scheduler/dequeued/memory': 1906,
 'scheduler/enqueued': 1906,
 'scheduler/enqueued/memory': 1906,
 'start_time': datetime.datetime(2023, 10, 29, 17, 52, 16, 717159)}
2023-10-29 12:56:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-30 10:28:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:28:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:28:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:28:03 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:28:03 [scrapy.extensions.telnet] INFO: Telnet Password: f437697e16f5fbad
2023-10-30 10:28:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:28:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:28:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:28:03 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:28:03 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:28:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:28:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:30:38 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:30:38 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:30:38 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:30:38 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:30:38 [scrapy.extensions.telnet] INFO: Telnet Password: 6808f646b872b092
2023-10-30 10:30:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:30:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:30:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:30:38 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:30:38 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:30:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:30:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:32:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:32:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:32:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:32:59 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:32:59 [scrapy.extensions.telnet] INFO: Telnet Password: 23c0e6e40f42e460
2023-10-30 10:32:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:32:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:32:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:32:59 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:32:59 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:32:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:32:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:33:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/smartstores@metroplex-133>: HTTP status code is not handled or not allowed
2023-10-30 10:33:54 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:33:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:33:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:33:54 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:33:54 [scrapy.extensions.telnet] INFO: Telnet Password: 385771543e4e8e96
2023-10-30 10:33:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:33:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:33:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:33:54 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:33:54 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:33:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:33:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:35:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:35:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:35:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:35:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:35:02 [scrapy.extensions.telnet] INFO: Telnet Password: fed8db86f787f8a8
2023-10-30 10:35:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:35:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:35:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:35:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:35:02 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:35:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:35:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:35:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/smartstores@metroplex-133>: HTTP status code is not handled or not allowed
2023-10-30 10:36:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:36:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:36:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:36:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:36:48 [scrapy.extensions.telnet] INFO: Telnet Password: ce3a28cd1e6d6539
2023-10-30 10:36:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:36:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:36:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:36:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:36:48 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:36:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:36:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:37:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:37:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:37:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:37:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:37:56 [scrapy.extensions.telnet] INFO: Telnet Password: d6933d3e5503b0ac
2023-10-30 10:37:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:37:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:37:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:37:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:37:56 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:37:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:37:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:39:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:39:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:39:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:39:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:39:36 [scrapy.extensions.telnet] INFO: Telnet Password: b1c2141b7ceeaffc
2023-10-30 10:39:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:39:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:39:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:39:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:39:36 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:39:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:39:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:41:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:41:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:41:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:41:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:41:10 [scrapy.extensions.telnet] INFO: Telnet Password: 2333255cc29bc3ad
2023-10-30 10:41:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:41:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:41:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:41:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:41:10 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:41:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:41:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:41:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:41:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:41:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:41:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:41:20 [scrapy.extensions.telnet] INFO: Telnet Password: b189dd3a1cc1f1e1
2023-10-30 10:41:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:41:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:41:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:41:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:41:20 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:41:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:41:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:42:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:42:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:42:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:42:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:42:56 [scrapy.extensions.telnet] INFO: Telnet Password: 30e1baf69cdb8345
2023-10-30 10:42:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:42:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:42:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:42:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:42:56 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:42:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:42:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:43:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/smartstores@metroplex-133>: HTTP status code is not handled or not allowed
2023-10-30 10:44:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:44:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:44:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:44:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:44:22 [scrapy.extensions.telnet] INFO: Telnet Password: 31e886bcd8d75e6b
2023-10-30 10:44:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:44:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:44:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:44:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:44:22 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:44:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:44:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:47:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:47:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:47:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:47:18 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:47:18 [scrapy.extensions.telnet] INFO: Telnet Password: d52e0036aa61274d
2023-10-30 10:47:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:47:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:47:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:47:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:47:18 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:47:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:47:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:51:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-30 10:51:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-30 10:51:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-30 10:51:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-30 10:51:37 [scrapy.extensions.telnet] INFO: Telnet Password: 3cb9ae7c8444be34
2023-10-30 10:51:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-30 10:51:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-30 10:51:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-30 10:51:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-30 10:51:37 [scrapy.core.engine] INFO: Spider opened
2023-10-30 10:51:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-30 10:51:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-30 10:51:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/smartstores@metroplex-133>: HTTP status code is not handled or not allowed
