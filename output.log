2023-10-22 17:58:11 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:58:11 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:11 [scrapy.extensions.telnet] INFO: Telnet Password: 9b5c7671d9d6f1cb
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:11 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:13 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 17:58:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17269,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3934665,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.257016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 0, 58, 23, 124289),
 'httpcompression/response_bytes': 18085934,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 50,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/AttributeError': 50,
 'start_time': datetime.datetime(2023, 10, 23, 0, 58, 11, 867273)}
2023-10-22 17:58:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 17:58:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:58:25 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:25 [scrapy.extensions.telnet] INFO: Telnet Password: eeeb5dc40f7e5543
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:25 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:26 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 17:58:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17249,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3932517,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 10.194672,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 0, 58, 36, 65623),
 'httpcompression/response_bytes': 18083233,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 50,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/AttributeError': 50,
 'start_time': datetime.datetime(2023, 10, 23, 0, 58, 25, 870951)}
2023-10-22 17:58:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 17:58:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:58:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:58:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': '11'}
2023-10-22 17:58:53 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:58:53 [scrapy.extensions.telnet] INFO: Telnet Password: 16111dc3457c0eb2
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:58:53 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:58:53 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:58:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:58:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 17:58:55 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 17:58:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:58:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Hudong\Desktop\scrapy-pyqt5\scraps\scraps\spiders\book.py", line 23, in parse
    items['review'] = review.split(' ')[-1]
                      ^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-10-22 17:59:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 17:59:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 17:59:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 17:59:20 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 17:59:20 [scrapy.extensions.telnet] INFO: Telnet Password: 54552a52c6ff65e7
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 17:59:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 17:59:20 [scrapy.core.engine] INFO: Spider opened
2023-10-22 17:59:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 17:59:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:01:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:01:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:01:18 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:01:18 [scrapy.extensions.telnet] INFO: Telnet Password: 6c9f34180780b806
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:01:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:01:18 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:01:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:01:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:21 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:01:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:01:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:01:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': 'ff'}
2023-10-22 18:01:26 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:01:26 [scrapy.extensions.telnet] INFO: Telnet Password: 7728fde1c686152e
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:01:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:01:26 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:01:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:01:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:01:27 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:04:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:04:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:04:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:04:02 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:04:02 [scrapy.extensions.telnet] INFO: Telnet Password: 7ac2cb01b2926e75
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:04:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:04:02 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:04:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:04:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:04:04 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:04:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:04:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17092,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3937127,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.118511,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 4, 15, 626704),
 'httpcompression/response_bytes': 18096335,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 4, 2, 508193)}
2023-10-22 18:04:15 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 18:33:49 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:33:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:33:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:33:49 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:33:49 [scrapy.extensions.telnet] INFO: Telnet Password: d82c551d182cbe3d
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:33:49 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:33:49 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:33:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:33:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:33:50 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:34:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:34:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17108,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3952542,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 12.631892,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 34, 1, 991556),
 'httpcompression/response_bytes': 18116370,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 33, 49, 359664)}
2023-10-22 18:34:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-22 18:34:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-22 18:34:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-22 18:34:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders'],
 'USER_AGENT': ''}
2023-10-22 18:34:09 [py.warnings] WARNING: C:\ProgramData\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-22 18:34:09 [scrapy.extensions.telnet] INFO: Telnet Password: 986d7f3d4ced0cd0
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-22 18:34:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-22 18:34:09 [scrapy.core.engine] INFO: Spider opened
2023-10-22 18:34:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-22 18:34:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-22 18:34:10 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\ProgramData\anaconda3\Lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache'
2023-10-22 18:34:20 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-22 18:34:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17224,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3951674,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.07249,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 1, 34, 20, 541469),
 'httpcompression/response_bytes': 18116581,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 1, 34, 9, 468979)}
2023-10-22 18:34:20 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-23 23:27:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:27:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:27:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:27:25 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:27:25 [scrapy.extensions.telnet] INFO: Telnet Password: a03e9462d9653945
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:27:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:27:25 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:27:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:27:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:28:33 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:28:33 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:28:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:28:33 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:28:33 [scrapy.extensions.telnet] INFO: Telnet Password: d42157d56822e353
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:28:33 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:28:33 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:28:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:28:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:29:33 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 30 pages/min), scraped 290 items (at 290 items/min)
2023-10-23 23:29:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-23 23:29:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20595,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3938374,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 80.059072,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 23, 29, 53, 184323),
 'httpcompression/response_bytes': 18286442,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 23, 23, 28, 33, 125251)}
2023-10-23 23:29:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-23 23:33:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-23 23:33:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-23 23:33:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-23 23:33:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-23 23:33:09 [scrapy.extensions.telnet] INFO: Telnet Password: 1a3d5ce2b3f45252
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-23 23:33:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-23 23:33:09 [scrapy.core.engine] INFO: Spider opened
2023-10-23 23:33:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-23 23:33:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-23 23:33:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-23 23:33:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79366,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 3.662685,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 23, 23, 33, 13, 364840),
 'httpcompression/response_bytes': 354915,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 23, 23, 33, 9, 702155)}
2023-10-23 23:33:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 01:22:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:22:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:22:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:22:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:22:37 [scrapy.extensions.telnet] INFO: Telnet Password: 97f19cafc5a34e66
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:22:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:22:37 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:22:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:22:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:42:06 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 01:48:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:48:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:48:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:48:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:48:13 [scrapy.extensions.telnet] INFO: Telnet Password: 0ef9d5e3c3c939d1
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:48:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:48:13 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:48:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:48:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:48:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:48:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:48:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:13 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:49:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:49:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:13 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:50:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:50:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:50:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:50:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:50:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:50:39 [scrapy.extensions.telnet] INFO: Telnet Password: 4c3e4da91c3e6bfb
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:50:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:50:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:50:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:50:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:51:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:51:39 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:51:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies']['name']
                              ~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
                              ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-10-24 01:53:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:53:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:53:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:53:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:53:02 [scrapy.extensions.telnet] INFO: Telnet Password: 1efd2404680fa838
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:53:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:53:02 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:53:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:53:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:53:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:53:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:53:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:53:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:53:50 [scrapy.extensions.telnet] INFO: Telnet Password: 7b2199a09209ec1d
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:53:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:53:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:53:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:53:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:55:04 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:55:04 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:55:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:55:04 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:55:04 [scrapy.extensions.telnet] INFO: Telnet Password: 6fc0fb1af9d51637
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:55:04 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:55:04 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:55:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:55:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:56:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:56:09 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['agency_company'] = item['agencies'][0]['name']
    ~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\item.py", line 85, in __setitem__
    raise KeyError(f"{self.__class__.__name__} does not support field: {key}")
KeyError: 'scrapsItem does not support field: agency_company'
2023-10-24 01:57:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:57:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:57:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:57:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:57:39 [scrapy.extensions.telnet] INFO: Telnet Password: 6ea2c6cfb41aff55
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:57:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:57:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:57:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:57:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 01:58:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 01:58:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 01:58:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 01:58:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 01:58:56 [scrapy.extensions.telnet] INFO: Telnet Password: 589c824d0a5c582f
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 01:58:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 01:58:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 01:58:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 01:58:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:02:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:02:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:02:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:02:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:02:06 [scrapy.extensions.telnet] INFO: Telnet Password: d4a775acd308502d
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:02:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:02:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:02:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:02:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:02:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:02:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:02:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:02:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:02:35 [scrapy.extensions.telnet] INFO: Telnet Password: a1439efaf433ec0a
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:02:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:02:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:02:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:02:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:03:06 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:35 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:43 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:03:43 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:03:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:03:43 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:03:43 [scrapy.extensions.telnet] INFO: Telnet Password: 3b96fa9767c6704d
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:03:43 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:03:43 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:03:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:03:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:03:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:03:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:06 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:04:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:04:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:04:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:04:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:04:10 [scrapy.extensions.telnet] INFO: Telnet Password: 4d18c6b9a212e161
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:04:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:04:10 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:04:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:04:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 02:04:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:04:56 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x28373c8c7d0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 55, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:04:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 17,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 12,
 'downloader/request_bytes': 26083,
 'downloader/request_count': 68,
 'downloader/request_method_count/GET': 68,
 'downloader/response_bytes': 3936049,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 169.65009,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 4, 56, 252219),
 'httpcompression/response_bytes': 17994885,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 51,
 'log_count/INFO': 12,
 'log_count/WARNING': 6,
 'response_received_count': 51,
 'retry/count': 17,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 5,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 67,
 'scheduler/dequeued/memory': 67,
 'scheduler/enqueued': 67,
 'scheduler/enqueued/memory': 67,
 'spider_exceptions/FileNotFoundError': 50,
 'start_time': datetime.datetime(2023, 10, 24, 2, 2, 6, 602129)}
2023-10-24 02:04:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:05:15 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 02:05:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:05:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 7,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 7,
 'downloader/request_bytes': 22059,
 'downloader/request_count': 58,
 'downloader/request_method_count/GET': 58,
 'downloader/response_bytes': 3940135,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 73.337621,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 5, 23, 909371),
 'httpcompression/response_bytes': 18014876,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 48,
 'request_depth_max': 1,
 'response_received_count': 51,
 'retry/count': 7,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 7,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 57,
 'scheduler/dequeued/memory': 57,
 'scheduler/enqueued': 57,
 'scheduler/enqueued/memory': 57,
 'start_time': datetime.datetime(2023, 10, 24, 2, 4, 10, 571750)}
2023-10-24 02:05:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:50:15 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:15 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:15 [scrapy.extensions.telnet] INFO: Telnet Password: 2b55b3a10beac79e
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:15 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:15 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:50:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=150000&minPrice=35000&maxFloorArea=2000&minFloorArea=250&maxSiteArea=10000&minSiteArea=200&numParkingSpaces=1&energyEfficiency=1&keywords=factory%2Bshop&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:50:28 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:50:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34957,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2840838,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.434935,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 50, 28, 664032),
 'httpcompression/response_bytes': 9351115,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 50, 15, 229097)}
2023-10-24 02:50:28 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:50:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:48 [scrapy.extensions.telnet] INFO: Telnet Password: 8d681800b23e4a72
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:48 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:50:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:50:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:50:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:50:57 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:50:57 [scrapy.extensions.telnet] INFO: Telnet Password: 9c50c2a7dbef01af
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:50:57 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:50:57 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:50:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:50:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:51:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=150000&minPrice=35000&maxFloorArea=2000&minFloorArea=250&maxSiteArea=10000&minSiteArea=200&numParkingSpaces=1&energyEfficiency=1&keywords=factory&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:51:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:51:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34603,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2839029,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.885398,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 51, 11, 126514),
 'httpcompression/response_bytes': 9338333,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 50, 57, 241116)}
2023-10-24 02:51:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:51:19 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:51:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:51:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:51:19 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:51:19 [scrapy.extensions.telnet] INFO: Telnet Password: 81e53762eadb2501
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:51:19 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:51:19 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:51:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:51:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:51:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&propertyTypes=land-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&tenure=vacant&maxPrice=1000000&numParkingSpaces=1&energyEfficiency=1&keywords=factory&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:51:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:51:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 30456,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2839072,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 16.002299,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 51, 35, 645678),
 'httpcompression/response_bytes': 9338119,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 2, 51, 19, 643379)}
2023-10-24 02:51:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:51:58 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:51:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:51:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:51:58 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:51:58 [scrapy.extensions.telnet] INFO: Telnet Password: 8bffe712f76b64c3
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:51:58 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:51:58 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:51:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:51:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting&numParkingSpaces=1&energyEfficiency=1&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:52:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:52:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 29428,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 2998434,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 14.283853,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 52, 12, 943884),
 'httpcompression/response_bytes': 10854209,
 'httpcompression/response_count': 50,
 'item_scraped_count': 16,
 'log_count/ERROR': 5,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 5,
 'start_time': datetime.datetime(2023, 10, 24, 2, 51, 58, 660031)}
2023-10-24 02:52:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:52:26 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:52:26 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:52:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:52:26 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:52:26 [scrapy.extensions.telnet] INFO: Telnet Password: 688f2bdc53f49ba6
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:52:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:52:26 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:52:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:52:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:52:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 507, in Client
    answer_challenge(c, authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 751, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 215, in recv_bytes
    buf = self._recv_bytes(maxlength)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=3> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=5> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=9> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=4> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=7> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=8> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=2> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=6> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=11> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=10> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Ccommercial-farming%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods%2Cmedical-consulting%2Cother&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:52:40 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x16df173d290>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 814, in _callmethod
    conn = self._tls.connection
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ForkAwareLocal' object has no attribute 'connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 55, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 818, in _callmethod
    self._connect()
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 805, in _connect
    conn = self._Client(self._token.address, authkey=self._authkey)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 499, in Client
    c = PipeClient(address)
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 701, in PipeClient
    _winapi.WaitNamedPipe(address, 1000)
FileNotFoundError: [WinError 2] The system cannot find the file specified
2023-10-24 02:52:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 27995,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3949410,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.87832,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 52, 40, 684184),
 'httpcompression/response_bytes': 18136370,
 'httpcompression/response_count': 50,
 'log_count/ERROR': 51,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/EOFError': 1,
 'spider_exceptions/FileNotFoundError': 49,
 'start_time': datetime.datetime(2023, 10, 24, 2, 52, 26, 805864)}
2023-10-24 02:52:40 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 02:53:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 02:53:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 02:53:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 02:53:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 02:53:47 [scrapy.extensions.telnet] INFO: Telnet Password: c41d9c7923c81ce4
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 02:53:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 02:53:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 02:53:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 02:53:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&propertyTypes=retail%2Cland-development%2Chotel-motel-leisure%2Cindustrial-warehouse%2Coffices%2Cshowrooms-bulky-goods&maxPrice=5000000&minPrice=100000&minFloorArea=50&minSiteArea=200&numParkingSpaces=3&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 02:54:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 02:54:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 29698,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3913133,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 13.353105,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 2, 54, 0, 794794),
 'httpcompression/response_bytes': 18623381,
 'httpcompression/response_count': 50,
 'item_scraped_count': 192,
 'log_count/ERROR': 31,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 19,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 31,
 'start_time': datetime.datetime(2023, 10, 24, 2, 53, 47, 441689)}
2023-10-24 02:54:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 03:16:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:16:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:16:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:16:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:16:40 [scrapy.extensions.telnet] INFO: Telnet Password: 6ec5e4c8f5c6e849
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:16:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:16:40 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:16:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:16:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:16:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=44> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=49> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=48> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&maxPrice=35000&minPrice=30000&page=50> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 03:16:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 03:16:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 21267,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3817741,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 12.43017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 3, 16, 52, 730155),
 'httpcompression/response_bytes': 17407705,
 'httpcompression/response_count': 50,
 'item_scraped_count': 356,
 'log_count/ERROR': 15,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 35,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 15,
 'start_time': datetime.datetime(2023, 10, 24, 3, 16, 40, 299985)}
2023-10-24 03:16:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 03:19:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:19:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:19:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:19:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:19:34 [scrapy.extensions.telnet] INFO: Telnet Password: 075ada4189015eec
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:19:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:19:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:19:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:19:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:19:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 03:19:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 03:19:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 03:19:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 03:19:47 [scrapy.extensions.telnet] INFO: Telnet Password: fff2f445b24a8940
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 03:19:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 03:19:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 03:19:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 03:19:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 03:19:59 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 03:19:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 19716,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3944105,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 11.488783,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 3, 19, 59, 445498),
 'httpcompression/response_bytes': 18024360,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 48,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 3, 19, 47, 956715)}
2023-10-24 03:19:59 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:52:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:52:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:52:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:52:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:52:30 [scrapy.extensions.telnet] INFO: Telnet Password: 07e15f4c6e558b22
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:52:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:52:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:52:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:52:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:52:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:52:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:52:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20517,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3602287,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 26.730949,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 52, 56, 915232),
 'httpcompression/response_bytes': 15407464,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 52, 30, 184283)}
2023-10-24 05:52:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:53:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:53:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:53:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:53:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:53:45 [scrapy.extensions.telnet] INFO: Telnet Password: e9f764025428e6db
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:53:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:53:45 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:53:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:53:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:53:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:53:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:54:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20516,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3601139,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 24.905852,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 54, 10, 843020),
 'httpcompression/response_bytes': 15407408,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 53, 45, 937168)}
2023-10-24 05:54:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:54:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:54:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:54:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:54:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:54:14 [scrapy.extensions.telnet] INFO: Telnet Password: 5c09fd50ca0d8a73
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:54:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:54:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:54:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:54:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:54:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:38 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:54:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20516,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3601170,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 24.419378,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 54, 38, 876401),
 'httpcompression/response_bytes': 15407409,
 'httpcompression/response_count': 50,
 'item_scraped_count': 112,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 11,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 29,
 'start_time': datetime.datetime(2023, 10, 24, 5, 54, 14, 457023)}
2023-10-24 05:54:38 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:54:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:54:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:54:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:54:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:54:44 [scrapy.extensions.telnet] INFO: Telnet Password: d895aa8a08e81314
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:54:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:54:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:54:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:54:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=12> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=13> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=15> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=14> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=37> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:55:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:55:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:55:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:55:14 [scrapy.extensions.telnet] INFO: Telnet Password: cb5730cc7101ebc6
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:55:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:55:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:55:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:55:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:55:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=16> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=19> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=21> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=18> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=17> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=24> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=22> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=25> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=20> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=29> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=27> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=30> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=26> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=23> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=31> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=36> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=28> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=33> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=32> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=35> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=34> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=38> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=39> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=43> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=40> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=45> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=41> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=47> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=42> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=46> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:55:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:55:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:55:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:55:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:55:36 [scrapy.extensions.telnet] INFO: Telnet Password: e6b2804945694fb2
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:55:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:55:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:55:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:55:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:55:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5013&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 38, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 05:56:04 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 05:56:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20463,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3480007,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'elapsed_time_seconds': 28.196052,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 56, 4, 626540),
 'httpcompression/response_bytes': 14510005,
 'httpcompression/response_count': 50,
 'item_scraped_count': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 30,
 'request_depth_max': 1,
 'response_received_count': 51,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 5, 55, 36, 430488)}
2023-10-24 05:56:04 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 05:56:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:56:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:56:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:56:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:56:56 [scrapy.extensions.telnet] INFO: Telnet Password: 30cffb5742593986
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:56:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:56:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:56:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:56:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:59:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 05:59:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 05:59:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 05:59:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 05:59:06 [scrapy.extensions.telnet] INFO: Telnet Password: 6bf3ee038599a49d
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 05:59:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 05:59:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 05:59:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 05:59:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 05:59:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:00:59 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scraps' at 0x15146cf0b10>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 56, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 06:00:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79799,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 25.333023,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 5, 59, 32, 30419),
 'httpcompression/response_bytes': 351686,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 5, 59, 6, 697396)}
2023-10-24 06:00:59 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:01:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:01:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:01:23 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:01:23 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:01:23 [scrapy.extensions.telnet] INFO: Telnet Password: 0b702b032ee6893d
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:01:23 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:01:23 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:01:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:01:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:01:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 53, in parse
    yield scrapy.follow(url=next_page_url, callback=self.parse)
          ^^^^^^^^^^^^^
AttributeError: module 'scrapy' has no attribute 'follow'
2023-10-24 06:01:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:01:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 618,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79954,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 8.319788,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 1, 31, 504912),
 'httpcompression/response_bytes': 351932,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 1, 23, 185124)}
2023-10-24 06:01:34 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:04:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:04:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:04:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:04:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:04:16 [scrapy.extensions.telnet] INFO: Telnet Password: d5969e2b3adba7a8
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:04:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:04:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:04:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:04:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:04:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:04:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 608,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79116,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 13.915329,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 4, 30, 714968),
 'httpcompression/response_bytes': 342400,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 4, 16, 799639)}
2023-10-24 06:04:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:15:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:15:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:15:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:15:55 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:15:55 [scrapy.extensions.telnet] INFO: Telnet Password: 1e26c5442af7fe40
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:15:55 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:15:55 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:15:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:15:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:16:29 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:17:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 76434,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 33.736372,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 16, 29, 33460),
 'httpcompression/response_bytes': 343907,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 15, 55, 297088)}
2023-10-24 06:17:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:17:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:17:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:17:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:17:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:17:44 [scrapy.extensions.telnet] INFO: Telnet Password: 67654778a8b92c22
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:17:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:17:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:17:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:17:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:17:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:17:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79345,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 6.767828,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 17, 51, 29941),
 'httpcompression/response_bytes': 354391,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 17, 44, 262113)}
2023-10-24 06:17:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:18:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:18:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:18:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:18:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:18:06 [scrapy.extensions.telnet] INFO: Telnet Password: 094f52b356e19d3a
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:18:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:18:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:18:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:18:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:18:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:20:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79349,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 8.04743,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 18, 14, 329277),
 'httpcompression/response_bytes': 354391,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 18, 6, 281847)}
2023-10-24 06:20:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:23:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:23:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:23:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:23:39 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:23:39 [scrapy.extensions.telnet] INFO: Telnet Password: 1e371f6a07d02edd
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:23:39 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:23:39 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:23:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:23:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:23:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:23:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79905,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 14.886245,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 23, 54, 143867),
 'httpcompression/response_bytes': 363521,
 'httpcompression/response_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1262,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 23, 39, 257622)}
2023-10-24 06:23:55 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:29:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:29:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:29:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:29:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:29:44 [scrapy.extensions.telnet] INFO: Telnet Password: 64d20a6fdb1abbd3
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:29:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:29:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:29:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:29:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:29:48 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:29:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79399,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.84203,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 29, 48, 379917),
 'httpcompression/response_bytes': 361397,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 29, 44, 537887)}
2023-10-24 06:29:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:31:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:31:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:31:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:31:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:31:14 [scrapy.extensions.telnet] INFO: Telnet Password: 8f2b9810bf4bccf8
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:31:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:31:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:31:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:31:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:31:17 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:31:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79799,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.644789,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 31, 17, 911182),
 'httpcompression/response_bytes': 361168,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 31, 14, 266393)}
2023-10-24 06:31:17 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:38:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:38:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:38:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:38:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:38:35 [scrapy.extensions.telnet] INFO: Telnet Password: 4b60057a5ae08282
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:38:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:38:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:38:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:38:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:39:35 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 19 pages/min), scraped 190 items (at 190 items/min)
2023-10-24 06:40:35 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 9 pages/min), scraped 280 items (at 90 items/min)
2023-10-24 06:41:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:41:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24373,
 'downloader/request_count': 49,
 'downloader/request_method_count/GET': 49,
 'downloader/response_bytes': 3852602,
 'downloader/response_count': 49,
 'downloader/response_status_count/200': 49,
 'elapsed_time_seconds': 175.715549,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 41, 31, 427751),
 'httpcompression/response_bytes': 17657395,
 'httpcompression/response_count': 49,
 'item_scraped_count': 490,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 48,
 'response_received_count': 49,
 'scheduler/dequeued': 49,
 'scheduler/dequeued/memory': 49,
 'scheduler/enqueued': 49,
 'scheduler/enqueued/memory': 49,
 'start_time': datetime.datetime(2023, 10, 24, 6, 38, 35, 712202)}
2023-10-24 06:41:31 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 06:42:29 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 06:42:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 06:42:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 06:42:29 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 06:42:29 [scrapy.extensions.telnet] INFO: Telnet Password: aefdb6543a0fc2aa
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 06:42:29 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 06:42:29 [scrapy.core.engine] INFO: Spider opened
2023-10-24 06:42:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 06:42:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 06:43:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=16> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&page=15)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    items['title'] = item['title']
                     ~~~~^^^^^^^^^
KeyError: 'title'
2023-10-24 06:43:27 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 06:43:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8613,
 'downloader/request_count': 16,
 'downloader/request_method_count/GET': 16,
 'downloader/response_bytes': 1262116,
 'downloader/response_count': 16,
 'downloader/response_status_count/200': 16,
 'elapsed_time_seconds': 58.260414,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 6, 43, 27, 616822),
 'httpcompression/response_bytes': 5918640,
 'httpcompression/response_count': 16,
 'item_scraped_count': 154,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 15,
 'response_received_count': 16,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 6, 42, 29, 356408)}
2023-10-24 06:43:27 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:28:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:28:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:28:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:28:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:28:36 [scrapy.extensions.telnet] INFO: Telnet Password: 1d8f42c710cf81d8
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:28:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:28:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:28:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:28:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:29:36 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 15 pages/min), scraped 150 items (at 150 items/min)
2023-10-24 07:30:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=22> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=21)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 45, in parse
    items['area'] = item['attributes']['area']
    ^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:30:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:30:06 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1befd9ac690>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 90, in close
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:30:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 10819,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 1738633,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'elapsed_time_seconds': 89.362009,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 30, 6, 256915),
 'httpcompression/response_bytes': 7974757,
 'httpcompression/response_count': 22,
 'item_scraped_count': 210,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 21,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 7, 28, 36, 894906)}
2023-10-24 07:30:06 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:30:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:30:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:30:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:30:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:30:16 [scrapy.extensions.telnet] INFO: Telnet Password: bc8f0b50e3f0f212
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:30:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:30:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:30:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:30:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:31:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2023-10-24 07:32:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:32:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:32:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:32:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:32:12 [scrapy.extensions.telnet] INFO: Telnet Password: ee8e5b526be6a01c
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:32:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:32:12 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:32:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:32:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:38:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:38:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:38:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:38:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:38:32 [scrapy.extensions.telnet] INFO: Telnet Password: ae2029cabcc2f00c
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:38:32 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:38:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:38:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:38:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:38:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:38:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:38:51 [scrapy.extensions.telnet] INFO: Telnet Password: 888db44d1be4f347
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:38:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:38:51 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:38:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:38:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 07:39:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:39:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:39:23 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:39:23 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:39:23 [scrapy.extensions.telnet] INFO: Telnet Password: d7ff28a2af3afa5e
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:39:23 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:39:23 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:39:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:39:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-10-24 07:39:32 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 160 items (at 160 items/min)
2023-10-24 07:39:49 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:39:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:39:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:39:49 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:39:49 [scrapy.extensions.telnet] INFO: Telnet Password: c03c04f3043ae98b
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:39:49 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:39:49 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:39:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:39:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2023-10-24 07:39:51 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 80 items (at 80 items/min)
2023-10-24 07:40:23 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 70 items (at 70 items/min)
2023-10-24 07:40:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:40:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:40:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:40:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:40:28 [scrapy.extensions.telnet] INFO: Telnet Password: bedc82c251503c91
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:40:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:40:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:40:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:40:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6027
2023-10-24 07:40:32 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 13 pages/min), scraped 290 items (at 130 items/min)
2023-10-24 07:40:49 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 140 items (at 140 items/min)
2023-10-24 07:40:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:40:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:40:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:40:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:40:50 [scrapy.extensions.telnet] INFO: Telnet Password: feccb415dd605bf2
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:40:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:40:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:40:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:40:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6028
2023-10-24 07:40:51 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 17 pages/min), scraped 250 items (at 170 items/min)
2023-10-24 07:41:23 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 13 pages/min), scraped 200 items (at 130 items/min)
2023-10-24 07:41:28 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 140 items (at 140 items/min)
2023-10-24 07:41:32 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 19 pages/min), scraped 480 items (at 190 items/min)
2023-10-24 07:41:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:41:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3936113,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 182.654622,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 41, 35, 126523),
 'httpcompression/response_bytes': 18069948,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 38, 32, 471901)}
2023-10-24 07:41:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:41:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=18> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=17)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 41, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:41:43 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:41:43 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x190eeba3bd0>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 59, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:41:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8811,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 1419940,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 53.113325,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 41, 43, 354209),
 'httpcompression/response_bytes': 6514800,
 'httpcompression/response_count': 18,
 'item_scraped_count': 170,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 17,
 'response_received_count': 18,
 'scheduler/dequeued': 18,
 'scheduler/dequeued/memory': 18,
 'scheduler/enqueued': 18,
 'scheduler/enqueued/memory': 18,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 7, 40, 50, 240884)}
2023-10-24 07:41:43 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:41:49 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 17 pages/min), scraped 310 items (at 170 items/min)
2023-10-24 07:41:51 [scrapy.extensions.logstats] INFO: Crawled 39 pages (at 14 pages/min), scraped 390 items (at 140 items/min)
2023-10-24 07:42:23 [scrapy.extensions.logstats] INFO: Crawled 36 pages (at 16 pages/min), scraped 360 items (at 160 items/min)
2023-10-24 07:42:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24855,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3937746,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 212.273575,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 23, 794195),
 'httpcompression/response_bytes': 18059905,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 38, 51, 520620)}
2023-10-24 07:42:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:42:28 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 24 pages/min), scraped 380 items (at 240 items/min)
2023-10-24 07:42:49 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 17 pages/min), scraped 480 items (at 170 items/min)
2023-10-24 07:42:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3936628,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 182.743873,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 51, 764218),
 'httpcompression/response_bytes': 18066868,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 39, 49, 20345)}
2023-10-24 07:42:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:42:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:42:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24826,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3935861,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 150.394355,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 42, 58, 548812),
 'httpcompression/response_bytes': 18063390,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 40, 28, 154457)}
2023-10-24 07:42:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:43:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:43:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 24875,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 3934667,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'elapsed_time_seconds': 223.40661,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 43, 6, 970595),
 'httpcompression/response_bytes': 18073291,
 'httpcompression/response_count': 50,
 'item_scraped_count': 500,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 49,
 'response_received_count': 50,
 'scheduler/dequeued': 50,
 'scheduler/dequeued/memory': 50,
 'scheduler/enqueued': 50,
 'scheduler/enqueued/memory': 50,
 'start_time': datetime.datetime(2023, 10, 24, 7, 39, 23, 563985)}
2023-10-24 07:43:06 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:55:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:55:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:55:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:55:41 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:55:41 [scrapy.extensions.telnet] INFO: Telnet Password: 932081b4673f6b53
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:55:41 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:55:41 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:55:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:55:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:56:26 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:56:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 56203,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7532566,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 44.566623,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 56, 26, 61231),
 'httpcompression/response_bytes': 26788379,
 'httpcompression/response_count': 110,
 'item_scraped_count': 200,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 7, 55, 41, 494608)}
2023-10-24 07:56:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:56:54 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:56:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:56:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:56:54 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:56:54 [scrapy.extensions.telnet] INFO: Telnet Password: eba441a2646bd872
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:56:54 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:56:54 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:56:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:56:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:57:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:57:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:57:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:57:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:57:20 [scrapy.extensions.telnet] INFO: Telnet Password: 01f34d4bb39988b9
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:57:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:57:20 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:57:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:57:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 07:57:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:57:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 56195,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7527321,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 42.06403,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 57, 36, 473926),
 'httpcompression/response_bytes': 26795643,
 'httpcompression/response_count': 110,
 'item_scraped_count': 200,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 7, 56, 54, 409896)}
2023-10-24 07:57:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:58:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-st-kilda-south-medical-centre-and-day-hospital-37-mitford-street-elwood-vic-3184-504384172> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=8)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 61, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-24 07:58:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 43, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-stage-8-eadie-court-brendale-qld-4500-504437472> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=9)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 61, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-12-24-26-hancock-way-baringa-qld-4551-504448392> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 61, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:05 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:58:05 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x2697ee44950>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 66, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 07:58:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 51467,
 'downloader/request_count': 101,
 'downloader/request_method_count/GET': 101,
 'downloader/response_bytes': 6924178,
 'downloader/response_count': 101,
 'downloader/response_status_count/200': 101,
 'elapsed_time_seconds': 44.757337,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 58, 5, 424793),
 'httpcompression/response_bytes': 24726357,
 'httpcompression/response_count': 101,
 'item_scraped_count': 178,
 'log_count/ERROR': 5,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 101,
 'scheduler/dequeued': 101,
 'scheduler/dequeued/memory': 101,
 'scheduler/enqueued': 101,
 'scheduler/enqueued/memory': 101,
 'spider_exceptions/BrokenPipeError': 4,
 'start_time': datetime.datetime(2023, 10, 24, 7, 57, 20, 667456)}
2023-10-24 07:58:05 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 07:58:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 07:58:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 07:58:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 07:58:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 07:58:44 [scrapy.extensions.telnet] INFO: Telnet Password: 55dce4ecba50e6f8
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 07:58:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 07:58:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 07:58:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 07:58:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 07:59:33 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 07:59:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 55686,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7465917,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 48.212655,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 7, 59, 33, 207717),
 'httpcompression/response_bytes': 26585987,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 7, 58, 44, 995062)}
2023-10-24 07:59:33 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:19:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:19:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:19:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:19:03 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:19:03 [scrapy.extensions.telnet] INFO: Telnet Password: c1ef03b06400aed1
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:19:03 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:19:03 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:19:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:19:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:19:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:19:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:19:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:19:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:19:28 [scrapy.extensions.telnet] INFO: Telnet Password: cf6a9e1d008fbb9e
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:19:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:19:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:19:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:19:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 08:19:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:19:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 55704,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7469941,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 51.296644,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 19, 54, 600071),
 'httpcompression/response_bytes': 26629876,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 8, 19, 3, 303427)}
2023-10-24 08:19:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:20:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-57-stewart-street-richmond-vic-3121-504432064> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 88, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 08:20:28 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 109 pages/min), scraped 198 items (at 198 items/min)
2023-10-24 08:20:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-cannon-hill-qld-4170-504421940> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 88, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 08:20:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:20:32 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1f3ba844f50>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 93, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 08:20:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 56213,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7534202,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 64.309094,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 20, 32, 872617),
 'httpcompression/response_bytes': 26836349,
 'httpcompression/response_count': 110,
 'item_scraped_count': 198,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'spider_exceptions/BrokenPipeError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 8, 19, 28, 563523)}
2023-10-24 08:20:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:20:42 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:20:42 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:20:42 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:20:42 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:20:42 [scrapy.extensions.telnet] INFO: Telnet Password: 54f2f333df081684
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:20:42 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:20:42 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:20:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:20:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:20:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=3000000&minPrice=250000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:20:46 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:20:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 327,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 78577,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 4.084264,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 20, 46, 257224),
 'httpcompression/response_bytes': 361668,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 20, 42, 172960)}
2023-10-24 08:20:49 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:21:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:21:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:21:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:21:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:21:45 [scrapy.extensions.telnet] INFO: Telnet Password: 01cd845edf394c13
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:21:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:21:45 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:21:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:21:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:22:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:22:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:22:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:22:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:22:46 [scrapy.extensions.telnet] INFO: Telnet Password: 167e4ab8bf7bc38e
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:22:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:22:46 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:22:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:22:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:22:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=50000000&minPrice=100000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:22:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:22:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 328,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 80933,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 7.230053,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 22, 53, 698750),
 'httpcompression/response_bytes': 377578,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 22, 46, 468697)}
2023-10-24 08:22:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:23:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:23:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:23:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:23:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:23:44 [scrapy.extensions.telnet] INFO: Telnet Password: 3d82a83fb5d03318
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:23:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:23:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:23:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:23:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:24:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=50000000&minPrice=200000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:24:02 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:24:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 328,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 80119,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 18.207429,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 24, 2, 917116),
 'httpcompression/response_bytes': 377538,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 23, 44, 709687)}
2023-10-24 08:24:02 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:24:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:24:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:24:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:24:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:24:35 [scrapy.extensions.telnet] INFO: Telnet Password: eabfa325ec91b719
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:24:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:24:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:24:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:24:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:24:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&maxPrice=50000000&minPrice=200000&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 39, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'
2023-10-24 08:24:57 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:24:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 328,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 80706,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 22.16591,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 24, 57, 511091),
 'httpcompression/response_bytes': 377578,
 'httpcompression/response_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 8, 24, 35, 345181)}
2023-10-24 08:24:57 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:25:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:25:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:25:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:25:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:25:16 [scrapy.extensions.telnet] INFO: Telnet Password: 62ae9c9416af68d9
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:25:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:25:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:25:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:25:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:26:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:26:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 60210,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7544917,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'elapsed_time_seconds': 50.688079,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 26, 7, 25861),
 'httpcompression/response_bytes': 26873973,
 'httpcompression/response_count': 110,
 'item_scraped_count': 200,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 8, 25, 16, 337782)}
2023-10-24 08:26:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:32:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:32:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:32:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:32:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:32:34 [scrapy.extensions.telnet] INFO: Telnet Password: ae377a1cff76af2d
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:32:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:32:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:32:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:32:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:33:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:33:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 59612,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7476976,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 49.038763,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 33, 23, 105230),
 'httpcompression/response_bytes': 26652173,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 8, 32, 34, 66467)}
2023-10-24 08:33:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:36:37 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:36:37 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:36:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:36:37 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:36:37 [scrapy.extensions.telnet] INFO: Telnet Password: 1188626b1e043b09
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:36:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:36:37 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:36:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:36:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:37:01 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:37:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:37:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:37:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:37:01 [scrapy.extensions.telnet] INFO: Telnet Password: 653e6b41058f5ab8
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:37:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:37:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:37:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:37:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:37:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:37:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 57473,
 'downloader/request_count': 109,
 'downloader/request_method_count/GET': 109,
 'downloader/response_bytes': 7466003,
 'downloader/response_count': 109,
 'downloader/response_status_count/200': 109,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 57.198611,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 37, 58, 979328),
 'httpcompression/response_bytes': 26611007,
 'httpcompression/response_count': 109,
 'item_scraped_count': 199,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 109,
 'scheduler/dequeued': 109,
 'scheduler/dequeued/memory': 109,
 'scheduler/enqueued': 109,
 'scheduler/enqueued/memory': 109,
 'start_time': datetime.datetime(2023, 10, 24, 8, 37, 1, 780717)}
2023-10-24 08:37:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:50:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:50:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:50:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:50:53 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:50:53 [scrapy.extensions.telnet] INFO: Telnet Password: 699d982202fd3601
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:50:53 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:50:53 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:50:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:50:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:51:53 [scrapy.extensions.logstats] INFO: Crawled 126 pages (at 126 pages/min), scraped 243 items (at 243 items/min)
2023-10-24 08:52:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:52:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:52:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:52:24 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:52:24 [scrapy.extensions.telnet] INFO: Telnet Password: 4e96442e113b1a27
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:52:24 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:52:24 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:52:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:52:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 08:52:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 08:52:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 121913,
 'downloader/request_count': 220,
 'downloader/request_method_count/GET': 220,
 'downloader/response_bytes': 14918866,
 'downloader/response_count': 220,
 'downloader/response_status_count/200': 220,
 'elapsed_time_seconds': 97.546085,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 8, 52, 30, 710858),
 'httpcompression/response_bytes': 52481655,
 'httpcompression/response_count': 220,
 'item_scraped_count': 400,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 20,
 'response_received_count': 220,
 'scheduler/dequeued': 220,
 'scheduler/dequeued/memory': 220,
 'scheduler/enqueued': 220,
 'scheduler/enqueued/memory': 220,
 'start_time': datetime.datetime(2023, 10, 24, 8, 50, 53, 164773)}
2023-10-24 08:52:30 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 08:53:24 [scrapy.extensions.logstats] INFO: Crawled 112 pages (at 112 pages/min), scraped 211 items (at 211 items/min)
2023-10-24 08:53:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:53:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:53:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:53:41 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:53:41 [scrapy.extensions.telnet] INFO: Telnet Password: df2ee8c119b35647
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:53:41 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:53:41 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:53:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:53:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 08:55:01 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 08:55:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 08:55:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 08:55:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 08:55:01 [scrapy.extensions.telnet] INFO: Telnet Password: 934249599af40e14
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 08:55:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 08:55:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 08:55:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 08:55:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:00:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:00:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:00:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:00:59 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:00:59 [scrapy.extensions.telnet] INFO: Telnet Password: 85a169f8b568f341
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:00:59 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:00:59 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:00:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:00:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:01:59 [scrapy.extensions.logstats] INFO: Crawled 92 pages (at 92 pages/min), scraped 172 items (at 172 items/min)
2023-10-24 09:02:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:02:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:02:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:02:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:02:14 [scrapy.extensions.telnet] INFO: Telnet Password: 5bdf55c436e0e114
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:02:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:02:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:02:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:02:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:02:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=400000&page=1&minPrice=200001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=400000&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '200001.0'
2023-10-24 09:02:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=200000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=400000&page=10)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '200000.0'
2023-10-24 09:03:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:03:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 61463,
 'downloader/request_count': 111,
 'downloader/request_method_count/GET': 111,
 'downloader/response_bytes': 7435404,
 'downloader/response_count': 111,
 'downloader/response_status_count/200': 111,
 'elapsed_time_seconds': 46.086581,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 3, 0, 847496),
 'httpcompression/response_bytes': 25959916,
 'httpcompression/response_count': 111,
 'item_scraped_count': 198,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 10,
 'response_received_count': 111,
 'scheduler/dequeued': 111,
 'scheduler/dequeued/memory': 111,
 'scheduler/enqueued': 111,
 'scheduler/enqueued/memory': 111,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 2, 14, 760915)}
2023-10-24 09:03:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:05:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:05:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:05:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:05:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:05:09 [scrapy.extensions.telnet] INFO: Telnet Password: 9743ea6090947823
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:05:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:05:09 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:05:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:05:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:06:09 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 30 pages/min), scraped 57 items (at 57 items/min)
2023-10-24 09:07:45 [scrapy.extensions.logstats] INFO: Crawled 42 pages (at 12 pages/min), scraped 79 items (at 22 items/min)
2023-10-24 09:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=250000&page=1&minPrice=125001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=250000&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '125001.0'
2023-10-24 09:08:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=125000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=250000&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '125000.0'
2023-10-24 09:08:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:08:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:08:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:08:41 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:08:41 [scrapy.extensions.telnet] INFO: Telnet Password: ab9cb6dd8ce05f0f
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:08:41 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:08:41 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:08:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:08:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:09:41 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 10 items (at 10 items/min)
2023-10-24 09:10:41 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 11 pages/min), scraped 21 items (at 11 items/min)
2023-10-24 09:11:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=100000&page=1&minPrice=50001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=100000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '50001.0'
2023-10-24 09:11:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=50000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=100000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '50000.0'
2023-10-24 09:11:05 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:11:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:11:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:11:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:11:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:11:22 [scrapy.extensions.telnet] INFO: Telnet Password: c6e0c896633ba2ed
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:11:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:11:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:11:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:11:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:11:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:11:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:11:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:11:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:11:36 [scrapy.extensions.telnet] INFO: Telnet Password: a9c583827990add9
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:11:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:11:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:11:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:11:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:12:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:12:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:12:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:12:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:12:22 [scrapy.extensions.telnet] INFO: Telnet Password: c1dce2fb39f61d3f
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:12:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:12:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:12:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:12:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:13:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:14:05 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:14:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1811,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 259280,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'elapsed_time_seconds': 102.172517,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 14, 5, 116471),
 'httpcompression/response_bytes': 867610,
 'httpcompression/response_count': 4,
 'item_scraped_count': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 10, 24, 9, 12, 22, 943954)}
2023-10-24 09:14:05 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:14:17 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:14:17 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:14:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:14:17 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:14:17 [scrapy.extensions.telnet] INFO: Telnet Password: b08353e622b98d05
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:14:17 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:14:17 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:14:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:14:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:15:34 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 2 items (at 2 items/min)
2023-10-24 09:15:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1000000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000000.0'
2023-10-24 09:16:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000&minPrice=1000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000001.0'
2023-10-24 09:16:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:16:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2546,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 315745,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 103.051112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 16, 0, 545676),
 'httpcompression/response_bytes': 1063386,
 'httpcompression/response_count': 5,
 'item_scraped_count': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 14, 17, 494564)}
2023-10-24 09:16:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:17:15 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:17:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:17:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:17:15 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:17:15 [scrapy.extensions.telnet] INFO: Telnet Password: 9294765fa695cb26
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:17:15 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:17:15 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:17:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:17:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:18:44 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 2 items (at 2 items/min)
2023-10-24 09:19:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000&minPrice=1000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000001.0'
2023-10-24 09:19:19 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 4 pages/min), scraped 3 items (at 1 items/min)
2023-10-24 09:20:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:20:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:20:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:20:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:20:20 [scrapy.extensions.telnet] INFO: Telnet Password: f98446f8f40aad66
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:20:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:20:20 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:20:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:20:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:22:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:22:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:22:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:22:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:22:22 [scrapy.extensions.telnet] INFO: Telnet Password: a1a44bc83b04b820
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:22:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:22:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:22:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:22:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:23:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '25000000.0'
2023-10-24 09:23:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000&minPrice=25000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '25000001.0'
2023-10-24 09:23:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:23:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3064,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 393710,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 48.428378,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 23, 10, 739065),
 'httpcompression/response_bytes': 1375685,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 22, 22, 310687)}
2023-10-24 09:23:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:24:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:24:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:24:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:24:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:24:14 [scrapy.extensions.telnet] INFO: Telnet Password: 9824fe61e4f5fe9e
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:24:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:24:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:24:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:24:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:24:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 54, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 09:24:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-penola-sa-5277-504405824> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:24:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:24:56 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x284f5e1e250>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 122, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:24:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 830,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 128159,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 42.142727,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 24, 56, 313626),
 'httpcompression/response_bytes': 436960,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/BrokenPipeError': 1,
 'spider_exceptions/EOFError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 9, 24, 14, 170899)}
2023-10-24 09:24:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:25:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:25:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:25:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:25:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:25:34 [scrapy.extensions.telnet] INFO: Telnet Password: cdfc2b17708ab247
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:25:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:25:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:25:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:25:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:26:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000&minPrice=1000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1000001.0'
2023-10-24 09:26:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:26:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:26:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:26:18 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:26:18 [scrapy.extensions.telnet] INFO: Telnet Password: 1eb9b6daf06fa9d5
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:26:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:26:18 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:26:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:26:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:28:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:28:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:28:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:28:06 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:28:06 [scrapy.extensions.telnet] INFO: Telnet Password: 1d8fdaede86c5ec9
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:28:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:28:06 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:28:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:28:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:28:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1500000&minPrice=750001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '750001.0'
2023-10-24 09:28:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=750000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=1500000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '750000.0'
2023-10-24 09:28:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:28:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2540,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 315617,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 24.027487,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 28, 30, 915001),
 'httpcompression/response_bytes': 1063356,
 'httpcompression/response_count': 5,
 'item_scraped_count': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 28, 6, 887514)}
2023-10-24 09:28:30 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:29:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:29:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:29:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:29:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: 2da219a033252788
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:29:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:29:13 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:29:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:29:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:30:15 [scrapy.extensions.logstats] INFO: Crawled 76 pages (at 76 pages/min), scraped 135 items (at 135 items/min)
2023-10-24 09:30:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=175000.0&page=1&minPrice=0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000&page=8)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '175000.0'
2023-10-24 09:30:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000&page=1&minPrice=175001.0> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000&page=8)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '175001.0'
2023-10-24 09:30:19 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:30:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 44819,
 'downloader/request_count': 81,
 'downloader/request_method_count/GET': 81,
 'downloader/response_bytes': 5416513,
 'downloader/response_count': 81,
 'downloader/response_status_count/200': 81,
 'elapsed_time_seconds': 66.381128,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 30, 19, 794434),
 'httpcompression/response_bytes': 18936662,
 'httpcompression/response_count': 81,
 'item_scraped_count': 142,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 8,
 'response_received_count': 81,
 'scheduler/dequeued': 81,
 'scheduler/dequeued/memory': 81,
 'scheduler/enqueued': 81,
 'scheduler/enqueued/memory': 81,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 29, 13, 413306)}
2023-10-24 09:30:19 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:30:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:30:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:30:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:30:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:30:40 [scrapy.extensions.telnet] INFO: Telnet Password: 470f0a78c26dea99
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:30:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:30:40 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:30:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:30:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:31:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 54, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 09:31:59 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 4 items (at 4 items/min)
2023-10-24 09:31:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lots-1-15-hornby-road-port-broughton-sa-5522-504410552> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:32:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lots-20-21-inglis-circuit-gillman-sa-5013-504438988> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:32:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-totness-trading-hub-lots-14-16-innovation-drive-totness-sa-5250-504137763> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:32:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-8-183-tynte-street-north-adelaide-sa-5006-504386812> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=south-australia&maxPrice=350000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 117, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:35:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:35:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:35:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:35:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:35:09 [scrapy.extensions.telnet] INFO: Telnet Password: 92d2e2677db42125
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:35:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:35:09 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:35:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:35:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:35:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000&minPrice=5000001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '5000001.0'
2023-10-24 09:35:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '5000000.0'
2023-10-24 09:35:20 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:35:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3057,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 386291,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 10.78922,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 35, 20, 656740),
 'httpcompression/response_bytes': 1314104,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 35, 9, 867520)}
2023-10-24 09:35:20 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:36:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:36:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:36:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:36:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:36:01 [scrapy.extensions.telnet] INFO: Telnet Password: 511022abb2ba2c65
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:36:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:36:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:36:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:36:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:36:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000&minPrice=2500001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500001.0'
2023-10-24 09:36:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2500000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500000.0'
2023-10-24 09:36:16 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:36:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3055,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 383905,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 15.768391,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 36, 16, 782512),
 'httpcompression/response_bytes': 1295499,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 36, 1, 14121)}
2023-10-24 09:36:16 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:36:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:36:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:36:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:36:24 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:36:24 [scrapy.extensions.telnet] INFO: Telnet Password: d400acbb9e5f8eaf
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:36:24 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:36:24 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:36:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:36:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:36:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000&minPrice=2500001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500001.0'
2023-10-24 09:36:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=2500000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '2500000.0'
2023-10-24 09:36:42 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:36:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3055,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 383913,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 17.907409,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 36, 42, 824749),
 'httpcompression/response_bytes': 1295503,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 36, 24, 917340)}
2023-10-24 09:36:42 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:37:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:37:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:37:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:37:03 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:37:03 [scrapy.extensions.telnet] INFO: Telnet Password: 10f091f643b8d828
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:37:03 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:37:03 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:37:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:37:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:37:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000&minPrice=12500001.0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 33, in parse
    minPrice = int(params['minPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '12500001.0'
2023-10-24 09:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=12500000.0&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 37, in parse
    maxPrice = int(params['maxPrice'][0])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '12500000.0'
2023-10-24 09:39:07 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 6 items (at 6 items/min)
2023-10-24 09:39:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:39:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3064,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 386918,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 124.175775,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 39, 7, 654169),
 'httpcompression/response_bytes': 1317077,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2023, 10, 24, 9, 37, 3, 478394)}
2023-10-24 09:39:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:39:29 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:39:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:39:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:39:29 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:39:29 [scrapy.extensions.telnet] INFO: Telnet Password: 719e8f0a07c39354
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:39:29 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:39:29 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:39:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:39:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:40:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=5000000&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=10000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 71, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:40:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:40:01 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1eaac9d8c90>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 130, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:40:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3058,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 386479,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 31.459315,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 40, 1, 330339),
 'httpcompression/response_bytes': 1314139,
 'httpcompression/response_count': 6,
 'item_scraped_count': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'spider_exceptions/BrokenPipeError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 9, 39, 29, 871024)}
2023-10-24 09:40:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:40:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:40:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:40:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:40:36 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:40:36 [scrapy.extensions.telnet] INFO: Telnet Password: 96350ca6d7bde006
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:40:36 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:40:36 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:40:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:40:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:42:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=25000000&minPrice=0&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 71, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 09:42:41 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 4 items (at 4 items/min)
2023-10-24 09:42:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-penola-sa-5277-504405824> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 125, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:42:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-must-coonawarra-apartments-cameron-s-cottage-126-church-street-penola-sa-5277-504216852> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=5277&maxPrice=50000000)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 125, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 09:44:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:44:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:44:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:44:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:44:02 [scrapy.extensions.telnet] INFO: Telnet Password: 8b5a9bb0127d6a7c
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:44:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:44:02 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:44:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:44:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:45:02 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 8 items (at 8 items/min)
2023-10-24 09:46:11 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:46:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:46:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:46:11 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:46:11 [scrapy.extensions.telnet] INFO: Telnet Password: 513c918356db1b20
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:46:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:46:11 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:46:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:46:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:46:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:46:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 13873,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 1504179,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'elapsed_time_seconds': 42.813879,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 46, 54, 40903),
 'httpcompression/response_bytes': 5206615,
 'httpcompression/response_count': 24,
 'item_scraped_count': 13,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 24,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2023, 10, 24, 9, 46, 11, 227024)}
2023-10-24 09:46:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 09:48:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:48:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:48:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:48:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:48:31 [scrapy.extensions.telnet] INFO: Telnet Password: c6d938a038cf0f55
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:48:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:48:31 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:48:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:48:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:48:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:48:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:48:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:48:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:48:50 [scrapy.extensions.telnet] INFO: Telnet Password: 32bf11b9f81b1a63
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:48:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:48:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:48:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:48:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:49:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 09:49:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 09:49:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 09:49:09 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 09:49:09 [scrapy.extensions.telnet] INFO: Telnet Password: 7f1e6d82f1cd5c5c
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 09:49:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 09:49:09 [scrapy.core.engine] INFO: Spider opened
2023-10-24 09:49:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 09:49:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 09:49:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 09:49:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 16501,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 1427577,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'elapsed_time_seconds': 48.890947,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 9, 49, 58, 609674),
 'httpcompression/response_bytes': 4769678,
 'httpcompression/response_count': 24,
 'item_scraped_count': 13,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 24,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2023, 10, 24, 9, 49, 9, 718727)}
2023-10-24 09:49:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:09:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:09:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:09:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:09:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:09:28 [scrapy.extensions.telnet] INFO: Telnet Password: 36db6457e42fc8a5
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:09:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:09:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:09:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:09:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:10:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:10:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 33783,
 'downloader/request_count': 49,
 'downloader/request_method_count/GET': 49,
 'downloader/response_bytes': 2954370,
 'downloader/response_count': 49,
 'downloader/response_status_count/200': 49,
 'elapsed_time_seconds': 40.260189,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 10, 8, 849628),
 'httpcompression/response_bytes': 10057701,
 'httpcompression/response_count': 49,
 'item_scraped_count': 52,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 49,
 'scheduler/dequeued': 49,
 'scheduler/dequeued/memory': 49,
 'scheduler/enqueued': 49,
 'scheduler/enqueued/memory': 49,
 'start_time': datetime.datetime(2023, 10, 24, 10, 9, 28, 589439)}
2023-10-24 10:10:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:14:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:14:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:14:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:14:07 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:14:07 [scrapy.extensions.telnet] INFO: Telnet Password: 3f4afefc98e07218
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:14:07 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:14:07 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:14:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:14:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:15:07 [scrapy.extensions.logstats] INFO: Crawled 141 pages (at 141 pages/min), scraped 266 items (at 266 items/min)
2023-10-24 10:15:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:15:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:15:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:15:22 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:15:22 [scrapy.extensions.telnet] INFO: Telnet Password: 322ed7cdd58cc0df
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:15:22 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:15:22 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:15:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:15:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 10:15:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-24 10:15:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:15:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 113056,
 'downloader/request_count': 220,
 'downloader/request_method_count/GET': 220,
 'downloader/response_bytes': 14989195,
 'downloader/response_count': 220,
 'downloader/response_status_count/200': 219,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 89.39871,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 15, 36, 754655),
 'httpcompression/response_bytes': 53339006,
 'httpcompression/response_count': 220,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 399,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 20,
 'response_received_count': 220,
 'scheduler/dequeued': 220,
 'scheduler/dequeued/memory': 220,
 'scheduler/enqueued': 220,
 'scheduler/enqueued/memory': 220,
 'start_time': datetime.datetime(2023, 10, 24, 10, 14, 7, 355945)}
2023-10-24 10:15:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:15:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:15:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:15:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:15:50 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:15:50 [scrapy.extensions.telnet] INFO: Telnet Password: 3248841a6882b0e9
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:15:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:15:50 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:15:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:15:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:16:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:16:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:16:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:16:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:16:30 [scrapy.extensions.telnet] INFO: Telnet Password: a3193824353f3dab
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:16:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:16:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:16:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:16:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:17:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:17:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:17:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:17:20 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:17:20 [scrapy.extensions.telnet] INFO: Telnet Password: 31dabc8a84741a70
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:17:20 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:17:20 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:17:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:17:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:19:17 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:19:17 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:19:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:19:17 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:19:17 [scrapy.extensions.telnet] INFO: Telnet Password: 2af15132745499fa
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:19:17 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:19:17 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:19:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:19:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:20:08 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:20:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:20:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:20:08 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:20:08 [scrapy.extensions.telnet] INFO: Telnet Password: 808ef195b4fc3713
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:20:08 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:20:08 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:20:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:20:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-10-24 10:20:17 [scrapy.extensions.logstats] INFO: Crawled 107 pages (at 107 pages/min), scraped 196 items (at 196 items/min)
2023-10-24 10:20:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-3-5-foster-street-leichhardt-nsw-2040-504416788> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 289, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended
2023-10-24 10:20:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-39-dalton-street-kippa-ring-qld-4021-504443036> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-level-1-881-collins-street-docklands-vic-3008-504422116> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=5)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-5-charlotte-street-dardanup-wa-6236-504389544> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-45-riversdale-road-newtown-vic-3220-504422108> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-44-cubitt-street-cremorne-vic-3121-504194655> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-31-park-street-st-kilda-west-vic-3182-504405688> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 54, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-mount-gambier-business-park-lot-13-riddoch-highway-suttontown-sa-5291-504395240> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lots-1-2-235-237-pirie-street-adelaide-sa-5000-504416784> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-luton-house-2-luton-lane-hawthorn-vic-3122-504443020> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-union-hotel-271-pacific-highway-north-sydney-nsw-2060-504400544> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-7-eleven-carl-s-jr-357-brisbane-street-west-ipswich-qld-4305-504432144> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-253-255-259-261-and-265-pacific-highway-north-sydney-nsw-2060-504384232> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=6)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-38-42-winnima-circuit-pemulwuy-nsw-2145-504427256> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&page=7)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 126, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:45 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:20:45 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1a37342bd50>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 131, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:20:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34501,
 'downloader/request_count': 68,
 'downloader/request_method_count/GET': 68,
 'downloader/response_bytes': 4668761,
 'downloader/response_count': 68,
 'downloader/response_status_count/200': 68,
 'elapsed_time_seconds': 36.187257,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 20, 45, 28588),
 'httpcompression/response_bytes': 16754603,
 'httpcompression/response_count': 68,
 'item_scraped_count': 107,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 7,
 'response_received_count': 68,
 'scheduler/dequeued': 68,
 'scheduler/dequeued/memory': 68,
 'scheduler/enqueued': 68,
 'scheduler/enqueued/memory': 68,
 'spider_exceptions/BrokenPipeError': 15,
 'start_time': datetime.datetime(2023, 10, 24, 10, 20, 8, 841331)}
2023-10-24 10:20:45 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:21:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/newpoint-business-park-128>: HTTP status code is not handled or not allowed
2023-10-24 10:21:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:21:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 112480,
 'downloader/request_count': 219,
 'downloader/request_method_count/GET': 219,
 'downloader/response_bytes': 14916024,
 'downloader/response_count': 219,
 'downloader/response_status_count/200': 218,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 117.672814,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 21, 15, 602940),
 'httpcompression/response_bytes': 53081003,
 'httpcompression/response_count': 219,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 398,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 20,
 'response_received_count': 219,
 'scheduler/dequeued': 219,
 'scheduler/dequeued/memory': 219,
 'scheduler/enqueued': 219,
 'scheduler/enqueued/memory': 219,
 'start_time': datetime.datetime(2023, 10, 24, 10, 19, 17, 930126)}
2023-10-24 10:21:15 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:21:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:21:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:21:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:21:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:21:47 [scrapy.extensions.telnet] INFO: Telnet Password: cba0e08b1afe6733
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:21:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:21:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:21:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:21:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:22:47 [scrapy.extensions.logstats] INFO: Crawled 121 pages (at 121 pages/min), scraped 229 items (at 229 items/min)
2023-10-24 10:23:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:23:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:23:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:23:13 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:23:13 [scrapy.extensions.telnet] INFO: Telnet Password: 571cb283f90d40dc
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:23:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:23:13 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:23:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:23:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:23:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:23:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3918,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 569993,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 9.62345,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 23, 23, 583826),
 'httpcompression/response_bytes': 2165669,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 23, 13, 960376)}
2023-10-24 10:23:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:24:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:24:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:24:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:24:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:24:00 [scrapy.extensions.telnet] INFO: Telnet Password: b53501c3a2f9893f
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:24:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:24:00 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:24:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:24:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:24:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:24:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4089,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 570306,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 12.990369,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 24, 13, 682219),
 'httpcompression/response_bytes': 2168330,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 24, 0, 691850)}
2023-10-24 10:24:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:25:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:25:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:25:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:25:55 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:25:55 [scrapy.extensions.telnet] INFO: Telnet Password: 324543728220dbcf
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:25:55 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:25:55 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:25:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:25:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:26:09 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:26:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4080,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 569607,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 14.55146,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 26, 9, 969568),
 'httpcompression/response_bytes': 2169036,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 25, 55, 418108)}
2023-10-24 10:26:09 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:27:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:27:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:27:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:27:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:27:10 [scrapy.extensions.telnet] INFO: Telnet Password: cd76dcc8a46a4104
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:27:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:27:10 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:27:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:27:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:27:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:27:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4080,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 569601,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 22.934795,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 27, 32, 952879),
 'httpcompression/response_bytes': 2169037,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2023, 10, 24, 10, 27, 10, 18084)}
2023-10-24 10:27:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:27:54 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:27:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:27:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:27:54 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:27:54 [scrapy.extensions.telnet] INFO: Telnet Password: 9bbacce20417f3ce
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:27:54 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:27:54 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:27:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:27:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:34:08 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 6 items (at 6 items/min)
2023-10-24 10:34:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:34:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 5,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 5,
 'downloader/request_bytes': 6618,
 'downloader/request_count': 13,
 'downloader/request_method_count/GET': 13,
 'downloader/response_bytes': 569156,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 380.159192,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 34, 14, 610642),
 'httpcompression/response_bytes': 2168872,
 'httpcompression/response_count': 8,
 'item_scraped_count': 12,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 8,
 'retry/count': 5,
 'retry/reason_count/twisted.internet.error.TimeoutError': 5,
 'scheduler/dequeued': 13,
 'scheduler/dequeued/memory': 13,
 'scheduler/enqueued': 13,
 'scheduler/enqueued/memory': 13,
 'start_time': datetime.datetime(2023, 10, 24, 10, 27, 54, 451450)}
2023-10-24 10:34:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:35:27 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:35:27 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:35:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:35:27 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:35:27 [scrapy.extensions.telnet] INFO: Telnet Password: 413502182a6ee1b9
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:35:27 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:35:27 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:35:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:35:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:35:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:35:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:35:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:35:40 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:35:40 [scrapy.extensions.telnet] INFO: Telnet Password: 587b0e44efef08d0
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:35:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:35:40 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:35:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:35:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:36:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:36:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 30679,
 'downloader/request_count': 59,
 'downloader/request_method_count/GET': 59,
 'downloader/response_bytes': 4000932,
 'downloader/response_count': 59,
 'downloader/response_status_count/200': 59,
 'elapsed_time_seconds': 27.54845,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 36, 8, 332580),
 'httpcompression/response_bytes': 13950895,
 'httpcompression/response_count': 59,
 'item_scraped_count': 106,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 59,
 'scheduler/dequeued': 59,
 'scheduler/dequeued/memory': 59,
 'scheduler/enqueued': 59,
 'scheduler/enqueued/memory': 59,
 'start_time': datetime.datetime(2023, 10, 24, 10, 35, 40, 784130)}
2023-10-24 10:36:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:36:19 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:36:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:36:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:36:19 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:36:19 [scrapy.extensions.telnet] INFO: Telnet Password: 8a5f2a9eaf9d82ca
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:36:19 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:36:19 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:36:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:36:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:37:19 [scrapy.extensions.logstats] INFO: Crawled 125 pages (at 125 pages/min), scraped 153 items (at 153 items/min)
2023-10-24 10:37:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:37:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 88994,
 'downloader/request_count': 154,
 'downloader/request_method_count/GET': 154,
 'downloader/response_bytes': 9700278,
 'downloader/response_count': 154,
 'downloader/response_status_count/200': 154,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 71.267328,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 37, 30, 462733),
 'httpcompression/response_bytes': 33316379,
 'httpcompression/response_count': 154,
 'item_scraped_count': 168,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 22,
 'response_received_count': 154,
 'scheduler/dequeued': 154,
 'scheduler/dequeued/memory': 154,
 'scheduler/enqueued': 154,
 'scheduler/enqueued/memory': 154,
 'start_time': datetime.datetime(2023, 10, 24, 10, 36, 19, 195405)}
2023-10-24 10:37:30 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:39:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:39:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:39:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:39:16 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:39:16 [scrapy.extensions.telnet] INFO: Telnet Password: 3d043904a64121f2
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:39:16 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:39:16 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:39:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:39:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:40:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2789063&minPrice=2778517&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2789063&minPrice=2767970&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 10:40:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1702735&minPrice=1697463&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1702735&minPrice=1692189&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2767969&minPrice=2762697&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2767969&minPrice=2757423&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2741602&minPrice=2736330&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2736330&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1697462&minPrice=1692189&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1702735&minPrice=1692189&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2731056&minPrice=2725783&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2736329&minPrice=2725783&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2741603&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2736330&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2673047&minPrice=2667775&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2673047&minPrice=2662501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2978907&minPrice=2973635&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2978907&minPrice=2968361&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2736329&minPrice=2731057&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2736329&minPrice=2725783&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2973634&minPrice=2968361&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2978907&minPrice=2968361&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2667774&minPrice=2662501&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2673047&minPrice=2662501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2488478&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2483205&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2488477&minPrice=2483205&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2483205&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2752149&minPrice=2746876&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2757422&minPrice=2746876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2778516&minPrice=2773244&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2778516&minPrice=2767970&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2757422&minPrice=2752150&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2757422&minPrice=2746876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2773243&minPrice=2767970&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2778516&minPrice=2767970&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2968360&minPrice=2963088&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2968360&minPrice=2957814&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2963087&minPrice=2957814&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2968360&minPrice=2957814&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2762696&minPrice=2757423&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2767969&minPrice=2757423&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:16 [scrapy.extensions.logstats] INFO: Crawled 161 pages (at 161 pages/min), scraped 465 items (at 465 items/min)
2023-10-24 10:40:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2483204&minPrice=2472658&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2493750&minPrice=2472658&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2008594&minPrice=1998048&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2008594&minPrice=1987501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=3000000&minPrice=2989455&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=3000000&minPrice=2978908&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-lot-233-commercial-hub-impressions-drive-eglinton-wa-6034-504409692> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2704688&minPrice=2683595&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-11-ellen-street-subiaco-wa-6008-504398556> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-68-paramount-drive-wangara-wa-6065-504393728> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1998047&minPrice=1987501&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2008594&minPrice=1987501&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-139-stirling-street-perth-wa-6000-504390304> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-3571-great-northern-highway-muchea-wa-6501-503217078> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2704689&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2989454&minPrice=2978908&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=3000000&minPrice=2978908&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2725782&minPrice=2704689&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2746875&minPrice=2704689&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-10-william-street-york-wa-6302-504447160> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1776564&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-civic-heart-1-mends-street-south-perth-wa-6151-503645862> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2957813&minPrice=2915626&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-61-york-street-subiaco-wa-6008-504353496> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2831250&minPrice=2789064&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-244-york-street-albany-wa-6330-504104755> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2957813&minPrice=2915626&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-8-boyd-cresent-hamilton-hill-wa-6163-504396056> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2831250&minPrice=2789064&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2894532&minPrice=2873439&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2915625&minPrice=2873439&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-94-262-lord-street-perth-wa-6000-504184803> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-238-oxford-street-leederville-wa-6007-504162219> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-48b-48c-glyde-street-mosman-park-wa-6012-504378108> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1776563&minPrice=1734376&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=1818750&minPrice=1734376&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2367188&minPrice=2325001&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2409375&minPrice=2325001&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-19-remisko-drive-forrestdale-wa-6112-504290384> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-192-stirling-street-perth-wa-6000-504311512> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2873438&minPrice=2831251&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2409375&minPrice=2367189&page=1> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2409375&minPrice=2325001&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 75, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-177-hay-street-subiaco-wa-6008-504421136> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-1-corner-barry-marshall-parade-fiona-wood-road-murdoch-wa-6150-503636870> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-20-22-railway-road-subiaco-wa-6008-504428044> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-10-karratha-street-welshpool-wa-6106-504432036> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2662500&minPrice=2493751&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-37-grisker-road-wanneroo-wa-6065-504447884> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2662500&minPrice=2493751&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-16-hutton-street-osborne-park-wa-6017-504374584> (referer: https://www.realcommercial.com.au/for-sale/?includePropertiesWithin=includesurrounding&locations=6168&maxPrice=2156250&minPrice=2071876&page=1)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 129, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:26 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:40:26 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x18ff8160a50>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 10:40:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 109899,
 'downloader/request_count': 191,
 'downloader/request_method_count/GET': 191,
 'downloader/response_bytes': 13174857,
 'downloader/response_count': 191,
 'downloader/response_status_count/200': 191,
 'dupefilter/filtered': 19,
 'elapsed_time_seconds': 69.49163,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 40, 26, 435330),
 'httpcompression/response_bytes': 49188249,
 'httpcompression/response_count': 191,
 'item_scraped_count': 465,
 'log_count/ERROR': 53,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 12,
 'response_received_count': 191,
 'scheduler/dequeued': 191,
 'scheduler/dequeued/memory': 191,
 'scheduler/enqueued': 191,
 'scheduler/enqueued/memory': 191,
 'spider_exceptions/BrokenPipeError': 51,
 'spider_exceptions/EOFError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 10, 39, 16, 943700)}
2023-10-24 10:40:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 10:40:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:40:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:40:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:40:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:40:30 [scrapy.extensions.telnet] INFO: Telnet Password: b6f478ee9e27ff26
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:40:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:40:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:40:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:40:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:41:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 10:41:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 10:41:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 10:41:02 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 10:41:02 [scrapy.extensions.telnet] INFO: Telnet Password: 523be881adc8837d
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 10:41:02 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 10:41:02 [scrapy.core.engine] INFO: Spider opened
2023-10-24 10:41:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 10:41:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 10:42:03 [scrapy.extensions.logstats] INFO: Crawled 173 pages (at 173 pages/min), scraped 315 items (at 315 items/min)
2023-10-24 10:43:02 [scrapy.extensions.logstats] INFO: Crawled 368 pages (at 195 pages/min), scraped 713 items (at 398 items/min)
2023-10-24 10:43:37 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 10:43:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 278629,
 'downloader/request_count': 472,
 'downloader/request_method_count/GET': 472,
 'downloader/response_bytes': 30398217,
 'downloader/response_count': 472,
 'downloader/response_status_count/200': 472,
 'dupefilter/filtered': 47,
 'elapsed_time_seconds': 154.695936,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 10, 43, 37, 686444),
 'httpcompression/response_bytes': 107632237,
 'httpcompression/response_count': 472,
 'item_scraped_count': 964,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 472,
 'scheduler/dequeued': 472,
 'scheduler/dequeued/memory': 472,
 'scheduler/enqueued': 472,
 'scheduler/enqueued/memory': 472,
 'start_time': datetime.datetime(2023, 10, 24, 10, 41, 2, 990508)}
2023-10-24 10:43:37 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:48:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:48:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:48:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:48:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:48:12 [scrapy.extensions.telnet] INFO: Telnet Password: ddd852c2e77ce256
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:48:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:48:12 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:48:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:48:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:48:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:48:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 72220,
 'downloader/request_count': 103,
 'downloader/request_method_count/GET': 103,
 'downloader/response_bytes': 7067860,
 'downloader/response_count': 103,
 'downloader/response_status_count/200': 103,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 40.666915,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 48, 53, 124810),
 'httpcompression/response_bytes': 25384794,
 'httpcompression/response_count': 103,
 'item_scraped_count': 202,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 103,
 'scheduler/dequeued': 103,
 'scheduler/dequeued/memory': 103,
 'scheduler/enqueued': 103,
 'scheduler/enqueued/memory': 103,
 'start_time': datetime.datetime(2023, 10, 24, 11, 48, 12, 457895)}
2023-10-24 11:48:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:49:47 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:49:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:49:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:49:47 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:49:47 [scrapy.extensions.telnet] INFO: Telnet Password: 7132ce4bde7d1f7b
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:49:47 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:49:47 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:49:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:49:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:49:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:49:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:49:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:49:56 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:49:56 [scrapy.extensions.telnet] INFO: Telnet Password: 140bc07cabe5de41
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:49:56 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:49:56 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:49:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:49:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:50:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:50:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:50:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:50:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:50:51 [scrapy.extensions.telnet] INFO: Telnet Password: 96986a35d342a770
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:50:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:50:51 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:50:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:50:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:51:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:51:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:51:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:51:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:51:31 [scrapy.extensions.telnet] INFO: Telnet Password: f015fd33bc4b63c7
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:51:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:51:31 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:51:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:51:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:51:47 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:51:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18466,
 'downloader/request_count': 28,
 'downloader/request_method_count/GET': 28,
 'downloader/response_bytes': 1861228,
 'downloader/response_count': 28,
 'downloader/response_status_count/200': 28,
 'elapsed_time_seconds': 15.871161,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 51, 47, 135253),
 'httpcompression/response_bytes': 6670823,
 'httpcompression/response_count': 28,
 'item_scraped_count': 71,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 4,
 'response_received_count': 28,
 'scheduler/dequeued': 28,
 'scheduler/dequeued/memory': 28,
 'scheduler/enqueued': 28,
 'scheduler/enqueued/memory': 28,
 'start_time': datetime.datetime(2023, 10, 24, 11, 51, 31, 264092)}
2023-10-24 11:51:47 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:53:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:53:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:53:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:53:31 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:53:31 [scrapy.extensions.telnet] INFO: Telnet Password: 4a727cad1e134b69
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:53:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:53:31 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:53:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:53:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:54:03 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:54:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 44966,
 'downloader/request_count': 72,
 'downloader/request_method_count/GET': 72,
 'downloader/response_bytes': 4909468,
 'downloader/response_count': 72,
 'downloader/response_status_count/200': 72,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 31.970117,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 54, 3, 430649),
 'httpcompression/response_bytes': 17612335,
 'httpcompression/response_count': 72,
 'item_scraped_count': 146,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 8,
 'response_received_count': 72,
 'scheduler/dequeued': 72,
 'scheduler/dequeued/memory': 72,
 'scheduler/enqueued': 72,
 'scheduler/enqueued/memory': 72,
 'start_time': datetime.datetime(2023, 10, 24, 11, 53, 31, 460532)}
2023-10-24 11:54:03 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:55:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:55:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:55:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:55:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:55:00 [scrapy.extensions.telnet] INFO: Telnet Password: 02c6e7951d017f26
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:55:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:55:00 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:55:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:55:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:55:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:55:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:55:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:55:34 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:55:34 [scrapy.extensions.telnet] INFO: Telnet Password: 86ce6210004bc9c1
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:55:34 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:55:34 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:55:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:55:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:56:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:56:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 31751,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 3483857,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 51,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 39.467452,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 56, 13, 845336),
 'httpcompression/response_bytes': 12578058,
 'httpcompression/response_count': 51,
 'item_scraped_count': 108,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 51,
 'scheduler/dequeued': 51,
 'scheduler/dequeued/memory': 51,
 'scheduler/enqueued': 51,
 'scheduler/enqueued/memory': 51,
 'start_time': datetime.datetime(2023, 10, 24, 11, 55, 34, 377884)}
2023-10-24 11:56:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:58:01 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:58:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:58:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:58:01 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:58:01 [scrapy.extensions.telnet] INFO: Telnet Password: a4ce8d8cf2682c5b
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:58:01 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:58:01 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:58:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:58:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:58:26 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:58:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 19081,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 1936216,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'elapsed_time_seconds': 25.84936,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 58, 26, 887029),
 'httpcompression/response_bytes': 6983801,
 'httpcompression/response_count': 29,
 'item_scraped_count': 76,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 4,
 'response_received_count': 29,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 29,
 'scheduler/enqueued/memory': 29,
 'start_time': datetime.datetime(2023, 10, 24, 11, 58, 1, 37669)}
2023-10-24 11:58:26 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 11:59:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 11:59:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 11:59:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 11:59:35 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 11:59:35 [scrapy.extensions.telnet] INFO: Telnet Password: 8aaeec1f2b1f1d8b
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 11:59:35 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 11:59:35 [scrapy.core.engine] INFO: Spider opened
2023-10-24 11:59:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 11:59:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 11:59:57 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 11:59:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 32230,
 'downloader/request_count': 53,
 'downloader/request_method_count/GET': 53,
 'downloader/response_bytes': 3609894,
 'downloader/response_count': 53,
 'downloader/response_status_count/200': 53,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 21.789925,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 11, 59, 57, 644268),
 'httpcompression/response_bytes': 12907468,
 'httpcompression/response_count': 53,
 'item_scraped_count': 112,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 53,
 'scheduler/dequeued': 53,
 'scheduler/dequeued/memory': 53,
 'scheduler/enqueued': 53,
 'scheduler/enqueued/memory': 53,
 'start_time': datetime.datetime(2023, 10, 24, 11, 59, 35, 854343)}
2023-10-24 11:59:57 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:01:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:01:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:01:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:01:07 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:01:07 [scrapy.extensions.telnet] INFO: Telnet Password: 54baeeb7ce4bf63d
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:01:07 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:01:07 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:01:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:01:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:01:54 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:01:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 69954,
 'downloader/request_count': 110,
 'downloader/request_method_count/GET': 110,
 'downloader/response_bytes': 7509366,
 'downloader/response_count': 110,
 'downloader/response_status_count/200': 110,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 46.257346,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 1, 54, 109255),
 'httpcompression/response_bytes': 26890428,
 'httpcompression/response_count': 110,
 'item_scraped_count': 216,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 110,
 'scheduler/dequeued': 110,
 'scheduler/dequeued/memory': 110,
 'scheduler/enqueued': 110,
 'scheduler/enqueued/memory': 110,
 'start_time': datetime.datetime(2023, 10, 24, 12, 1, 7, 851909)}
2023-10-24 12:01:54 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:05:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:05:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:05:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:05:00 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:05:00 [scrapy.extensions.telnet] INFO: Telnet Password: 82295b8eb8c8f2c8
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:05:00 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:05:00 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:05:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:05:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:05:19 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:05:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 19081,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 1933749,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'elapsed_time_seconds': 18.490486,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 5, 19, 316231),
 'httpcompression/response_bytes': 6983602,
 'httpcompression/response_count': 29,
 'item_scraped_count': 76,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 4,
 'response_received_count': 29,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 29,
 'scheduler/enqueued/memory': 29,
 'start_time': datetime.datetime(2023, 10, 24, 12, 5, 0, 825745)}
2023-10-24 12:05:19 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:09:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:09:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:09:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:09:44 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:09:44 [scrapy.extensions.telnet] INFO: Telnet Password: ea0958833a9d206c
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:09:44 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:09:44 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:09:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:09:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:09:51 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:09:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:09:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:09:51 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:09:51 [scrapy.extensions.telnet] INFO: Telnet Password: 950c8e665807995c
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:09:51 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:09:51 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:09:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:09:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:10:34 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:10:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 63499,
 'downloader/request_count': 104,
 'downloader/request_method_count/GET': 104,
 'downloader/response_bytes': 7066387,
 'downloader/response_count': 104,
 'downloader/response_status_count/200': 104,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 43.3282,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 10, 34, 561697),
 'httpcompression/response_bytes': 25163347,
 'httpcompression/response_count': 104,
 'item_scraped_count': 204,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 11,
 'response_received_count': 104,
 'scheduler/dequeued': 104,
 'scheduler/dequeued/memory': 104,
 'scheduler/enqueued': 104,
 'scheduler/enqueued/memory': 104,
 'start_time': datetime.datetime(2023, 10, 24, 12, 9, 51, 233497)}
2023-10-24 12:10:34 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:11:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:11:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:11:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:11:48 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:11:48 [scrapy.extensions.telnet] INFO: Telnet Password: 2cf9133659e012d9
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:11:48 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:11:48 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:11:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:11:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:12:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 311, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 109] The pipe has been ended

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 57, in parse
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 822, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 249, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 320, in _recv_bytes
    raise EOFError
EOFError
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-35-york-street-wingfield-sa-5013-504445036> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-146-148-marion-road-west-richmond-sa-5033-504421128> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-6-4-aldenhoven-road-lonsdale-sa-5160-504406632> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-9-347-main-south-road-morphett-vale-sa-5162-504330904> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-3-6-charlotte-street-smithfield-sa-5114-504402924> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-16-4-aldenhoven-road-lonsdale-sa-5160-504428548> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-1003-north-east-road-modbury-sa-5092-504347232> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-15-17-19-buchanan-court-hindmarsh-valley-sa-5211-504393220> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-28-racecourse-road-whyalla-norrie-sa-5608-504379364> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-174-gray-street-adelaide-sa-5000-504438032> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-3-32-simper-crescent-mount-barker-sa-5251-504436976> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-224-mount-gambier-road-millicent-sa-5280-504390660> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-3-5-enterprise-court-lonsdale-sa-5160-504431556> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-unit-2-16-proper-bay-road-port-lincoln-sa-5606-500083726> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-410-regency-road-prospect-sa-5082-504420592> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-6-57-61-west-avenue-edinburgh-sa-5111-504431148> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-sale/property-cub-lonsdale-105-o-sullivans-beach-road-lonsdale-sa-5160-503981286?listingType=rent> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=2)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.realcommercial.com.au//for-lease/property-2b-16-20-birmingham-street-mile-end-south-sa-5031-504430388> (referer: https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&maxPrice=50000&minPrice=15000&page=3)
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\core\spidermw.py", line 104, in process_sync
    for r in iterable:
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 134, in parse_detail
    self.Q.put(items)
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:12:12 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method scrapspider.close of <scrapspider 'scrapspider' at 0x1f96b5f3d90>>
Traceback (most recent call last):
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\defer.py", line 312, in maybeDeferred_coro
    result = f(*args, **kw)
  File "C:\Users\Jackson\anaconda3\Lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\Project\scrapping\pyqt-scrapy\scraps\spiders\scraps.py", line 139, in close
    spider.Q.put('')
  File "<string>", line 2, in put
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 205, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\Jackson\anaconda3\Lib\multiprocessing\connection.py", line 279, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed
2023-10-24 12:12:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18626,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 2052147,
 'downloader/response_count': 31,
 'downloader/response_status_count/200': 31,
 'elapsed_time_seconds': 23.664229,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 12, 12, 381104),
 'httpcompression/response_bytes': 7172429,
 'httpcompression/response_count': 31,
 'item_scraped_count': 37,
 'log_count/ERROR': 20,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 3,
 'response_received_count': 31,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'spider_exceptions/BrokenPipeError': 18,
 'spider_exceptions/EOFError': 1,
 'start_time': datetime.datetime(2023, 10, 24, 12, 11, 48, 716875)}
2023-10-24 12:12:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:16:46 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:16:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:16:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:16:46 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:16:46 [scrapy.extensions.telnet] INFO: Telnet Password: beb5c2ad9128da38
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:16:46 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:16:46 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:16:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:16:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:17:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:17:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 26721,
 'downloader/request_count': 41,
 'downloader/request_method_count/GET': 41,
 'downloader/response_bytes': 2782684,
 'downloader/response_count': 41,
 'downloader/response_status_count/200': 41,
 'elapsed_time_seconds': 22.605334,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 17, 8, 902372),
 'httpcompression/response_bytes': 10115221,
 'httpcompression/response_count': 41,
 'item_scraped_count': 126,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 6,
 'response_received_count': 41,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2023, 10, 24, 12, 16, 46, 297038)}
2023-10-24 12:17:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:23:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:23:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:23:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:23:57 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:23:57 [scrapy.extensions.telnet] INFO: Telnet Password: f2e0c6f29e8a3f08
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:23:57 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:23:57 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:23:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:23:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:24:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:24:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 42490,
 'downloader/request_count': 70,
 'downloader/request_method_count/GET': 70,
 'downloader/response_bytes': 4746555,
 'downloader/response_count': 70,
 'downloader/response_status_count/200': 70,
 'elapsed_time_seconds': 34.412474,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 24, 32, 332044),
 'httpcompression/response_bytes': 16765156,
 'httpcompression/response_count': 70,
 'item_scraped_count': 126,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 7,
 'response_received_count': 70,
 'scheduler/dequeued': 70,
 'scheduler/dequeued/memory': 70,
 'scheduler/enqueued': 70,
 'scheduler/enqueued/memory': 70,
 'start_time': datetime.datetime(2023, 10, 24, 12, 23, 57, 919570)}
2023-10-24 12:24:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:26:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:26:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:26:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:26:10 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:26:10 [scrapy.extensions.telnet] INFO: Telnet Password: 8412ba9897ce9a8b
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:26:10 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:26:10 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:26:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:26:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:27:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 12:27:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 106513,
 'downloader/request_count': 153,
 'downloader/request_method_count/GET': 153,
 'downloader/response_bytes': 9774105,
 'downloader/response_count': 153,
 'downloader/response_status_count/200': 153,
 'elapsed_time_seconds': 56.386589,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 12, 27, 7, 329647),
 'httpcompression/response_bytes': 34453676,
 'httpcompression/response_count': 153,
 'item_scraped_count': 296,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 9,
 'response_received_count': 153,
 'scheduler/dequeued': 153,
 'scheduler/dequeued/memory': 153,
 'scheduler/enqueued': 153,
 'scheduler/enqueued/memory': 153,
 'start_time': datetime.datetime(2023, 10, 24, 12, 26, 10, 943058)}
2023-10-24 12:27:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-10-24 12:39:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:39:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:39:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:39:14 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:39:14 [scrapy.extensions.telnet] INFO: Telnet Password: 8110a1161c3c97e0
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:39:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:39:14 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:39:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:39:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:39:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/bourke-place-109>: HTTP status code is not handled or not allowed
2023-10-24 12:40:14 [scrapy.extensions.logstats] INFO: Crawled 132 pages (at 132 pages/min), scraped 248 items (at 248 items/min)
2023-10-24 12:40:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/388-queen-st-134>: HTTP status code is not handled or not allowed
2023-10-24 12:41:14 [scrapy.extensions.logstats] INFO: Crawled 311 pages (at 179 pages/min), scraped 569 items (at 321 items/min)
2023-10-24 12:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/smartstores@metroplex-133>: HTTP status code is not handled or not allowed
2023-10-24 12:42:14 [scrapy.extensions.logstats] INFO: Crawled 488 pages (at 177 pages/min), scraped 1043 items (at 474 items/min)
2023-10-24 12:42:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/10-market-street-126>: HTTP status code is not handled or not allowed
2023-10-24 12:43:14 [scrapy.extensions.logstats] INFO: Crawled 653 pages (at 165 pages/min), scraped 1450 items (at 407 items/min)
2023-10-24 12:43:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/adc-house-92>: HTTP status code is not handled or not allowed
2023-10-24 12:44:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/melbourne-central-tower-131>: HTTP status code is not handled or not allowed
2023-10-24 12:44:14 [scrapy.extensions.logstats] INFO: Crawled 820 pages (at 167 pages/min), scraped 2254 items (at 804 items/min)
2023-10-24 12:45:14 [scrapy.extensions.logstats] INFO: Crawled 1010 pages (at 190 pages/min), scraped 2804 items (at 550 items/min)
2023-10-24 12:46:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 12:46:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 12:46:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 12:46:28 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 12:46:28 [scrapy.extensions.telnet] INFO: Telnet Password: c49108c0181345d8
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 12:46:28 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 12:46:28 [scrapy.core.engine] INFO: Spider opened
2023-10-24 12:46:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 12:46:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 12:47:28 [scrapy.extensions.logstats] INFO: Crawled 131 pages (at 131 pages/min), scraped 239 items (at 239 items/min)
2023-10-24 12:48:28 [scrapy.extensions.logstats] INFO: Crawled 278 pages (at 147 pages/min), scraped 551 items (at 312 items/min)
2023-10-24 12:49:28 [scrapy.extensions.logstats] INFO: Crawled 487 pages (at 209 pages/min), scraped 1555 items (at 1004 items/min)
2023-10-24 12:50:28 [scrapy.extensions.logstats] INFO: Crawled 670 pages (at 183 pages/min), scraped 2455 items (at 900 items/min)
2023-10-24 12:51:28 [scrapy.extensions.logstats] INFO: Crawled 854 pages (at 184 pages/min), scraped 3616 items (at 1161 items/min)
2023-10-24 12:52:28 [scrapy.extensions.logstats] INFO: Crawled 1052 pages (at 198 pages/min), scraped 5158 items (at 1542 items/min)
2023-10-24 12:53:28 [scrapy.extensions.logstats] INFO: Crawled 1248 pages (at 196 pages/min), scraped 6452 items (at 1294 items/min)
2023-10-24 12:54:28 [scrapy.extensions.logstats] INFO: Crawled 1454 pages (at 206 pages/min), scraped 7772 items (at 1320 items/min)
2023-10-24 12:55:28 [scrapy.extensions.logstats] INFO: Crawled 1640 pages (at 186 pages/min), scraped 9189 items (at 1417 items/min)
2023-10-24 12:56:28 [scrapy.extensions.logstats] INFO: Crawled 1839 pages (at 199 pages/min), scraped 10507 items (at 1318 items/min)
2023-10-24 12:57:28 [scrapy.extensions.logstats] INFO: Crawled 2012 pages (at 173 pages/min), scraped 11694 items (at 1187 items/min)
2023-10-24 12:58:28 [scrapy.extensions.logstats] INFO: Crawled 2194 pages (at 182 pages/min), scraped 13003 items (at 1309 items/min)
2023-10-24 12:59:28 [scrapy.extensions.logstats] INFO: Crawled 2393 pages (at 199 pages/min), scraped 14361 items (at 1358 items/min)
2023-10-24 13:00:28 [scrapy.extensions.logstats] INFO: Crawled 2584 pages (at 191 pages/min), scraped 15516 items (at 1155 items/min)
2023-10-24 13:01:28 [scrapy.extensions.logstats] INFO: Crawled 2760 pages (at 176 pages/min), scraped 15516 items (at 0 items/min)
2023-10-24 13:02:28 [scrapy.extensions.logstats] INFO: Crawled 2953 pages (at 193 pages/min), scraped 15516 items (at 0 items/min)
2023-10-24 13:03:20 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-10-24 13:03:20 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-10-24 13:03:20 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=334963&maxPrice=335938. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=234376&maxPrice=236329. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=3&minPrice=78126&maxPrice=93750. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:20 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.realcommercial.com.au/for-lease/?includePropertiesWithin=includesurrounding&locations=south-australia&propertyTypes=industrial-warehouse&page=1&minPrice=333986&maxPrice=334962. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2023-10-24 13:03:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:03:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:03:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:03:32 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:03:32 [scrapy.extensions.telnet] INFO: Telnet Password: c1b93145316faa9b
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:03:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:03:32 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:03:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:03:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:04:32 [scrapy.extensions.logstats] INFO: Crawled 119 pages (at 119 pages/min), scraped 227 items (at 227 items/min)
2023-10-24 13:05:32 [scrapy.extensions.logstats] INFO: Crawled 275 pages (at 156 pages/min), scraped 500 items (at 273 items/min)
2023-10-24 13:05:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.realcommercial.com.au/http://www.realcommercial.com.au/building/243-sherbrooke-129>: HTTP status code is not handled or not allowed
2023-10-24 13:06:32 [scrapy.extensions.logstats] INFO: Crawled 400 pages (at 125 pages/min), scraped 861 items (at 361 items/min)
2023-10-24 13:07:32 [scrapy.extensions.logstats] INFO: Crawled 549 pages (at 149 pages/min), scraped 1702 items (at 841 items/min)
2023-10-24 13:08:32 [scrapy.extensions.logstats] INFO: Crawled 720 pages (at 171 pages/min), scraped 2496 items (at 794 items/min)
2023-10-24 13:09:32 [scrapy.extensions.logstats] INFO: Crawled 920 pages (at 200 pages/min), scraped 3318 items (at 822 items/min)
2023-10-24 13:10:32 [scrapy.extensions.logstats] INFO: Crawled 1096 pages (at 176 pages/min), scraped 4311 items (at 993 items/min)
2023-10-24 13:11:32 [scrapy.extensions.logstats] INFO: Crawled 1263 pages (at 167 pages/min), scraped 5252 items (at 941 items/min)
2023-10-24 13:12:32 [scrapy.extensions.logstats] INFO: Crawled 1463 pages (at 200 pages/min), scraped 6932 items (at 1680 items/min)
2023-10-24 13:13:32 [scrapy.extensions.logstats] INFO: Crawled 1657 pages (at 194 pages/min), scraped 8287 items (at 1355 items/min)
2023-10-24 13:14:32 [scrapy.extensions.logstats] INFO: Crawled 1836 pages (at 179 pages/min), scraped 9531 items (at 1244 items/min)
2023-10-24 13:15:32 [scrapy.extensions.logstats] INFO: Crawled 2034 pages (at 198 pages/min), scraped 11159 items (at 1628 items/min)
2023-10-24 13:16:32 [scrapy.extensions.logstats] INFO: Crawled 2235 pages (at 201 pages/min), scraped 12135 items (at 976 items/min)
2023-10-24 13:17:32 [scrapy.extensions.logstats] INFO: Crawled 2409 pages (at 174 pages/min), scraped 12495 items (at 360 items/min)
2023-10-24 13:18:32 [scrapy.extensions.logstats] INFO: Crawled 2630 pages (at 221 pages/min), scraped 13913 items (at 1418 items/min)
2023-10-24 13:19:32 [scrapy.extensions.logstats] INFO: Crawled 2826 pages (at 196 pages/min), scraped 14870 items (at 957 items/min)
2023-10-24 13:20:32 [scrapy.extensions.logstats] INFO: Crawled 3041 pages (at 215 pages/min), scraped 16027 items (at 1157 items/min)
2023-10-24 13:21:32 [scrapy.extensions.logstats] INFO: Crawled 3244 pages (at 203 pages/min), scraped 17825 items (at 1798 items/min)
2023-10-24 13:22:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:22:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:22:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:22:30 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:22:30 [scrapy.extensions.telnet] INFO: Telnet Password: c9592027828388a1
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:22:30 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:22:30 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:22:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:22:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:23:30 [scrapy.extensions.logstats] INFO: Crawled 133 pages (at 133 pages/min), scraped 251 items (at 251 items/min)
2023-10-24 13:24:30 [scrapy.extensions.logstats] INFO: Crawled 347 pages (at 214 pages/min), scraped 468 items (at 217 items/min)
2023-10-24 13:25:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:25:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:25:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:25:45 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:25:45 [scrapy.extensions.telnet] INFO: Telnet Password: 47337ba8503812e6
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:25:45 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:25:45 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:25:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:25:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:26:45 [scrapy.extensions.logstats] INFO: Crawled 122 pages (at 122 pages/min), scraped 230 items (at 230 items/min)
2023-10-24 13:27:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scraps)
2023-10-24 13:27:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.10 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19045-SP0
2023-10-24 13:27:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scraps',
 'LOG_FILE': 'output.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'scraps.spiders',
 'SPIDER_MODULES': ['scraps.spiders']}
2023-10-24 13:27:12 [py.warnings] WARNING: C:\Users\Jackson\anaconda3\Lib\site-packages\scrapy\utils\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-10-24 13:27:12 [scrapy.extensions.telnet] INFO: Telnet Password: 16e183578e54197d
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-10-24 13:27:12 [scrapy.middleware] INFO: Enabled item pipelines:
['scraps.pipelines.ChanelPipeline']
2023-10-24 13:27:12 [scrapy.core.engine] INFO: Spider opened
2023-10-24 13:27:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-10-24 13:27:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-10-24 13:28:12 [scrapy.extensions.logstats] INFO: Crawled 136 pages (at 136 pages/min), scraped 273 items (at 273 items/min)
2023-10-24 13:29:12 [scrapy.extensions.logstats] INFO: Crawled 364 pages (at 228 pages/min), scraped 601 items (at 328 items/min)
2023-10-24 13:30:12 [scrapy.extensions.logstats] INFO: Crawled 588 pages (at 224 pages/min), scraped 921 items (at 320 items/min)
2023-10-24 13:31:12 [scrapy.extensions.logstats] INFO: Crawled 825 pages (at 237 pages/min), scraped 1460 items (at 539 items/min)
2023-10-24 13:32:12 [scrapy.extensions.logstats] INFO: Crawled 1054 pages (at 229 pages/min), scraped 1999 items (at 539 items/min)
2023-10-24 13:33:12 [scrapy.extensions.logstats] INFO: Crawled 1266 pages (at 212 pages/min), scraped 2487 items (at 488 items/min)
2023-10-24 13:34:12 [scrapy.extensions.logstats] INFO: Crawled 1504 pages (at 238 pages/min), scraped 3017 items (at 530 items/min)
2023-10-24 13:35:12 [scrapy.extensions.logstats] INFO: Crawled 1723 pages (at 219 pages/min), scraped 3429 items (at 412 items/min)
2023-10-24 13:36:12 [scrapy.extensions.logstats] INFO: Crawled 1963 pages (at 240 pages/min), scraped 3808 items (at 379 items/min)
2023-10-24 13:37:12 [scrapy.extensions.logstats] INFO: Crawled 2206 pages (at 243 pages/min), scraped 4195 items (at 387 items/min)
2023-10-24 13:38:12 [scrapy.extensions.logstats] INFO: Crawled 2430 pages (at 224 pages/min), scraped 4587 items (at 392 items/min)
2023-10-24 13:39:04 [scrapy.core.engine] INFO: Closing spider (finished)
2023-10-24 13:39:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1864283,
 'downloader/request_count': 2623,
 'downloader/request_method_count/GET': 2623,
 'downloader/response_bytes': 167137809,
 'downloader/response_count': 2623,
 'downloader/response_status_count/200': 2623,
 'elapsed_time_seconds': 711.856315,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 10, 24, 13, 39, 4, 37770),
 'httpcompression/response_bytes': 589506680,
 'httpcompression/response_count': 2623,
 'item_scraped_count': 4769,
 'log_count/INFO': 21,
 'log_count/WARNING': 1,
 'request_depth_max': 59,
 'response_received_count': 2623,
 'scheduler/dequeued': 2623,
 'scheduler/dequeued/memory': 2623,
 'scheduler/enqueued': 2623,
 'scheduler/enqueued/memory': 2623,
 'start_time': datetime.datetime(2023, 10, 24, 13, 27, 12, 181455)}
2023-10-24 13:39:04 [scrapy.core.engine] INFO: Spider closed (finished)
